./1-using/1-configuration.mdx:::--- title: 'How to configure Conduit' sidebar_label: 'Configuration' slug: '/configuration' --- Conduit accepts CLI flags, environment variables and a configuration file to configure its behavior. Each CLI flag has a corresponding environment variable and a corresponding field in the configuration file. Conduit uses the value for each configuration option based on the following priorities: ## CLI flags **CLI flags** (highest priority) - if a CLI flag is provided it will always be respected, regardless of the environment variable or configuration file. To see a full list of available flags run `conduit --help`: ```text Conduit CLI is a command-line that helps you interact with and manage Conduit. Usage: conduit [flags] conduit [command] Pipelines pipelines Initialize and manage pipelines Additional Commands: help Help about any command init Initialize Conduit with a configuration file and directories. Flags: --api.enabled enable HTTP and gRPC API (default true) --connectors.path string path to standalone connectors' directory (default "/home/haris/projects/conduitio/conduit/connectors") --db.badger.path string path to badger DB (default "/home/haris/projects/conduitio/conduit/conduit.db") --db.postgres.connection-string string postgres connection string, may be a database URL or in PostgreSQL keyword/value format --db.postgres.table string postgres table in which to store data (will be created if it does not exist) (default "conduit_kv_store") --db.sqlite.path string path to sqlite3 DB (default "/home/haris/projects/conduitio/conduit/conduit.db") --db.sqlite.table string sqlite3 table in which to store data (will be created if it does not exist) (default "conduit_kv_store") --db.type string database type; accepts badger,postgres,inmemory,sqlite (default "badger") --grpc.address string address for serving the gRPC API (default ":8084") -h, --help help for conduit --http.address string address for serving the HTTP API (default ":8080") --log.format string sets the format of the logging; accepts json, cli (default "cli") --log.level string sets logging level; accepts debug, info, warn, error, trace (default "info") --pipelines.error-recovery.backoff-factor int backoff factor applied to the last delay (default 2) --pipelines.error-recovery.max-delay duration maximum delay before restart (default 10m0s) --pipelines.error-recovery.max-retries int maximum number of retries (default -1) --pipelines.error-recovery.max-retries-window duration amount of time running without any errors after which a pipeline is considered healthy (default 5m0s) --pipelines.error-recovery.min-delay duration minimum delay before restart (default 1s) --pipelines.exit-on-degraded exit Conduit if a pipeline enters a degraded state --pipelines.path string path to the directory that has the yaml pipeline configuration files, or a single pipeline configuration file (default "/home/haris/projects/conduitio/conduit/pipelines") --preview.pipeline-arch-v2 enables experimental pipeline architecture v2 (note that the new architecture currently supports only 1 source and 1 destination per pipeline) --processors.path string path to standalone processors' directory (default "/home/haris/projects/conduitio/conduit/processors") --schema-registry.confluent.connection-string string confluent schema registry connection string --schema-registry.type string schema registry type; accepts builtin,confluent (default "builtin") -v, --version version for conduit Use "conduit [command] --help" for more information about a command. ``` ## Environment variables **Environment variables** (lower priority) - an environment variable is only used if no CLI flag is provided for the same option. Environment variables have the prefix `CONDUIT` and contain underscores instead of dots and hyphens (e.g. the flag `-db.postgres.connection-string` corresponds to `CONDUIT_DB_POSTGRES_CONNECTION_STRING`). ## Configuration file **Configuration file** (lowest priority) - Conduit by default loads the file `conduit.yaml` placed in the same folder as Conduit. The path to the file can be customized using the CLI flag `-config`. It is not required to provide a configuration file and any value in the configuration file can be overridden by an environment variable or a flag. The file content should be a YAML document where keys can be hierarchically split on `.`. For example: ```yaml db: type: postgres # corresponds to flag -db.type and env variable CONDUIT_DB_TYPE postgres: connection-string: postgres://localhost:5432/conduitdb # -db.postgres.connection-string or CONDUIT_DB_POSTGRES_CONNECTION_STRING ``` ![scarf pixel conduit-site-docs-using](https://static.scarf.sh/a.png?x-pxid=76d5981f-cd63-422b-8cf7-eab9e99f0cd3)
./1-using/0-installing-and-running.mdx:::--- title: "Installing and running" hide_table_of_contents: true slug: '/installing-and-running' --- import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; <Tabs groupId="instalation-options" queryString="option"> <TabItem value="macos-linux" label="macOS and Linux" default> The recommended way of running Conduit on a local machine is using the latest version which you can install by running the following command: ``` curl https://conduit.io/install.sh | bash ``` ### Run Once installed, you can start Conduit by running: ```shell conduit ``` </TabItem> <TabItem value="windows" label="Windows"> Below we'll walk through getting Conduit installed and running on your Windows OS. ### Download First, download the [latest Conduit release](https://github.com/ConduitIO/conduit/releases/latest) for your platform. Let's say you downloaded `conduit_0.12.2_Windows_x86_64.zip`. ### Unzip the archive ```shell tar -xf conduit_0.12.2_Windows_x86_64.zip ``` ### Run In the directory in which you expanded the archive. Execute the Conduit binary: ```shell conduit.exe ``` </TabItem> <TabItem value="docker" label="Docker"> We distribute [Docker](https://docs.docker.com/) images to ease the use of Conduit in a containerized environment. To see details about how our images are built please feel free to take a look at the `Dockerfile` [here](https://github.com/ConduitIO/conduit/blob/main/Dockerfile). Conduit's Docker images are hosted on [GitHub](https://github.com/ConduitIO/conduit/pkgs/container/conduit). ### Run After you install Docker, you can pull the Conduit image by running the following command: ```shell docker pull conduit.docker.scarf.sh/conduitio/conduit ``` Then, you can run Conduit by running: ```shell docker run -it -p 8080:8080 conduit.docker.scarf.sh/conduitio/conduit ``` The command aboves runs Conduit with no pipelines. To run Conduit with a custom pipeline file you can mount it as a volume into the container: ```shell docker run -it -p 8080:8080 \ -v /path/to/pipeline.yaml:/app/pipelines/pipeline.yaml \ conduit.docker.scarf.sh/conduitio/conduit ``` :::tip If you're using Docker, check out [Watchtower](https://containrrr.dev/watchtower/). After we push an update, Watchtower will keep your Conduit container updated by gracefully shutting down your existing container and restarting it with the same options used when initially deployed. ::: </TabItem> <TabItem value="source" label="Building from source"> Before you can build Conduit from source, you need to have the latest version of Go installed. The Go website has [comprehensive instructions](https://go.dev/doc/install) on how to get the language installed on your machine. Once installed, you can follow the rest of the instructions. 1. Start by downloading the source code from the latest stable release on the Conduit [Releases Page](https://github.com/ConduitIO/conduit/releases/latest). Alternatively, you can run this command to automatically download the latest stable source to your current directory: ```shell $ TAG=v0.12.2; curl -o conduit.tar.gz -L https://github.com/ConduitIO/conduit/archive/refs/tags/$TAG.tar.gz ``` A file called `conduit.tgz` will be in your current directory. The next step is to expand the source: ```shell $ tar zxvf conduit.tgz ``` Then change directories to the appropriate folder. Keep in mind that the folder name might be different between releases since it's tied to the latest git sha for the commit. ```shell $ cd conduit-0.12.2 ``` Now build the project: ```shell $ make ``` You will have a new binary built for your architecture and machine. All that's left is to run it! ```shell ./conduit ``` </TabItem> </Tabs> You should now be able to interact with the Conduit UI and HTTP API on port 8080: ```shell .... .::::::::::. .:::::‘‘‘‘:::::. .:::: ::::. .:::::::: ::::::::. `:::::::: ::::::::‘ `:::: ::::‘ `:::::....:::::‘ `::::::::::‘ Conduit v0.12.2 windows/amd64 ‘‘‘‘ 2024-02-20T21:37:45+00:00 INF All 0 tables opened in 0s component=badger.DB 2024-02-20T21:37:45+00:00 INF Discard stats nextEmptySlot: 0 component=badger.DB 2024-02-20T21:37:45+00:00 INF Set nextTxnTs to 0 component=badger.DB 2024-02-20T21:37:45+00:00 INF builtin plugins initialized component=builtin.Registry count=6 2024-02-20T21:37:45+00:00 WRN could not read plugin directory error="open /app/connectors: no such file or directory" component=standalone.Registry 2024-02-20T21:37:45+00:00 INF standalone plugins initialized component=standalone.Registry count=0 plugin_path=/app/connectors 2024-02-20T21:37:45+00:00 INF processors initialized component=processor.Service count=0 2024-02-20T21:37:45+00:00 INF connectors initialized component=connector.Service count=0 2024-02-20T21:37:45+00:00 INF pipelines initialized component=pipeline.Service count=0 2024-02-20T21:37:45+00:00 INF pipeline configs provisioned component=provisioning.Service created=[] deleted=[] pipelines_path=./pipelines 2024-02-20T21:37:45+00:00 INF grpc server started address=[::]:8084 2024-02-20T21:37:45+00:00 INF http server started address=[::]:8080 2024-02-20T21:37:45+00:00 INF 2024-02-20T21:37:45+00:00 INF click here to navigate to Conduit UI: http://localhost:8080/ui 2024-02-20T21:37:45+00:00 INF click here to navigate to explore the HTTP API: http://localhost:8080/openapi 2024-02-20T21:37:45+00:00 INF ``` ## Next Steps Now that you have Conduit installed you can learn [how to get started]÷(/docs/getting-started). You can also explore some other topics, such as: - [Pipelines](/docs/using/pipelines/configuration-file) - [Connectors](/docs/using/connectors/getting-started) - [Processors](/docs/using/processors/getting-started ![scarf pixel conduit-site-docs-using](https://static.scarf.sh/a.png?x-pxid=76d5981f-cd63-422b-8cf7-eab9e99f0cd3)
./1-using/2-opencdc-record.mdx:::--- title: 'OpenCDC record' --- An OpenCDC record in Conduit aims to standardize the format of data records exchanged between different connectors within a data processing pipeline. The primary objective is to ensure compatibility between various combinations of source and destination connectors. ## Benefits 1. **Support for Operations**: The format should support representing records for `create`, `update`, `delete`, and `snapshot` operations. 2. **Standard Metadata Fields**: Definition of standard metadata fields to provide essential information about each record. These can vary depending on the record. See [Metadata Fields](#metadata-fields) for more information. 3. **Integration with Data Tools**: We believe that being strict about the record format Conduit consumes and produces will make it easier to integrate with other data processing tools. ## Fields * `.Position` uniquely represents the position of record. This is used to track the position of a record in a source connector, enabling Conduit to resume a stopped pipeline. * `.Operation` defines what triggered the creation of a record. There are four possibilities: `create`, `update`, `delete` or `snapshot`. The first three operations are encountered during normal CDC operation, while `snapshot` is meant to represent records during an initial load. Depending on the operation, the record will contain either the payload before the change, after the change, both or none (see fields `.Payload.Before` and `.Payload.After`). * `.Key` represents a value that should identify the entity (e.g. database row). * `.Metadata` contains additional information regarding the record. * `.Payload.Before` holds the payload before the operation ocurred. These could be present in operations such as `update` and `delete`. * `.Payload.After` holds the payload after the operation ocurred. These could be present in operations such as `create`, `snapshot` or `update`. :::note We're indicating `.Position`, and not `.position` as defined in its [`Record` message](https://buf.build/conduitio/conduit-commons/docs/main:opencdc.v1#opencdc.v1.Record), to show its [Go template](https://pkg.go.dev/text/template) notation as used by the [Go representation of an OpenCDC record](https://github.com/ConduitIO/conduit-commons/blob/main/opencdc/record.go#L32). This field is public and must start with an uppercase letter. ::: ## Representation Conduit uses [Protocol Buffers (protobuf)](https://protobuf.dev/) to define an OpenCDC record. Its definition can be found in the [Buf Schema Registry](https://buf.build/conduitio/conduit-commons/docs/main:opencdc.v1). When processing records in Conduit, you can always expect a similar structure to the following: ```json { "position": "c3RhbmRpbmc=", "operation": "update", "metadata": { "file.path": "./example.in", "opencdc.readAt": "1663858188836816000", "opencdc.version": "v1" }, "key": "cGFkbG9jay1rZXk=", "payload": { "before": "eWVsbG93", "after": { "bool": true, "float32": 1.2, "float64": 1.2, "int": 1, "int32": 1, "int64": 1, "string": "orange" } } } ``` :::note `.Position`, `.Key`, and `.Payload.Before` are represented as `Base64` encoded in the example above because these will be a byte slice when represented as JSON. ::: ## Data types There are no limitations when it comes to data types a source connector can read from a source. Certain type limitations might apply depending on how the data is moved internally in Conduit, from source connector to Conduit and from Conduit to destination connectors. The key and the payload data can be represented in two ways: raw and structured. ## Raw data Raw data in Conduit is an array of bytes that can represent the source data in its original form (such as a file) or encoded in a certain way (for example, a JSON representation of a database row or an Avro encoded structure). It's represented by the `opencdc.RawData` Go type. For example, when writing a connector or a processor in Go, you can construct raw data like this: ```go opencdc.RawData([]byte{1, 3, 5}) ``` ## Structured data Structured data in Conduit is a map in which the keys are field names and values are field values. An example of that is the below record's `.Payload.After` field: ```json { // other record fields "payload": { "before": "eWVsbG93", "after": { "bool_field": true, "float_field": 1.2, "int_field": 1, "string_field": "orange" } } } ``` When writing a connector or a processor in Go, it's represented by the `opencdc.StructuredData` type. The supported data types for values in `opencdc.StructuredData` depend on following: - connector or processor type (built-in or standalone) - [schema support](/docs/using/other-features/schema-support) (enabled or disabled). In built-in connectors, the field values can be of any Go type, given that there's no (de)serialization involved. In standalone connectors with schema middleware enabled (which is the default), any type that is supported by the [Apache Avro™ format](https://avro.apache.org) is also supported by Conduit. If the schema middleware is disabled in a connector, then the supported types are limited to what Protobuf allows as a [value](https://protobuf.dev/reference/protobuf/google.protobuf/#value). That translates to the following Go types: * `bool` * `int`, `int32`, `int64`, `uint`, `uint32`, `uint64` * `float32`, `float64` * `string` * `[]byte` (stored as a string, base64-encoded) * `map[string]interface{}` (a map of strings to any of the values that are supported) * `[]interface{}` (a slice of any value that is supported) A notable limitation is timestamps, i.e. `time.Time` values are not supported. ## Metadata fields As part of an OpenCDC record, there will be a set of fields provided that will vary depending on the connector. These fields can be common to all **OpenCDC** records as part of our standard, some related to **Conduit**, and others that will be provided by each **Connector** implementation independently. These fields can be useful to define conventions that will be then used by Conduit to expand its functionality. Notice that all these fields use a dot notation syntax to indicate what they refer to, preventing accidental clashes. Here are the ones you can find: ### `opencdc.version` Contains the version of the OpenCDC format (e.g., "v1"). This field exists to ensure the OpenCDC format version can be easily identified in case the record gets marshaled into a different untyped format (e.g. JSON). ```json { // other record fields "metadata": { "opencdc.version": "v1", // rest of metadata }, // other record fields } ``` ### `opencdc.createdAt` Contains the time when the record was created in the 3rd party system. The expected format is a Unix timestamp in nanoseconds. ```json { // other record fields "metadata": { "opencdc.createdAt": "1663858188836816000", // rest of metadata }, // other record fields } ``` ### `opencdc.readAt` Contains the time when the record was read from the 3rd party system. The expected format is a Unix timestamp in nanoseconds. ```json { // other record fields "metadata": { "opencdc.readAt": "1663858188836816000", // rest of metadata }, // other record fields } ``` ### `opencdc.collection` Contains the name of the collection from which the record originated and/or where it should be written to. :::note It's up to the connector to populate this field. In other words, not all records may have this field. ::: ```json { // other record fields "metadata": { "opencdc.collection": "employees", // rest of metadata }, // other record fields } ``` ### `opencdc.key.schema.*` #### `opencdc.key.schema.subject`, `opencdc.key.schema.version` Contains the subject and version of the schema for the records' `.Key` field. :::note This field will only be populated when using structured data. ::: ```json { // other record fields "metadata": { "opencdc.key.schema.subject": "employees.key.v1", "opencdc.key.schema.version": "1", // rest of metadata }, // other record fields } ``` ### `opencdc.payload.schema.*` #### `opencdc.payload.schema.subject`, `opencdc.payload.schema.version` Contains the subject and version of the schema for the records' `.Payload.After` and `.Paylod.Before` fields. :::note This field will only be populated when using structured data. ::: ```json { // other record fields "metadata": { "opencdc.payload.schema.subject": "connector-id:collection.payload", "opencdc.payload.schema.version": "1", // rest of metadata }, // other record fields } ``` ### `conduit.source.plugin.name` The name of the source plugin that created the record. ```json { // other record fields "metadata": { "conduit.source.plugin.name": "builtin:file", // rest of metadata }, // other record fields } ``` ### `conduit.source.plugin.version` The version of the source plugin that created the record. ```json { // other record fields "metadata": { "conduit.source.plugin.version": "v1.0.2", // rest of metadata }, // other record fields } ``` ### `conduit.source.connector.id` `conduit.source.connector.id` is the ID of the source connector that received the record. ```json { // other record fields "metadata": { "conduit.source.connector.id": "connectorID", // rest of metadata }, // other record fields } ``` ### `conduit.destination.plugin.name` The name of the destination plugin that has written the record. ```json { // other record fields "metadata": { "conduit.destination.plugin.name": "builtin:file", // rest of metadata }, // other record fields } ``` ### `conduit.destination.plugin.version` The version of the destination plugin that has written the record. ```json { // other record fields "metadata": { "conduit.destination.plugin.version": "v0.9.1", // rest of metadata }, // other record fields } ``` ### `conduit.dlq.nack.error` Contains the error that caused a record to be nacked and pushed to the [dead-letter queue (DLQ)](/docs/using/other-features/dead-letter-queue). ### `conduit.dlq.nack.node.id` The ID of the internal node that nacked the record. ### Connector-specific metadata These metadata fields will be provided by each connector implementation allowing them to add any necessary metadata. As previously mentioned, to avoid unintended conflicts of metadata keys, the convention these will follow are the same as before, indicating first the connector name that's adding them. Taking the same [previous record example](#representation), you'll notice there is a metadata key named `file.path`, which would indicate this field was added by a `file` plugin. ```json { // other record fields "metadata": { "file.path": "./example.in", // rest of metadata }, // other record fields } ``` ![scarf pixel conduit-site-docs-using](https://static.scarf.sh/a.png?x-pxid=76d5981f-cd63-422b-8cf7-eab9e99f0cd3)
./1-using/6-other-features/6-metrics.mdx:::--- title: 'Metrics' --- Conduit comes with a number of already defined metrics. The metrics available are exposed through an HTTP API and ready to be scraped by Prometheus. It's also possible to easily define new metrics with existing types, or just create a completely new metric type. ## Accessing metrics Metrics are exposed at `/metrics`. For example, if you're running Conduit locally, you can fetch metrics by navigating to [`http://localhost:8080/metrics`](http://localhost:8080/metrics). ## Available metrics * **Conduit metrics**: We currently have a number of high level pipeline, processor and connector metrics, all of which are defined in [measure.go](https://github.com/ConduitIO/conduit/blob/main/pkg/foundation/metrics/measure/measure.go) . Those are: | Pipeline name | Type | Description | |------------------------------------------------|-----------|----------------------------------------------------------------------------------------------------------------| | `conduit_pipelines` | Gauge | Number of pipelines by status. | | `conduit_connectors` | Gauge | Number of connectors by type (source, destination). | | `conduit_processors` | Gauge | Number of processors by name and type. | | `conduit_connector_bytes` | Histogram | Number of bytes a connector processed by pipeline name, plugin and type (source, destination). | | `conduit_pipeline_execution_duration_seconds` | Histogram | Amount of time records spent in a pipeline. | | `conduit_connector_execution_duration_seconds` | Histogram | Amount of time spent reading or writing records per pipeline, plugin and connector type (source, destination). | | `conduit_processor_execution_duration_seconds` | Histogram | Amount of time spent on processing records per pipeline and processor. | * **Go runtime metrics**: The default metrics exposed by Prometheus' official Go package [client_golang](https://pkg.go.dev/github.com/prometheus/client_golang). * **gRPC metrics**: The gRPC instrumentation package we use is [promgrpc](https://github.com/piotrkowalczuk/promgrpc). The metrics exposed are listed [here](https://github.com/piotrkowalczuk/promgrpc#metrics). * **HTTP API metrics**: We use [promhttp](https://pkg.go.dev/github.com/prometheus/client_golang/prometheus/promhttp), Prometheus' official package for instrumentation of HTTP servers. ![scarf pixel conduit-site-docs-using-other-features](https://static.scarf.sh/a.png?x-pxid=8c8ff6d5-2756-41a3-b4ca-3ca2be381842)
./1-using/6-other-features/4-record-routing.mdx:::--- title: 'Record routing' keywords: [ 'synchronization', 'collections', 'routing' ] --- ## Overview Record routing in Conduit makes it easier to build one-to-many and many-to-many pipeline topologies. Or, in other words, it's possible to: * route data from one source to multiple collections in one destination * route data from multiple sources to multiple destinations. Through this, a number of use cases are possible: * Keep two instances synchronized. For example, two database instances with multiple tables in each can be kept in sync with a single pipeline with only one source and one destination connector. * Route data from one collection (table, topic, index, etc.) to multiple collections on a single server. * Route data to different instances or different systems altogether. This can be done in two ways: * by using a connector that supports writing to multiple collections, * by using filter processors. ## Using a connector that supports writing to multiple collections The [OpenCDC format](/docs/using/opencdc-record) recommends that a record's source or destination collection is written to its metadata using the [`opencdc.collection` key](/docs/using/opencdc-record#opencdccollection). A destination connector can then utilize that information and write the record to the correct collection. :::note It's the connectors that eventually provide the ability to read from multiple source collections or write to multiple destination collections. Consult the connector documentation to check if it supports this feature. ::: ### Example: Route data from multiple Kafka topics to multiple PostgreSQL tables on a single database server Let's assume we have a pipeline that streams data from two Kafka topics, `employees_finance` and `employees_legal` to PostgreSQL (both of the connectors support reads from multiple collections and writes to multiple collections). A pipeline configuration file could look like this: ```yaml --- version: 2.2 pipelines: - id: example-pipeline status: running name: example-pipeline description: 'Kafka to PostgreSQL' connectors: - id: kafka-source type: source plugin: "builtin:kafka" name: kafka-source settings: servers: "localhost:9092" topics: "employees_finance,employees_legal" - id: pg-destination type: destination plugin: "builtin:postgres" name: pg-destination settings: url: "postgresql://username:password@localhost/testdb?sslmode=disable" table: "employees" ``` The Kafka connector will read records like the ones below: ```json lines { "metadata": { "opencdc.collection": "employees_finance", // rest of metadata }, "payload": { "after": { "name": "John" } } // other record fields } { "metadata": { "opencdc.collection": "employees_legal", // rest of metadata }, "payload": { "after": { "name": "Adam" } } // other record fields } ``` The PostgreSQL destination connector will check the value in the `opencdc.collection` metadata field. Based on that, it will write John's information to the `employees_finance` table and Adam's information to the `employees_legal` table. ## Using a `filter` processor with a condition In some cases, records from a single source collection need to be written to multiple connectors. Even if a destination connector can write to multiple collections, that feature sometimes cannot be used because the destinations have very different configurations. For example, the records may need to be written to different databases or different systems altogether. In scenarios like these, we can route records to multiple destination connectors using the built-in [`filter` processor](/docs/using/processors/builtin/filter) with a [condition](/docs/using/processors/conditions). The advantage of this approach is that there will still be only one source, which means that we will read the data only once. ### Example: Route data from a single Kafka topic to different PostgreSQL databases Let's assume we have a pipeline that reads employee data from a topic. The employees from the finance department (identified by `department: finance` in the metadata) need to be written to one PostgreSQL database. Employees from the legal department (identified by `department: legal` in the metadata) need to be written into a different database. ```yaml --- version: 2.2 pipelines: - id: example-pipeline status: running name: example-pipeline description: 'Kafka to PostgreSQL' connectors: - id: kafka-source type: source plugin: "builtin:kafka" name: kafka-source settings: servers: "localhost:9092" topics: "employees_finance,employees_legal" - id: finance type: destination plugin: "builtin:postgres" name: finance settings: url: "postgresql://username:password@localhost/finance-db?sslmode=disable" table: "employees" processors: - id: employees_finance_only plugin: "filter" # Filter out records where the metadata field "department" is NOT set to "finance". condition: `{{ eq .Metadata.department "finance" | not }}` - id: legal type: destination plugin: "builtin:postgres" name: legal settings: url: "postgresql://username:password@localhost/legal-db?sslmode=disable" table: "employees" processors: - id: employees_legal_only plugin: "filter" # Filter out records where the metadata field "department" is NOT set to "legal". condition: `{{ eq .Metadata.department "legal" | not }}` ``` ![scarf pixel conduit-site-docs-using-other-features](https://static.scarf.sh/a.png?x-pxid=8c8ff6d5-2756-41a3-b4ca-3ca2be381842)
./1-using/6-other-features/5-api.mdx:::--- title: 'API' --- Conduit exposes a gRPC API and a REST API. ## gRPC API The gRPC API is by default running on port 8084. You can define a custom address using the CLI flag `-grpc.address`. To learn more about the gRPC API please have a look at the [protobuf file](https://github.com/ConduitIO/conduit/blob/main/proto/api/v1/api.proto) . ## REST API The REST API is by default running on port 8080. You can define a custom address using the CLI flag `-http.address`. It is generated using [gRPC gateway](https://github.com/grpc-ecosystem/grpc-gateway) and is thus providing the same functionality as the gRPC API. To learn more about the REST API please have a look at the [API documentation](https://www.conduit.io/api), [OpenAPI definition](https://github.com/ConduitIO/conduit/blob/main/pkg/web/openapi/swagger-ui/api/v1/api.swagger.json) or run Conduit and navigate to [`http://localhost:8080/openapi`](http://localhost:8080/openapi) to open a [Swagger UI](https://github.com/swagger-api/swagger-ui) which makes it easy to try it out. ![scarf pixel conduit-site-docs-using-other-features](https://static.scarf.sh/a.png?x-pxid=8c8ff6d5-2756-41a3-b4ca-3ca2be381842)
./1-using/6-other-features/3-storage.mdx:::--- title: 'Storage' --- Conduit's own data (information about pipelines, connectors, etc.) can be stored in the following databases: * BadgerDB (default) * PostgreSQL * SQLite It's also possible to store all the data in memory, which is sometimes useful for development purposes. The database type used can be configured with the `db.type` parameter (through any of the [configuration](/docs/configuration#configuration-file) options in Conduit). For example, to configure a PostgresSQL database with Conduit, the CLI flag to use a is as follows: `-db.type=postgres`. Changing database parameters (e.g. the PostgreSQL connection string) is done through parameters of the following form: `db.<db type>.<parameter name>`. For example, if we wanted to use a PostgreSQL instance listening on `localhost:5432` using command line flags: ```shell $ ./conduit -db.type=postgres -db.postgres.connection-string="postgresql://localhost:5432/conduitdb" ``` This can also be done using configuration options in your [Conduit configuration file](/docs/configuration#configuration-file): ```yaml db: type: postgres postgres: connection-string: postgresql://localhost:5432/conduitdb ``` ![scarf pixel conduit-site-docs-using-other-features](https://static.scarf.sh/a.png?x-pxid=8c8ff6d5-2756-41a3-b4ca-3ca2be381842)
./1-using/6-other-features/1-schema-support.mdx:::--- title: 'Schema Support' keywords: [ 'schema', 'avro', 'confluent' ] --- ## Overview Conduit can manage the structure and format of data as it moves through the pipeline. This makes it possible to take advantage of the benefits that the associated type information provides, such as: - **Data Integrity**: Ensuring that data adheres to the expected structure, reducing the risk of errors and inconsistencies. - **Type Safety**: Retaining type information throughout the data pipeline, allowing for safe and accurate data processing. - **Future-Proofing**: Preparing the system to handle evolving data structures, making it easier to adapt to changes without significant disruptions. Additionally, this solves the problem of transferring type information in standalone connectors. Namely, Conduit and standalone connectors communicate via Protobuf messages that have a limited set of types. Every [record](/docs/using/opencdc-record) that contains structured data can be associated with a schema. The [Apache Avro™](https://avro.apache.org/) format is supported and support for more schema types is planned. Since both, a record's key and payload, can be structured, schemas can be associated with either. Schemas are not part of a record for performance reasons. Instead, a record's metadata contains information about the [key schema](/docs/using/opencdc-record#opencdckeyschema)'s subject and version as well the [payload schema](/docs/using/opencdc-record#opencdcpayloadschema)'s subject and version. The schemas themselves are managed by [the schema registry](#schema-registry). Conduit's [Connector SDK](https://github.com/ConduitIO/conduit-connector-sdk) and [Processor SDK](https://github.com/ConduitIO/conduit-processor-sdk) make it possible to: - automatically extract a schema from a record's key or payload (note that the data has to be structured) - automatically encode or decode a record's key or payload (i.e. connectors and processors can work with structured data that contain correct types without being involved in fetching the schemas and encoding/decoding the data themselves) - work directly with the schema registry (for example, in cases where automatic schema extraction isn't enough and a schema needs to be built manually using the information from a source) More information about how to work with schemas can be found in the relevant pages for [source connectors](/docs/developing/connectors/developing-source-connectors), [destination connectors](/docs/developing/connectors/developing-destination-connectors) and [processors](/docs/developing/processors/building#schemas). :::tip To learn more about configuring source and destination connectors to automatically extract the schema from the key and payload of a record, check out the [schema extraction](/docs/using/connectors/configuration-parameters/schema-extraction) configuration parameters. ::: ## Schema Registry Conduit uses a schema registry to store the schemas of records. You can either configure Conduit to use the built-in schema registry (default) or to connect to an external standalone service exposing a REST API that's compatible with [Confluent's Schema registry](https://docs.confluent.io/current/schema-registry/develop/api.html). To use an external schema registry, you can use the following configuration options in your [Conduit configuration file](/docs/configuration#configuration-file). ```yaml schema-registry: type: confluent confluent: connection-string: http://localhost:8081 ``` Or when running Conduit from the command line you can use the following flags: ```shell $ ./conduit -schema-registry.type=confluent -schema-registry.confluent.connection-string=http://localhost:8081 ``` When using a **built-in schema registry**, schemas will be stored in the same [**persistence layer**](/docs/core-concepts/architecture#persistence-layer) as the rest of Conduit's data. By default, it uses BadgerDB, but it can be configured to use PostgreSQL, SQLite, or in-memory storage. More information on [storage](/docs/using/other-features/storage). :::note You can check the GitHub repository for the schema registry [here](https://github.com/conduitIO/conduit-schema-registry). ::: ![scarf pixel conduit-site-docs-using-other-features](https://static.scarf.sh/a.png?x-pxid=8c8ff6d5-2756-41a3-b4ca-3ca2be381842)
./1-using/6-other-features/2-dead-letter-queue.mdx:::--- title: 'Dead-letter queue' --- A dead-letter queue is a connector to which Conduit redirects records that can't be delivered or processed successfully. Any existing Conduit destination connector can be used for the purpose of a dead-letter queue. For a record to be considered processed successfully it needs to be acked. A record is acked (acknowledged) when it reaches all destinations in a pipeline or it is filtered out by a processor. Likewise, a record is nacked (negatively acknowledged) when a processor or destination return an error while processing the record. In other words, an ack indicates success and a nack indicates failure. When a record can't be delivered or processed, it is sent to the dead-letter queue connector so that it can be handled separately from the other records in the pipeline. This can be useful for debugging, since it allows you to isolate records that are causing problems and figure out why they are failing. Additionally, it can help to prevent the rest of the records in the pipeline from being blocked or delayed due to the failure of a single record. ## Nack window and threshold Rerouting records to a dead-letter queue causes the pipeline to keep running despite of nacked (negatively acknowledged) records. However, experiencing a lot of nacked records can indicate that a pipeline is corrupted or malfunctioning. In such a case it might be better to stop the pipeline to prevent further errors and investigate the problem before resuming. Conduit can monitor a fixed number of last records and track which ones are acked or nacked. As new records are processed, they are added into the window while the oldest ones are discarded to make room for them. This allows the window to "slide" through records, always containing the latest records. Conduit keeps track of the number of nacks in the sliding window and stops the pipeline if the threshold gets exceeded. This can be useful for ensuring that the pipeline is operating correctly and effectively. Below is a visual representation of a sliding window monitoring 5 records with the nack threshold set to 2. Notice that once the threshold is exceeded, the pipeline stops. ![nack window animation](/img/dlq-window-animation.gif) ## Configuring a dead-letter queue By default, a Conduit pipeline will stop immediately when a record gets nacked. You can make the pipeline more fault-tolerant by enabling a dead-letter queue. The first step is to change the window size and nack threshold: - If the window size is greater than 0 and the nack threshold is 0, then the first nacked record will cause the pipeline to stop running immediately. Such a configuration essentially disables the dead-letter queue. **This is the default configuration.** - If the window size is set to 0, then no records are monitored and all nacked records will be rerouted to the dead-letter queue without any limits. Such a configuration essentially disables the nack window and enables the dead-letter queue. - If both the window size and the nack threshold are greater than 0, then nacked records will be sent to the dead-letter queue. Conduit will keep track of the number of nacks in the current window and stop the pipeline if the threshold gets exceeded. Note that the window size needs to be greater than the nack threshold, otherwise the threshold will never be reached. If the window settings are adjusted accordingly, then Conduit will route nacked records to the builtin log connector by default, causing nacked records to show up in Conduit logs. You can adjust the connector used as a dead-letter queue to any Conduit connector you wish. Below is an example of a pipeline config file that uses the file connector as a dead-letter queue and stops the pipeline if more than 2 out of the last 5 records are nacked. ```yaml version: 2.2 pipelines: - id: dlq-example connectors: # define source and destination connectors # ... dead-letter-queue: plugin: "builtin:file" # use builtin file plugin as DLQ connector settings: path: "./dlq.out" # route records to file ./dlq.out window-size: 5 # monitor last 5 records window-nack-threshold: 2 # tolerate up to 2 negatively acknowledged records ``` Please check the [pipeline config file documentation](/docs/using/pipelines/configuration-file) for more information about configuring a pipeline. ![scarf pixel conduit-site-docs-using-other-features](https://static.scarf.sh/a.png?x-pxid=8c8ff6d5-2756-41a3-b4ca-3ca2be381842)
./1-using/6-other-features/index.mdx:::--- title: 'Other features' --- import DocCardList from '@theme/DocCardList'; <DocCardList/> ![scarf pixel conduit-site-docs-using-other-features](https://static.scarf.sh/a.png?x-pxid=8c8ff6d5-2756-41a3-b4ca-3ca2be381842)
./1-using/6-other-features/0-pipeline-recovery.mdx:::--- title: 'Pipeline Recovery' --- Pipeline Recovery is a feature in Conduit through which pipelines that experience certain types of errors are automatically restarted. This document describes how Pipeline Recovery in Conduit works and how it can be configured. :::note Pipeline Recovery is enabled by default since [Conduit v0.12](https://github.com/ConduitIO/conduit/releases/tag/v0.12.0). The feature can be [disabled](#how-to-disable-pipeline-recovery) if needed. ::: ## Introduction Most pipeline errors encountered are a result of temporary issues like network interruptions or services being unavailable due to maintenance. It then becomes a matter of how we handle the pipeline. In most cases, simply retrying is enough to get through transient errors efficiently. This can and should be done by connectors and processors, but that may not always be the case. For Conduit users, this typically means they would need to wait for the connector or processor to be updated. This implies that we need to have in Conduit an automatic mechanism for restarting pipelines that experienced an error. ## What triggers pipeline recovery Any _non-fatal_ error can trigger pipeline recovery. In the context of pipelines, we differentiate between two types of errors: fatal and non-fatal. **Fatal errors** are the errors that a pipeline cannot recover from. Two types of errors are fatal by default: 1. [DLQ threshold](/docs/using/other-features/dead-letter-queue) exceeded (because the purpose of the DLQ threshold is to stop a pipeline if too many records are nack-ed) 2. Processor errors (because processors are usually deterministic, so if a processor failed processing a record once, it will most likely fail processing a record again) :::info In one of the next versions of Conduit and the Connector SDK, we'll make it possible for connectors to define what errors they consider as fatal. ::: **Non-fatal errors** are all the other errors, i.e. errors for which it makes sense to restart a pipeline and retry the data streaming. ## Algorithm Pipeline Recovery restarts a pipeline using a linear backoff algorithm. One important addition is that Conduit tracks the number of retries over a period of time configured by the option `max-retries-window`. If `max-retries` has been set, then Conduit will make sure that there are at most `max-retries` over **any** `max-retries-window` period of time. In other words, the number of restart attempts is reset to 0 after `max-retries-window`. This way, `max-retries-window` makes sure that we can safely reset the attempts to 0 even in the following cases: 1. When a pipeline has been running long enough. For example, if a pipeline failed `max-retries - 1` times months ago, then it's unexpected that it fails now because of one failure. 2. When a pipeline is unstable and frequently failing. For example, a pipeline might experience an error and recover just before `max-retries` has been reached. It might run for some time more and then fail again. If we're not careful about resetting `max-retries`, then the pipeline might enter another recovery cycle very soon, which would be against what a user intended (limiting the number of retries). Here's a diagram of the algorithm: ![Pipeline Recovery](/img/pipeline-recovery.png) ## How to disable Pipeline Recovery To disable Pipeline Recovery, you should set the configuration parameter `pipelines.error-recovery.max-retries` to 0. With this, you'll have the pre-v0.12 behavior, where a pipeline stops and goes into the `degraded` state the first time it experiences an error. ## Configuration ### pipelines.error-recovery.min-delay - **Data Type**: [Duration](https://pkg.go.dev/time#ParseDuration) - **Required**: No - **Default**: 1s - **Description**: Minimum delay before restarting the pipeline after an error. ### pipelines.error-recovery.max-delay - **Data Type**: [Duration](https://pkg.go.dev/time#ParseDuration) - **Required**: No - **Default**: 10m - **Description**: Maximum delay allowed before restarting the pipeline after an error. ### pipelines.error-recovery.backoff-factor - **Data Type**: Integer - **Required**: No - **Default**: 2 - **Description**: Backoff factor applied to the last delay when recovering from errors. ### pipelines.error-recovery.max-retries - **Data Type**: Integer - **Required**: No - **Default**: -1 - **Description**: Maximum number of retry attempts before the pipeline gives up. A value of `-1` indicates infinite retries. ### pipelines.error-recovery.max-retries-window - **Data Type**: [Duration](https://pkg.go.dev/time#ParseDuration) - **Required**: No - **Default**: 5m - **Description**: No more than `max-retries` are allowed within any `max-retries-window` period of time. ## Examples ### Example 1: Setting a custom `max-delay` The `conduit.yaml` below shows how to set a custom maximum delay before a pipeline is restarted. By default, `max-delay` is 10 minutes and here we're setting it to 10 seconds. ```yaml pipelines: error-recovery: max-delay: "10s" ``` If a pipeline experiences an error, the delays between restarts will be as follows: * Attempt 1: 1s * Attempt 2: 2s * Attempt 3: 4s * Attempt 4: 8s * Attempt 5: 10s * Attempt 6: 10s * ... * Attempt N: 10s ## Example 2: Setting a custom `max-retries` and `max-retries-window` The `conduit.yaml` below shows how to limit the number of pipeline restarts to 3 (by default, the number of retries isn't limited). It also sets the `max-retries-window` to 10 minutes (default value is 5 minutes). ```yaml pipelines: error-recovery: max-retries: "2" max-retries-window: "10m" ``` Below, we'll explain different scenarios with failing pipelines using the same Conduit configuration. **Scenario 1: Maximum number of retries reached** **15:00** Pipeline starts. **15:10** Pipeline experiences an error (source system unavailable). **15:10** Pipeline is restarted (retry #1). **15:11** Pipeline experiences an error again. **15:11** Pipeline is restarted (retry #2). **15:15** Pipeline experiences an error again. **15:15** Pipeline has been restarted 2 times (`max-retries`) over the last 10 minutes (`max-retries-window`). No more retries are allowed, hence the pipeline stops and its status is set to `degraded`. **Scenario 2: Number of retries reset after some time** **15:00** Pipeline starts. **15:10** Pipeline experiences an error (source system unavailable). **15:10** Pipeline is restarted (retry #1). **15:11** Pipeline experiences an error again. **15:11** Pipeline is restarted (retry #2). **15:20** Internally, Conduit increments the number of available retries by 1, because 10 minutes (`max-retries-window`) since the first retry have passed. **15:35** Pipeline experiences an error again. **15:35** Pipeline is restarted because we didn't restart 2 times (`max-retries`) over the last 10 minutes (`max-retries-window`). ![scarf pixel conduit-site-docs-using-other-features](https://static.scarf.sh/a.png?x-pxid=8c8ff6d5-2756-41a3-b4ca-3ca2be381842)
./1-using/6-other-features/7-stream-inspector.mdx:::--- title: 'Stream Inspector' tag: ['Stream Inspector', 'Connector'] keywords: ['stream', 'inspector'] --- Conduit's stream inspector makes it possible to peek at the data as it enters Conduit via source connectors and what the data looks like as it travels to destination connectors. Keep in mind that this feature is about sampling data as it passes through the pipeline not tailing the pipeline. Stream inspection doesn't affect a pipeline's performance. However, if the data is being pushed through the stream very fast, it's possible that you may not see all of the records. This is done to protect Conduit's performance (by not keeping too much data in the memory) and to protect the pipeline's performance (by not blocking it until data is inspected). The inspection will **not** be automatically stopped if the connector being inspected is stopped. This makes it possible to "catch" all the records, from the moment a connector starts. It also makes it possible to inspect records as you're potentially restarting a pipeline. Stream inspection is available via the Conduit UI and API: ## UI To access the stream inspector through the UI, first navigate to the pipeline which you'd like to inspect. Then, click on the connector in which you're interested. You'll see something similar to this: ![stream inspector pipeline view](/img/stream-inspector-pipeline.png) Click the "Inspect Stream" button to start inspecting the connector. A new pop-up window will show the records: ![stream inspector show stream](/img/stream-inspector-show-stream.png) On the "Stream" tab you'll see the latest 10 records. If you switch to the "Single record" view, only the last record will be shown. You can use the "Pause" button to pause the inspector and stop receiving the latest record(s). The ones that are already shown will be kept so you can inspect them more thoroughly. ## API To access the stream inspector through the API, you'll need a WebSocket client (for example [wscat](https://github.com/websockets/wscat)). The URL on which the inspector is available comes in the following format: `ws://host:port/v1/connectors/<connector ID>/inspect`. For example, if you run Conduit locally with the default settings, you can inspect a connector by running the following command: ```shell $ wscat -c ws://localhost:8080/v1/connectors/pipeline1:destination1/inspect | jq . { "result": { "position": "NGVmNTFhMzUtMzUwMi00M2VjLWE2YjEtMzdkMDllZjRlY2U1", "operation": "OPERATION_CREATE", "metadata": { "opencdc.readAt": "1669886131666337227" }, "key": { "rawData": "NzQwYjUyYzQtOTNhOS00MTkzLTkzMmQtN2Q0OWI3NWY5YzQ3" }, "payload": { "before": { "rawData": "" }, "after": { "structuredData": { "company": "string 1d4398e3-21cf-41e0-9134-3fe012e6d1fb", "id": 1534737621, "name": "string fbc664fa-fdf2-4c5a-b656-d52cbddab671", "trial": true } } } } } ``` The above command also uses `jq` to pretty-print the output. You can also use `jq` to decode Base64-encoded strings, which may represent record positions, keys or payloads: ```shell wscat -c ws://localhost:8080/v1/connectors/pipeline1:destination1/inspect | jq '.result.key.rawData |= @base64d' ``` ![scarf pixel conduit-site-docs-using-other-features](https://static.scarf.sh/a.png?x-pxid=8c8ff6d5-2756-41a3-b4ca-3ca2be381842)
./1-using/4-connectors/5-additional-built-in-plugins.mdx:::--- title: "Adding built-in Connectors" --- Built-in connectors offer better performance when compared to standalone ones, which is why in some cases it's desirable to have a custom build of Conduit that includes additional built-in connectors. The simplest way to achieve so is to write a small application that embeds Conduit (i.e. uses Conduit as a library) and adds one or more connectors to its default configuration. In the example below we will add the [HTTP connector](https://github.com/conduitio-labs/conduit-connector-http) to Conduit as a built-in connector. First, we initialize a Go module with `go mod init github.com/conduitio-labs/custom-conduit`. Then , we need to add Conduit and the HTTP connector as dependencies: ```shell go get github.com/conduitio/conduit go get github.com/conduitio-labs/conduit-connector-http go mod tidy ``` Once that is done, we need to write a `main` function that: 1. Adds the HTTP connector the default Conduit configuration 2. Runs Conduit with the custom configuration. That's done in the code below: ```go package main import ( http "github.com/conduitio-labs/conduit-connector-http" "github.com/conduitio/conduit/pkg/conduit" ) func main() { // Get the default configuration, including all built-in connectors cfg := conduit.DefaultConfig() // Add the HTTP connector to list of built-in connectors cfg.ConnectorPlugins["http"] = http.Connector conduit.Serve(cfg) } ``` This custom version of Conduit can be built with `go build -o custom-conduit main.go`. If you run the built binary, you can check that the HTTP connector has been included in the build by listing all the connector plugins: ```shell curl 'http://localhost:8080/v1/connectors/plugins' [ { "name": "builtin:http@(devel)", "summary": "HTTP source and destination connectors for Conduit.", "description": "Conduit HTTP source and destination connectors, they connect to an HTTP URL and send HTTP requests.", "version": "(devel)", "author": "", "destinationParams": {}, "sourceParams": {} } // other plugins ] ``` ![scarf pixel conduit-site-docs-using-connectors](https://static.scarf.sh/a.png?x-pxid=53743422-c614-4e45-acaa-f1f9f604321e)
./1-using/4-connectors/0-getting-started.mdx:::--- title: 'Getting Started with Connectors' sidebar_label: "Getting Started" --- A Conduit Connector knows how to read/write records from/to a data source/destination (e.g. a database). When thinking about connectors for Conduit, our goals were to: - provide a good development experience to connector developers, - ship Conduit with real built-in connectors (compiled into the Conduit binary), - to make it as easy as possible to write plugins in _any_ programming language, - the [Connector SDK](https://github.com/conduitio/conduit-connector-sdk) to be decoupled from Conduit and be able to change without changing Conduit itself. Conduit ships with a number of built-in connectors: - [File connector](https://github.com/ConduitIO/conduit-connector-file) provides a source/destination to read/write a local file (useful for quickly trying out Conduit without additional setup). - [Kafka connector](https://github.com/ConduitIO/conduit-connector-kafka) provides a source/destination for Apache Kafka. - [Postgres connector](https://github.com/ConduitIO/conduit-connector-postgres) provides a source/destination for PostgreSQL. - [S3 connector](https://github.com/ConduitIO/conduit-connector-s3) provides a source/destination for AWS S3. - [Generator connector](https://github.com/ConduitIO/conduit-connector-generator) provides a source which generates random data (useful for testing). - [Log connector](https://github.com/ConduitIO/conduit-connector-log) provides a destination which outputs records in the Conduit logs. Besides these connectors there is a number of standalone connectors that can be added to Conduit as plugins (find the complete list [here](/docs/using/connectors/list)). Have a look at how to [install a connector](/docs/using/connectors/installing next! ![scarf pixel conduit-site-docs-using-connectors](https://static.scarf.sh/a.png?x-pxid=53743422-c614-4e45-acaa-f1f9f604321e)
./1-using/4-connectors/4-kafka-connect-connector.mdx:::--- title: "Kafka Connect Connectors with Conduit" --- # Using Kafka Connect Connectors with Conduit The [Conduit Kafka Connect Wrapper connector](https://github.com/ConduitIO/conduit-kafka-connect-wrapper) is a special connector that allows you to use [Kafka Connect](https://docs.confluent.io/platform/current/connect/index.html) connectors with Conduit. Conduit doesn't come bundled with Kafka Connect connectors, but you can use it to bring any Kafka Connect connector with Conduit. This connector gives you the ability to: - Easily migrate from Kafka Connect to Conduit. - Remove Kafka as a dependency to move data between data infrastructure. - Leverage a datastore if Conduit doesn't have a native connector. Since the Conduit Kafka Connect Wrapper itself is written in Java, but most of Conduit's connectors are written in Go, it also serves as a good example of the flexibility of the Conduit Plugin SDK. Let's begin. ## How it works To use the Kafka Connect wrapper connector, you'll need to: 1. Download the [`conduit-kafka-connect-wrapper`](https://github.com/ConduitIO/conduit-kafka-connect-wrapper) latest release, at the time of writing this, we'll go with [v0.4.3](https://github.com/ConduitIO/conduit-kafka-connect-wrapper/releases/tag/v0.4.3) and download the file `conduit-kafka-connect-wrapper-v0.4.3.zip`. 1. Download Kafka Connect JARs and any dependencies you would like to add. 1. Create a pipeline configuration file. 1. Run Conduit. ## Setup To begin, download the [`conduit-kafka-connect-wrapper`](https://github.com/ConduitIO/conduit-kafka-connect-wrapper) [latest release](https://github.com/ConduitIO/conduit-kafka-connect-wrapper/releases), at the time of writing this, we'll go with `v0.4.3` and download the file `conduit-kafka-connect-wrapper-v0.4.3.zip`. Downloading the release is our **preferred** option to get the connector JAR. However, another option to get the JAR file is to build it from source, this could be useful in case you added your own changes to the connector and wanted to test them out, to do that we will clone the connector: ``` git clone git@github.com:ConduitIO/conduit-kafka-connect-wrapper.git ``` Then, we need to build the connector. The Kafka Connect wrapper connector is written in Java, so it needs to be compiled. ```bash cd conduit-kafka-connect-wrapper ./scripts/dist.sh ``` Running `scripts/dist.sh` will create a directory called `dist` with following contents: 1. A script (which runs the connector). This script starts a connector instance. 1. Directory `libs`. This is where you add the connector JAR itself, and put the Kafka connector JARs and their dependencies (if any). Now that we have everything setup, we can add the Kafka Connect connectors. ## Download a connector and its dependencies The `libs` directory is where you put the Kafka Connect connector JARs and their dependencies (if any). The wrapper plugin will automatically load connectors and all the other dependencies from a `libs` directory. To download a connector from a Maven repository and all of its dependencies, you can use `scripts/download-connector.sh`. For example: ```shell ./scripts/download-connector.sh io.example jdbc-connector 2.1.3 ``` For usage, run `./scripts/download-connector.sh --help`. You can also download them manually if needed, we will use the PostgreSQL Kafka Connect JDBC Connector. To install, add the following: - [Aiven's Kafka Connect JDBC Connectors](https://github.com/aiven/jdbc-connector-for-apache-kafka) - [Postgres Connector JAR](https://repo1.maven.org/maven2/org/postgresql/postgresql/42.3.3/postgresql-42.3.3.jar) This connector allows you to connect to any [JDBC database](https://en.wikipedia.org/wiki/Java_Database_Connectivity). ## Create Pipeline Configuration File Now that the Kafka Connect connectors included in `lib`, we can use it in a pipeline configuration file. 1. [Install Conduit](https://github.com/ConduitIO/conduit#installation-guide). 2. Create a pipeline configuration file: Create a folder called `pipelines` at the same level as your Conduit binary. Inside of that folder create a file named `jdbc-to-file.yml`, check [Specifications](https://conduit.io/docs/using/pipelines/configuration-file) for more details about Pipeline Configuration Files. ````yaml version: 2.0 pipelines: - id: kafka-connect-pipeline status: running description: This pipeline is for testing connectors: - id: jdbc-kafka-connect type: source plugin: standalone:conduit-kafka-connect-wrapper settings: wrapper.connector.class: "io.aiven.connect.jdbc.JdbcSourceConnector", connection.url: "jdbc:postgresql://localhost/conduit-test-db", connection.user: "username", connection.password: "password", incrementing.column.name: "id", mode: "incrementing", tables: "customers", topic.prefix: "my_topic_prefix" - id: file-dest type: destination plugin: builtin:file settings: path: "path/to/the/file.txt" ```` 3. Run Conduit! And see how simple it is to migrate to Conduit. Note that the `wrapper.connector.class` should be a class which is present on the classpath, i.e. in one of the JARs in the `libs` directory. For more information, check the [Wrapper Configuration](https://github.com/ConduitIO/conduit-kafka-connect-wrapper#configuration) section. ![scarf pixel conduit-site-docs-using-connectors](https://static.scarf.sh/a.png?x-pxid=53743422-c614-4e45-acaa-f1f9f604321e)
./1-using/4-connectors/2-referencing.mdx:::--- title: "Referencing Connectors" --- The name, used to reference a connector plugin in API requests or a pipeline configuration file, is using the following format: `[PLUGIN-TYPE:]PLUGIN-NAME[@VERSION]` - `PLUGIN-TYPE` (`builtin`, `standalone` or `any`) - Defines if the specified plugin should be builtin or standalone. - If `any`, Conduit will use a standalone plugin if it exists and fall back to a builtin plugin. - Default is `any`. - `PLUGIN-NAME` - Defines the name of the plugin as specified in the plugin specifications, it has to be an exact match. - `VERSION` - Defines the plugin version as specified in the plugin specifications, it has to be an exact match. - If `latest`, Conduit will use the latest semantic version. - Default is `latest`. ## Examples - `postgres` - will use the **latest** **standalone** **postgres** plugin - will fallback to the **latest** **builtin** **postgres** plugin if standalone wasn't found - `postgres@v0.2.0` - will use the **standalone** **postgres** plugin with version **v0.2.0** - will fallback to a **builtin** **postgres** plugin with version **v0.2.0** if standalone wasn't found - `builtin:postgres` - will use the **latest** **builtin** **postgres** plugin - `standalone:postgres@v0.3.0` - will use the **standalone** **postgres** plugin with version **v0.3.0** (no fallback to builtin) ![scarf pixel conduit-site-docs-using-connectors](https://static.scarf.sh/a.png?x-pxid=53743422-c614-4e45-acaa-f1f9f604321e)
./1-using/4-connectors/1-installing.mdx:::--- title: "Installing Connectors" --- Beside the built-in connectors shipped with Conduit there is a [list](/docs/using/connectors/list) of connectors that can be added to Conduit as plugins. These are called standalone connectors. To install a standalone connector you first need the compiled connector binary. A binary can normally be downloaded from the latest release in the connector's GitHub repository (this may vary in 3rd party connectors not developed by the Conduit team). Make sure to download the binary that matches your operating system and architecture. Alternatively you can build the binary yourself (for instructions on building a connector please refer to the readme of that specific connector). ## Installing a Connector in Conduit Conduit loads standalone connectors at startup. The connector binaries need to be placed in the `connectors` directory relative to the Conduit binary so Conduit can find them. Alternatively, the path to the standalone connectors can be adjusted using the CLI flag `-connectors.path`, for example: ```shell ./conduit -connectors.path=/path/to/connectors/ ``` Names of the connector binaries are not important, since Conduit is getting the information about connectors from connectors themselves (using their gRPC API). Find out how to [reference your connector](/docs/using/connectors/referencing). ![scarf pixel conduit-site-docs-using-connectors](https://static.scarf.sh/a.png?x-pxid=53743422-c614-4e45-acaa-f1f9f604321e)
./1-using/4-connectors/index.mdx:::--- title: "Connectors" --- import DocCardList from '@theme/DocCardList'; <DocCardList/> ![scarf pixel conduit-site-docs-using-connectors](https://static.scarf.sh/a.png?x-pxid=53743422-c614-4e45-acaa-f1f9f604321e)
./1-using/4-connectors/6-configuration-parameters/1-rate-limiting.mdx:::--- title: "Rate Limiting" --- Destination connectors can be configured to limit the rate at which records can be written. This is especially useful when the destination resource has a rate limit to ensure that the connector does not exceed it. By default, Conduit does not limit the rate at which records are written. ## Configuration parameters * `sdk.rate.perSecond`: Maximum number of records written per second (0 means no rate limit). * `sdk.rate.burst`: Allow bursts of at most X records (0 or less means that bursts are not limited). Only takes effect if a rate limit per second is set. Note that if `sdk.batch.size` is bigger than `sdk.rate.burst`, the effective batch size will be equal to `sdk.rate.burst`. ## Example The pipeline will generate structured records as fast as possible, and write them to the log. When you run it, you'll notice that by using both `sdk.rate.perSecond` and `sdk.rate.burst`, the log destination connector will limit the rate of processed records 8 records per second in batches of 2 (i.e. 2 records every 250ms). ```yaml version: 2.2 pipelines: - id: generator-to-log status: running description: > Example pipeline using the generator source connector and the log destination connector. Showing how to limit the rate of processing records. connectors: - id: example type: source plugin: generator settings: rate: 0 # generating source records as fast as possible format.type: structured format.options.id: int format.options.name: string operations: create - id: log type: destination plugin: log settings: level: info # limit the rate of proccessing records sdk.rate.perSecond: 8 sdk.rate.burst: 2 ``` ![scarf pixel conduit-site-docs-using-connectors](https://static.scarf.sh/a.png?x-pxid=53743422-c614-4e45-acaa-f1f9f604321e)
./1-using/4-connectors/6-configuration-parameters/2-schema-extraction.mdx:::--- title: "Schema Extraction" --- Source and destination connectors can be configured to automatically extract the schema from the key and payload of a record. This is especially useful when the data is structured and the schema is known in advance. By default, Conduit extracts the schema from the key and the payload of a record and encodes them using the extracted schema. ## Configuration parameters These are the configuration parameters that control schema extraction on a source connector (Note that `sdk.schema.extract.payload.enabled` and `sdk.schema.extract.key.enabled` are also available on destination connectors): * `sdk.schema.extract.type`: The type of schema extraction to perform. Supported value is `avro`. * `sdk.schema.extract.payload.enabled`: A boolean value that indicates whether the payload should be extracted. * `sdk.schema.extract.payload.subject`: The subject of the payload schema. * `sdk.schema.extract.key.enabled`: A boolean value that indicates whether the key should be extracted. * `sdk.schema.extract.key.subject`: The subject of the key schema. :::caution `sdk.schema.extract.payload.enabled` and `sdk.schema.extract.key.enabled` should be set to `false` when producing raw (not structured) data, as shown in the example below. If you are developing a connector, you can disable this automatically by updating the connector's default middleware. For more information about `NewSource()` when developing a source connector, see [here](/docs/developing/connectors/developing-source-connectors/#newsource). ::: ## Example The below pipeline will generate a single record and write it to a file. Notice that it's configured so that the generator source does not extract the schema or encode the data. ```yaml version: "2.2" pipelines: - id: generator-to-file status: running name: generator-to-file description: Generates a single record, no schema generated, writes to file connectors: - id: file-src type: source plugin: builtin:generator name: file-src settings: recordCount: "1" collections.users.format.type: structured collections.users.format.options.id: int collections.users.format.options.name: string sdk.schema.extract.payload.enabled: false sdk.schema.extract.key.enabled: false - id: file-dest type: destination plugin: builtin:file name: file-dest settings: path: /tmp/file-destination.txt ``` When the pipeline is run, `/tmp/file-destination.txt` will contain output similar to this: ```json { "position": "MQ==", "operation": "create", "metadata": { "conduit.source.connector.id": "generator-to-file:file-src", "opencdc.collection": "users", "opencdc.createdAt": "1723046776830339829" }, "key": "c2F1cm9wc2lkYW4=", "payload": { "before": null, "after": { "id": 7819649577989235000, "name": "Iambe" } } } ``` Notice that the written record doesn't contain any schema information in its metadata. However, if you leave the schema extraction enabled, then you'll see something below in the record's metadata: ``` "opencdc.payload.schema.subject": "generator-to-file:file-src:users.payload", "opencdc.payload.schema.version": "1" ``` :::tip To learn more about **Schema Support**, check out [this page](/docs/using/other-features/schema-support). ::: ![scarf pixel conduit-site-docs-using-connectors](https://static.scarf.sh/a.png?x-pxid=53743422-c614-4e45-acaa-f1f9f604321e)
./1-using/4-connectors/6-configuration-parameters/3-batching.mdx:::--- title: "Batching" --- Destination connectors can be configured to process records in batches. This is especially useful when the destination resource can receive multiple records in a single round-trip. By default, Conduit processes records one by one. Enabling batch processing can improve the performance of the connector, as it reduces the number of round-trips to the destination resource. However, it can also increase the memory usage of the connector, as it needs to store multiple records in memory before flushing the batch. It can also increase the latency of the connector, as it needs to wait for the batch to be full. ## Configuration parameters There are two connector configuration parameters which control the batch size: * `sdk.batch.size`: used to configure the number of records to be sent in a single batch. Default value is `1`. * `sdk.batch.delay`: used to configure the maximum time to wait for a batch to be full before sending it to the destination resource. Default value is `0`. ## Examples ### Example 1: Batch size The following pipeline is configured to process batches of 100 records when writing to the destination resource. Note that the source connector is generating records at a rate of 10 records per second, meaning that records will be flushed approximately every 10 seconds. ```yaml version: 2.2 pipelines: - id: pipeline1 status: running name: pipeline1 description: 'A pipeline batching 100 records at a time.' connectors: - id: source1 type: source plugin: builtin:generator name: source1 settings: rate: 10 operations: "create" format.type: "structured" format.options.name: "string" format.options.company: "string" - id: destination1 type: destination plugin: "builtin:file" name: destination1 settings: sdk.batch.size: 100 path: /tmp/file-destination.txt ``` ### Example 2: Batch delay The following pipeline is configured to collect records for 5 seconds before flushing the batch to the destination resource. Note that the source connector is generating records at a rate of 10 records per second, meaning that a batch will contain approximately 50 records. ```yaml version: 2.2 pipelines: - id: pipeline1 status: running name: pipeline1 description: 'A pipeline batching 100 records at a time.' connectors: - id: source1 type: source plugin: builtin:generator name: source1 settings: rate: 10 operations: "create" format.type: "structured" format.options.name: "string" format.options.company: "string" - id: destination1 type: destination plugin: "builtin:file" name: destination1 settings: sdk.batch.delay: "5s" path: /tmp/file-destination.txt ``` ### Example 3: Batch size and delay The following pipeline is configured to collect batches of 100 records for up to 5 seconds before flushing them to the destination resource. This means that records will be flushed at most every 5 seconds, or sooner if the batch collects 100 records. ```yaml version: 2.2 pipelines: - id: pipeline1 status: running name: pipeline1 description: 'A pipeline batching 100 records at a time.' connectors: - id: source1 type: source plugin: builtin:generator name: source1 settings: rate: 10 operations: "create" format.type: "structured" format.options.name: "string" format.options.company: "string" - id: destination1 type: destination plugin: "builtin:file" name: destination1 settings: sdk.batch.size: 100 sdk.batch.delay: "5s" path: /tmp/file-destination.txt ``` ![scarf pixel conduit-site-docs-using-connectors](https://static.scarf.sh/a.png?x-pxid=53743422-c614-4e45-acaa-f1f9f604321e)
./1-using/4-connectors/6-configuration-parameters/0-output-format.mdx:::--- title: "Output Format" --- One of the challenges to be solved when integrating Conduit with other systems, such as Kafka Connect and Debezium, is the data format. This is present in situations where raw data needs to be written, for example when writing messages to Kafka. By default, Conduit uses the [OpenCDC format](/docs/using/opencdc-record). Conduit also makes it possible to change the output format so that the data can be consumed by other systems. :::note Configuring different output formats is possible for connectors which use Conduit SDK's default middleware. Please consult the connector's documentation to confirm the feature is available. ::: The output format is configured at the level of a destination connector. This implies that different connectors in the same pipeline can use different formats. ## Configuration parameters There are two connector configuration parameters which control the output format: * `sdk.record.format`: used to choose the format itself * `sdk.record.format.options`: used to configure the specifics of the chosen format. ## Available formats Most formats come in the form of `name/encoding`. `name` is the name of the format and through that we control the structure of the output. `encoding` is the encoding used for the structure created by the converter. Currently, a `json` encoding is supported. More encoders are planned in future (e.g. Avro, Parquet, ...) ### `opencdc/json` This is the default format. An extensive description of this format is available [here](https://github.com/ConduitIO/conduit/blob/main/docs/design-documents/20220309-opencdc.md). #### **Options** None. ### `debezium/json` Generates output compatible with Debezium connectors. #### **Options** ##### `debezium.schema.name` (Optional) Name of schema to be used in converted Debezium records. **Default value**: none ##### `debezium.rawData.key` (Optional) If the record to be converted doesn't contain a structured payload or its payload cannot be parsed as JSON, then this key defines the field into which the "raw" payload data will be put. **Default value**: `opencdc.rawData` ### `template` The user gets full control over the output format by using [Go templates](https://pkg.go.dev/text/template). The value provided to the template is a [sdk.Record](https://github.com/ConduitIO/conduit-connector-sdk/blob/e08068428247af6e3b34bef8abffad2b758d5529/record.go#L81), so the template has access to all its fields (e.g. `.Position`, `.Key`, `.Metadata`, and so on). We also inject all template functions provided by [sprig](http://masterminds.github.io/sprig/) to make it easier to write templates. #### **Options** The only option for this formatter is Go template itself, as described [here](https://pkg.go.dev/text/template). ## Examples ### Example 1: A pipeline with a Debezium record formatter The following pipeline has a generator source and a file destination. We use the `debezium/json` record formatter so that the file destination outputs the records in the Debezium format. Additionally, we set the schema name to be `employee` by setting the format options to `debezium.schema.name=employee`. ```yaml version: 2.2 pipelines: - id: pipeline1 status: running name: pipeline1 description: 'A pipeline with a Debezium record formatter' connectors: - id: source1 type: source plugin: builtin:generator name: source1 settings: recordCount: "10" readTime: "1s" format.options: "name:string,company:string" format.type: "structured" - id: destination1 type: destination plugin: "builtin:file" name: destination1 settings: sdk.record.format: "debezium/json" sdk.record.format.options: 'debezium.schema.name=employee' path: /tmp/file-destination.txt ``` If you run this pipeline and `tail` the destination file, you'll see something like this: ```json { "schema": { "type": "struct", "name": "employee", "fields": [ // omitted for brevity ] }, "payload": { "before": null, "after": { "company": "string 3cea28f1-8ec2-4035-b4d3-790a9239feb3", "name": "string 8fbbbf0c-aaaa-499f-b145-ac5345c8c067" }, "source": { "conduit.source.connector.id": "pipeline1:source1", "opencdc.readAt": "1680796794653155968", "opencdc.version": "v1" }, "op": "c", "ts_ms": 1680796794653, "transaction": null } } ``` ### Example 2: A pipeline with a template record formatter The following pipeline has a generator source and a file destination. The generator source produces records which have a `name` field and a `company` field. In this pipeline, we use the `template` record formatter to extract only the `id` field and then prefix it with `employee ID`. ```yaml version: 2.2 pipelines: - id: pipeline1 status: running name: pipeline1 description: 'A pipeline with a template record formatter' connectors: - id: source1 type: source plugin: builtin:generator name: source1 settings: recordCount: "3" readTime: "1s" format.options: "id:int,name:string,company:string" format.type: "structured" - id: destination1 type: destination plugin: "builtin:file" name: destination1 settings: sdk.record.format: template # print only the payload, format it as a string (normally it's a byte slice) sdk.record.format.options: '{{ printf "employee ID %d" .Payload.After.id }}' path: /tmp/file-destination.txt ``` If you run Conduit and then `tail` the destination file, you'll see something like this: ```shell employee ID 636561854 employee ID 1848322815 employee ID 123675258 ``` ![scarf pixel conduit-site-docs-using-connectors](https://static.scarf.sh/a.png?x-pxid=53743422-c614-4e45-acaa-f1f9f604321e)
./1-using/4-connectors/6-configuration-parameters/index.mdx:::--- title: 'Configuration Parameters' --- import DocCardList from '@theme/DocCardList'; When configuring a connector, you can use the following parameters: <DocCardList/> ![scarf pixel conduit-site-docs-using-connectors](https://static.scarf.sh/a.png?x-pxid=53743422-c614-4e45-acaa-f1f9f604321e)
./1-using/4-connectors/7-list/0-kafka.mdx:::--- IMPORTANT: This file was generated using src/connectorgen/main.go. DO NOT EDIT. title: "kafka" description: "A Kafka source and destination plugin for Conduit, written in Go." --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import GitHubIcon from '@mui/icons-material/GitHub'; import IconButton from '@mui/material/IconButton'; import Stack from '@mui/material/Stack'; import Link from '@mui/material/Link'; import StarIcon from '@mui/icons-material/Star'; import Tooltip from '@mui/material/Tooltip'; # kafka <Stack className='align-items-center' direction='row' justifyContent='flex-start' spacing={2} > <IconButton size='large' href="https://github.com/hariso/conduit-connector-kafka" target='_blank' > <GitHubIcon fontSize='inherit' /> </IconButton> <Chip icon={<StarIcon />} label="0" size='large' /> </Stack> ## Author Meroxa, Inc. ## Latest release - [conduit-connector-kafka_0.22.0_Linux_x86_64.tar.gz](https://conduit.gateway.scarf.sh/connector/download/hariso/conduit-connector-kafka/releases/download/v0.22.0/conduit-connector-kafka_0.22.0_Linux_x86_64.tar.gz) ## Description This is the description of this connector. You can use _markdown_ ..... ## Source Parameters ```yaml version: 2.2 pipelines: - id: example status: running connectors: - id: example-source type: source plugin: "kafka" name: example-source settings: # CACert is the Kafka broker's certificate. # Type: string caCert: "" # ClientCert is the Kafka client's certificate. # Type: string clientCert: "" # ClientID is a unique identifier for client connections established # by this connector. # Type: string clientID: "conduit-connector-kafka" # ClientKey is the Kafka client's private key. # Type: string clientKey: "" # GroupID defines the consumer group id. # Type: string groupID: "" # InsecureSkipVerify defines whether to validate the broker's # certificate chain and host name. If 'true', accepts any certificate # presented by the server and any host name in that certificate. # Type: bool insecureSkipVerify: "" # ReadFromBeginning determines from whence the consumer group should # begin consuming when it finds a partition without a committed # offset. If this options is set to true it will start with the first # message in that partition. # Type: bool readFromBeginning: "" # RetryGroupJoinErrors determines whether the connector will # continually retry on group join errors. # Type: bool retryGroupJoinErrors: "true" # Mechanism configures the connector to use SASL authentication. If # empty, no authentication will be performed. # Type: string saslMechanism: "" # Password sets up the password used with SASL authentication. # Type: string saslPassword: "" # Username sets up the username used with SASL authentication. # Type: string saslUsername: "" # Maximum delay before an incomplete batch is read from the source. # Type: duration sdk.batch.delay: "0" # Maximum size of batch before it gets read from the source. # Type: int sdk.batch.size: "0" # Specifies whether to use a schema context name. If set to false, no # schema context name will be used, and schemas will be saved with the # subject name specified in the connector (not safe because of name # conflicts). # Type: bool sdk.schema.context.enabled: "true" # Schema context name to be used. Used as a prefix for all schema # subject names. If empty, defaults to the connector ID. # Type: string sdk.schema.context.name: "" # Whether to extract and encode the record key with a schema. # Type: bool sdk.schema.extract.key.enabled: "false" # The subject of the key schema. If the record metadata contains the # field "opencdc.collection" it is prepended to the subject name and # separated with a dot. # Type: string sdk.schema.extract.key.subject: "key" # Whether to extract and encode the record payload with a schema. # Type: bool sdk.schema.extract.payload.enabled: "false" # The subject of the payload schema. If the record metadata contains # the field "opencdc.collection" it is prepended to the subject name # and separated with a dot. # Type: string sdk.schema.extract.payload.subject: "payload" # The type of the payload schema. # Type: string sdk.schema.extract.type: "avro" # Servers is a list of Kafka bootstrap servers, which will be used to # discover all the servers in a cluster. # Type: string servers: "" # TLSEnabled defines whether TLS is needed to communicate with the # Kafka cluster. # Type: bool tls.enabled: "" # Topics is a comma separated list of Kafka topics to read from. # Type: string topics: "" ``` ## Destination Parameters ```yaml version: 2.2 pipelines: - id: example status: running connectors: - id: example-destination type: destination plugin: "kafka" name: example-destination settings: # Acks defines the number of acknowledges from partition replicas # required before receiving a response to a produce request. None = # fire and forget, one = wait for the leader to acknowledge the # writes, all = wait for the full ISR to acknowledge the writes. # Type: string acks: "all" # BatchBytes limits the maximum size of a request in bytes before # being sent to a partition. This mirrors Kafka's max.message.bytes. # Type: int batchBytes: "1000012" # CACert is the Kafka broker's certificate. # Type: string caCert: "" # ClientCert is the Kafka client's certificate. # Type: string clientCert: "" # ClientID is a unique identifier for client connections established # by this connector. # Type: string clientID: "conduit-connector-kafka" # ClientKey is the Kafka client's private key. # Type: string clientKey: "" # Compression set the compression codec to be used to compress # messages. # Type: string compression: "snappy" # DeliveryTimeout for write operation performed by the Writer. # Type: duration deliveryTimeout: "" # InsecureSkipVerify defines whether to validate the broker's # certificate chain and host name. If 'true', accepts any certificate # presented by the server and any host name in that certificate. # Type: bool insecureSkipVerify: "" # Mechanism configures the connector to use SASL authentication. If # empty, no authentication will be performed. # Type: string saslMechanism: "" # Password sets up the password used with SASL authentication. # Type: string saslPassword: "" # Username sets up the username used with SASL authentication. # Type: string saslUsername: "" # Maximum delay before an incomplete batch is written to the # destination. # Type: duration sdk.batch.delay: "0" # Maximum size of batch before it gets written to the destination. # Type: int sdk.batch.size: "0" # Allow bursts of at most X records (0 or less means that bursts are # not limited). Only takes effect if a rate limit per second is set. # Note that if `sdk.batch.size` is bigger than `sdk.rate.burst`, the # effective batch size will be equal to `sdk.rate.burst`. # Type: int sdk.rate.burst: "0" # Maximum number of records written per second (0 means no rate # limit). # Type: float sdk.rate.perSecond: "0" # The format of the output record. See the Conduit documentation for a # full list of supported formats # (https://conduit.io/docs/using/connectors/configuration-parameters/output-format). # Type: string sdk.record.format: "opencdc/json" # Options to configure the chosen output record format. Options are # normally key=value pairs separated with comma (e.g. # opt1=val2,opt2=val2), except for the `template` record format, where # options are a Go template. # Type: string sdk.record.format.options: "" # Whether to extract and decode the record key with a schema. # Type: bool sdk.schema.extract.key.enabled: "true" # Whether to extract and decode the record payload with a schema. # Type: bool sdk.schema.extract.payload.enabled: "true" # Servers is a list of Kafka bootstrap servers, which will be used to # discover all the servers in a cluster. # Type: string servers: "" # TLSEnabled defines whether TLS is needed to communicate with the # Kafka cluster. # Type: bool tls.enabled: "" # Topic is the Kafka topic. It can contain a [Go # template](https://pkg.go.dev/text/template) that will be executed # for each record to determine the topic. By default, the topic is the # value of the `opencdc.collection` metadata field. # Type: string topic: "{{ index .Metadata "opencdc.collection" }}" ```
./1-using/4-connectors/7-list/1-foo.mdx:::--- IMPORTANT: This file was generated using src/connectorgen/main.go. DO NOT EDIT. title: "foo" description: "describe your connector" --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import GitHubIcon from '@mui/icons-material/GitHub'; import IconButton from '@mui/material/IconButton'; import Stack from '@mui/material/Stack'; import Link from '@mui/material/Link'; import StarIcon from '@mui/icons-material/Star'; import Tooltip from '@mui/material/Tooltip'; # foo <Stack className='align-items-center' direction='row' justifyContent='flex-start' spacing={2} > <IconButton size='large' href="https://github.com/hariso/conduit-connector-foo" target='_blank' > <GitHubIcon fontSize='inherit' /> </IconButton> <Chip icon={<StarIcon />} label="0" size='large' /> </Stack> ## Author your name ## Latest release - [conduit-connector-foo_0.6.0_Darwin_arm64.tar.gz](https://conduit.gateway.scarf.sh/connector/download/hariso/conduit-connector-foo/releases/download/v0.6.0/conduit-connector-foo_0.6.0_Darwin_arm64.tar.gz) - [conduit-connector-foo_0.6.0_Darwin_x86_64.tar.gz](https://conduit.gateway.scarf.sh/connector/download/hariso/conduit-connector-foo/releases/download/v0.6.0/conduit-connector-foo_0.6.0_Darwin_x86_64.tar.gz) - [conduit-connector-foo_0.6.0_Linux_arm64.tar.gz](https://conduit.gateway.scarf.sh/connector/download/hariso/conduit-connector-foo/releases/download/v0.6.0/conduit-connector-foo_0.6.0_Linux_arm64.tar.gz) - [conduit-connector-foo_0.6.0_Linux_i386.tar.gz](https://conduit.gateway.scarf.sh/connector/download/hariso/conduit-connector-foo/releases/download/v0.6.0/conduit-connector-foo_0.6.0_Linux_i386.tar.gz) - [conduit-connector-foo_0.6.0_Linux_x86_64.tar.gz](https://conduit.gateway.scarf.sh/connector/download/hariso/conduit-connector-foo/releases/download/v0.6.0/conduit-connector-foo_0.6.0_Linux_x86_64.tar.gz) - [conduit-connector-foo_0.6.0_Windows_arm64.tar.gz](https://conduit.gateway.scarf.sh/connector/download/hariso/conduit-connector-foo/releases/download/v0.6.0/conduit-connector-foo_0.6.0_Windows_arm64.tar.gz) - [conduit-connector-foo_0.6.0_Windows_i386.tar.gz](https://conduit.gateway.scarf.sh/connector/download/hariso/conduit-connector-foo/releases/download/v0.6.0/conduit-connector-foo_0.6.0_Windows_i386.tar.gz) - [conduit-connector-foo_0.6.0_Windows_x86_64.tar.gz](https://conduit.gateway.scarf.sh/connector/download/hariso/conduit-connector-foo/releases/download/v0.6.0/conduit-connector-foo_0.6.0_Windows_x86_64.tar.gz) ## Description describe your connector in detail ## Source Parameters ```yaml version: 2.2 pipelines: - id: example status: running connectors: - id: example-source type: source plugin: "foo" name: example-source settings: # SourceConfigParam is named foo and must be provided by the user. # Type: string foo: "" # GlobalConfigParam is named global_config_param_name and needs to be # provided by the user. # Type: string global_config_param_name: "" # Maximum delay before an incomplete batch is read from the source. # Type: duration sdk.batch.delay: "0" # Maximum size of batch before it gets read from the source. # Type: int sdk.batch.size: "0" # Specifies whether to use a schema context name. If set to false, no # schema context name will be used, and schemas will be saved with the # subject name specified in the connector (not safe because of name # conflicts). # Type: bool sdk.schema.context.enabled: "true" # Schema context name to be used. Used as a prefix for all schema # subject names. If empty, defaults to the connector ID. # Type: string sdk.schema.context.name: "" # Whether to extract and encode the record key with a schema. # Type: bool sdk.schema.extract.key.enabled: "true" # The subject of the key schema. If the record metadata contains the # field "opencdc.collection" it is prepended to the subject name and # separated with a dot. # Type: string sdk.schema.extract.key.subject: "key" # Whether to extract and encode the record payload with a schema. # Type: bool sdk.schema.extract.payload.enabled: "true" # The subject of the payload schema. If the record metadata contains # the field "opencdc.collection" it is prepended to the subject name # and separated with a dot. # Type: string sdk.schema.extract.payload.subject: "payload" # The type of the payload schema. # Type: string sdk.schema.extract.type: "avro" ``` ## Destination Parameters Writing data with foo is not supported.
./1-using/4-connectors/7-list/index.mdx:::--- title: "Connector List" --- The Conduit team and our community of developers are always adding new connectors. Don't see the connector you need below? Help us prioritize by creating new issues, commenting, or leaving a `+1` on our [source](https://github.com/ConduitIO/conduit/issues?q=is%3Aissue+label%3Aconnector%3Asource+is%3Aopen) and [destination](https://github.com/ConduitIO/conduit/issues?q=is%3Aissue+label%3Aconnector%3Adestination+is%3Aopen) connector lists. Don't have time to wait? You can get started [building your own](/docs/developing/connectors/) in no time. ![scarf pixel conduit-site-docs-using-connectors](https://static.scarf.sh/a.png?x-pxid=53743422-c614-4e45-acaa-f1f9f604321e) ### Built-in vs Standalone A Conduit connector can run in one of two ways: _built-in_ or _standalone_. Built-in refers to connectors that are compiled into the Conduit binary, while standalone refers to connectors that run separately from Conduit. You can learn more about standalone vs built-in connectors on our [Connector Behavior](/docs/developing/connectors/behavior) page. A small set of connectors are built into Conduit by default. For those connectors no additional setup is required and you can start using them in Conduit right away. ### Connector Types Source means the connector has the ability to get data from an upstream data store. Destination means the connector can write to a downstream data store. import ConnectorList from '@site/src/components/ConnectorList'; import { useEffect, useState } from "react" import useBaseUrl from '@docusaurus/useBaseUrl'; export const Connectors = ({url}) => { const [connectors, setConnectors] = useState([]) const [loading, setLoading] = useState(true); const [error, setError] = useState(null); useEffect(() => { fetch(url) .then(response => response.json() ) .then(data => { setConnectors(data); setLoading(false); }) .catch(err => { setError(err); setLoading(false); }); }, []); if (loading) { return <p>Loading list...</p>; } if (error) { return <p>Error: {error.message}</p>; } return <ConnectorList connectors={connectors} /> } <Connectors url={useBaseUrl('/connectors.json')} />
./1-using/7-guides/build-generator-to-log-pipeline.mdx:::--- title: "How to build a generator-to-log pipeline" --- In this guide, we'll learn how to build a basic Conduit pipeline. It will use two built-in connectors: - A generator source (a source connector that generates random data). Our pipeline will generate imaginary employee data. - A logging destination (a destination that simply logs all incoming records). As you'll see, all it takes to accomplish this is to write a [pipeline configuration file](/docs/using/pipelines/configuration-file)! The steps below will guide you through installing Conduit and gradually building the pipeline. If you're in a hurry, you can skip ahead and [run the pipeline](#step-6-run-the-pipeline) immediately. ## Step 1: Install Conduit Download the [latest Conduit release](https://github.com/ConduitIO/conduit/releases/latest) to a directory of your choice. In this guide, the directory will be `~/conduit-playground`. ## Step 2: Create a pipeline configuration file Pipelines can be created through YAML configuration files, that are, by default, put into a directory called `pipelines`. With the following command, we'll create the `pipelines` directory and a file that will contain our pipeline's configuration: ```bash mkdir pipelines && touch pipelines/generator-to-log.yaml ``` The directory layout should be as below: ```console ~/conduit-playground ├── conduit └── pipelines └── generator-to-log.yaml ``` ## Step 3: The pipeline's basics Next, write the following to `generator-to-log.yaml`: ```yaml version: "2.2" pipelines: - id: pipeline1 # Tells Conduit to run the pipeline automatically status: running name: pipeline1 description: A generator-to-log pipeline ``` ## Step 4: Add the generator source Now we can start adding connectors to the pipeline. Add a source connector under `connectors` in the configuration file: ```yaml connectors: - id: source1 type: source plugin: builtin:generator settings: format.type: structured format.options.id: int format.options.name: string rate: "2s" ``` The fields `id`, `type`, `plugin` and `settings` are found in every connector. Every connector has its own set of configuration parameters. The generator's configuration can be found [here](https://github.com/ConduitIO/conduit-connector-generator?tab=readme-ov-file#configuration). The configuration above instructs the generator to produce a record every 2 seconds, each with a structured payload. The payload will contain two fields: `id` (an integer) and `name` (a string). ## Step 5: Add the logging destination The last piece in the configuration is the destination connector: ```yaml - id: destination1 type: destination plugin: builtin:log ``` We're fine with the log connector's default [configuration](https://github.com/ConduitIO/conduit-connector-log?tab=readme-ov-file#configuration), so we're leaving out the `settings` field. ## Step 6: Run the pipeline To summarize the steps from above, we have the following directory structure: ```console ~/conduit-playground ├── conduit └── pipelines └── generator-to-log.yaml ``` The file `generator-to-log.yaml` has the following content: ```yaml version: "2.2" pipelines: - id: pipeline1 status: running name: pipeline1 description: A generator-to-log pipeline connectors: - id: source1 type: source plugin: builtin:generator settings: format.type: structured format.options.id: int format.options.name: string rate: "2s" - id: destination1 type: destination plugin: builtin:log ``` Now you can run Conduit: ```shell ./conduit .... .::::::::::. .:::::‘‘‘‘:::::. .:::: ::::. .:::::::: ::::::::. `:::::::: ::::::::‘ `:::: ::::‘ `:::::....:::::‘ `::::::::::‘ Conduit v0.12.2 linux/amd64 ‘‘‘‘ 2024-09-19T10:34:54+00:00 INF All 1 tables opened in 0s component=badger.DB 2024-09-19T10:34:54+00:00 INF Discard stats nextEmptySlot: 0 component=badger.DB 2024-09-19T10:34:54+00:00 INF Set nextTxnTs to 5 component=badger.DB 2024-09-19T10:34:54+00:00 INF loading processor plugins from directory ~/conduit-playground/processors ... component=plugin.processor.standalone.Registry ``` You'll also notice the following line: ```console 2024-09-19T10:34:54+00:00 INF pipeline configs provisioned component=provisioning.Service created=["pipeline1"] deleted=[] pipelines_path=./pipelines ``` that confirms our pipeline was loaded. So, where do we see the results? We're using the log connector, so our test data will be in the logs: ```console 2024-09-19T10:34:54+00:00 INF component=plugin connector_id=pipeline1:destination1 plugin_name=builtin:log plugin_type=destination record={"key":"Y29iYWx0aWM=","metadata":{"conduit.source.connector.id":"pipeline1:source1","opencdc.createdAt":"1726734894822378094","opencdc.payload.schema.subject":"pipeline1:source1:payload","opencdc.payload.schema.version":"1"},"operation":"create","payload":{"after":{"id":8433756117589358088,"name":"suspicious"},"before":null},"position":"MQ=="} ``` Congratulations on your first pipeline! You may want to learn more about [pipeline configuration files](/docs/using/pipelines/configuration-file) or other [features](/docs/using/other-features of Conduit.
./1-using/7-guides/index.mdx:::--- title: "Guides" --- import DocCardList from '@theme/DocCardList'; <DocCardList/>
./1-using/3-pipelines/0-configuration-file.mdx:::--- title: 'Pipeline Configuration File' sidebar_label: 'Configuration File' toc_max_heading_level: 6 --- :::info This document describes configuration files with version 2.2. Configurations with version 1.x are deprecated, but can still be used with newer versions of Conduit. ::: :::warning If you have a pipeline configuration file with version 1.x and multiple processors please update it to version 2.2 to ensure that processors are ordered correctly. ::: The pipeline configuration file is a YAML file with a specific structure, that allows you to configure one or more pipelines. All pipelines, defined in the configuration file, will be provisioned in the order in which they are defined. You can get a quick overview of the fields by looking at the following example: ```yaml title="example_pipeline.yml" version: 2.2 # Parser version pipelines: # A list of pipeline configurations - id: pipeline1 # Pipeline ID [required] status: running # Pipeline status at startup (running or stopped) name: pipeline1-name # Pipeline name description: desc # Pipeline description connectors: # A list of connector configurations - id: con1 # Connector ID [required] type: source # Connector type (source or destination) [required] plugin: builtin:file # Connector plugin [required] name: my-file-source # Connector name settings: # A map of configuration keys and values for the plugin (specific to the chosen plugin) path: ./file1.txt # This property is specific to the file plugin - id: con2 type: destination plugin: builtin:file name: my-file-destination settings: path: ./file2.txt processors: # A list of processor configurations, processors are attached to the connector - id: proc1 # Processor ID [required] plugin: custom.javascript # Processor plugin name [required] condition: '{{ eq .Metadata.foo "bar" }}' # Condition (Go template expression) that dictates if the record will be passed to the processor or not. workers: 2 # Number of parallel workers settings: # A map of configuration keys and values for the processor (specific to the chosen processor plugin) script: > function process(record) { return record; // pass through } processors: # A list of processor configurations, processors are attached to the pipeline - id: proc2 # Processor ID [required] plugin: field.set # Processor type [required] settings: # A map of configuration keys and values for the processor (specific to the chosen processor plugin) field: .Payload.After.key value: ${ENV_VAR} # You can use environment variables by wrapping them in a dollar sign and curly braces ${} dead-letter-queue: # Dead-letter queue (DLQ) configuration plugin: "builtin:file" # DLQ Connector plugin settings: # A map of configuration keys and values for the plugin (specific to the chosen plugin) path: "./dlq.out" window-size: 5 # DLQ nack window size window-nack-threshold: 2 # DLQ nack window threshold ``` ## version - **Data Type**: String - **Required**: No - **Default**: latest (`2.2`) - **Allowed Values**: `1.0`, `1.1`, `2.0`, `2.1` , `2.2` - **Description**: Version defines the schema version for the pipeline configuration file and controls which parser is used to decode it. ## pipelines - **Data Type**: Sequence node containing [`pipeline`](#pipeline) nodes - **Required**: Yes - **Default**: None - **Description**: The node contains a list of [`pipeline`](#pipeline) definitions. Pipelines will be provisioned in the order in which they are defined. ## pipeline - **Data Type**: Mapping node - **Required**: n/a - **Default**: None - **Description**: This node contains the configuration of a single pipeline. ### id - **Data Type**: String - **Required**: Yes - **Default**: None - **Allowed Values**: Strings containing alphanumeric characters (letters `A-Z`, both uppercase and lowercase, and digits `0-9`), as well as hyphens `-`, underscores `_`, colons `:`, and periods `.`, with a *max length* of *128* characters. - **Description**: ID of the [`pipeline`](#pipeline). It should uniquely identify the pipeline across all pipelines. If multiple pipelines use the same ID, none of them will be provisioned. **Warning**: Changing this property will cause the pipeline to be recreated and start from the beginning. ### status - **Data Type**: String - **Required**: No - **Default**: `stopped` - **Allowed Values**: `stopped`, `running` - **Description**: This field controls the status of the pipeline at startup. If set to `running` the pipeline will be automatically started, if set to `stopped` the pipeline will be provisioned without being started. ### name - **Data Type**: String - **Required**: No - **Default**: Same as pipeline [`id`](#id) - **Allowed Values**: Strings with a length limit of *128* characters. - **Description**: Human readable name for the pipeline. Needs to be unique across all pipelines (it is used as a label in pipeline [metrics](/docs/using/other-features/metrics)). ### description - **Data Type**: String - **Required**: No - **Default**: None - **Allowed Values**: Strings with a length limit of *8192* characters. - **Description**: Human readable description of the pipeline. ### connectors - **Data Type**: Sequence node containing [`connector`](#connector) nodes - **Required**: No - **Default**: None - **Description**: The node contains a list of [`connector`](#connector) definitions. The order of the connectors has no effect on the provisioned pipeline. ### processors - **Data Type**: Sequence node containing [`processor`](#processor) nodes - **Required**: No - **Default**: None - **Description**: The node contains a list of [`processor`](#processor) definitions. These processors process all records flowing through the pipeline. They are executed after source processors and before destination processors. The processors are executed in the order in which they are defined in the list. ### dead-letter-queue - **Data Type**: Mapping node - **Required**: No - **Default**: See sub-fields - **Description**: This node contains the dead-letter queue configuration. Read more about [dead-letter queues](/docs/using/other-features/dead-letter-queue) in Conduit. #### plugin - **Data Type**: String - **Required**: No - **Default**: `builtin:log` - **Description**: This node references the destination connector plugin used for storing dead-letters. See how to [reference a connector](/docs/using/connectors/referencing). #### settings - **Data Type**: Mapping node - **Required**: No - **Default**: `{"level": "warn", "message": "record delivery failed"}` - **Description**: A map of configuration keys and values for the dead-letter queue connector. These settings depend on the value of the dead-letter queue [`plugin`](#plugin). Check the chosen destination connector's documentation for a list of settings it supports and requires. #### window-size - **Data Type**: Integer - **Required**: No - **Default**: `1` - **Description**: Defines the nack window size. See [dead-letter queue](/docs/using/other-features/dead-letter-queue#nack-window-and-threshold). #### window-nack-threshold - **Data Type**: Integer - **Required**: No - **Default**: `0` - **Description**: Defines the nack window threshold. See [dead-letter queue](/docs/using/other-features/dead-letter-queue#nack-window-and-threshold). ## connector - **Data Type**: Mapping node - **Required**: n/a - **Default**: None - **Description**: This node contains the configuration of a single connector. ### id - **Data Type**: String - **Required**: Yes - **Default**: None - **Allowed Values**: Strings containing alphanumeric characters (letters `A-Z`, both uppercase and lowercase, and digits `0-9`), as well as hyphens `-`, underscores `_`, colons `:`, and periods `.`, with a *max length* of *256* characters. - **Description**: ID of the [`connector`](#connector). It should uniquely identify the connector inside the pipeline. If multiple connectors inside the pipeline use the same ID, provisioning will fail. **Warning**: Changing this property will cause the connector to be recreated and start from the beginning. ### type - **Data Type**: String - **Required**: Yes - **Default**: None - **Allowed Values**: `source`, `destination` - **Description**: Defines if the connector is a source or a destination. A valid pipeline needs at least one source and one destination. **Warning**: Changing this property will cause the connector to be recreated and start from the beginning. ### plugin - **Data Type**: String - **Required**: Yes - **Default**: None - **Description**: This node references the connector plugin. See how to [reference a connector](/docs/using/connectors/referencing). **Warning**: Changing this property will cause the connector to start from the beginning. ### name - **Data Type**: String - **Required**: No - **Default**: Same as connector [`id`](#id-1) - **Allowed Values**: Strings with a length limit of *256* characters. - **Description**: Human readable name for the connector. ### settings - **Data Type**: Mapping node - **Required**: Yes - **Default**: None - **Description**: A map of configuration keys and values for the connector plugin. These settings depend on the value of connector [`type`](#type) and [`plugin`](#plugin-1). Check the chosen connector's documentation for a list of settings it supports and requires. ### processors - **Data Type**: Sequence node containing [`processor`](#processor) nodes - **Required**: No - **Default**: None - **Description**: The node contains a list of [`processor`](#processor) definitions. These processors only process records coming from or flowing to a source or destination. The processors are executed in the order in which they are defined in the list. ## processor - **Data Type**: Mapping node - **Required**: n/a - **Default**: None - **Description**: This node contains the configuration of a single processor. ### id - **Data Type**: String - **Required**: Yes - **Default**: None - **Description**: ID of the [`processor`](#processor). It should uniquely identify the processor inside the pipeline. If multiple processors inside the pipeline use the same ID, provisioning will fail. ### plugin - **Data Type**: String - **Required**: Yes - **Default**: None - **Description**: Defines the processor's plugin name (e.g. `field.set`). Check out the [processors documentation](/docs/using/processors/builtin) to find the list of builtin processors we provide. ### condition - **Data Type**: String - **Required**: No - **Default**: `true` (all records will be passed to the processor) - **Description**: A [Go template expression](https://pkg.go.dev/text/template) that will be used as a condition to pass the record to the processor or not. The Go template expression will be evaluated using each record and needs to produce `true` or `false`. If it produces `true` the record will be passed to the processor, `false` means it will continue down the pipeline without getting processed. ### settings - **Data Type**: Mapping node - **Required**: Yes - **Default**: None - **Description**: A map of configuration keys and values for the processor. The values in this map depend on the chosen processor [`plugin`](#plugin-2). ### workers - **Data Type**: Integer - **Required**: No - **Default**: `1` - **Description**: Defines the number of workers that should execute this processor in parallel. The number needs to be larger than 0. ![scarf pixel conduit-site-docs-using-pipelines](https://static.scarf.sh/a.png?x-pxid=4f41012a-be75-4b82-8112-86a7cb40f98a)
./1-using/3-pipelines/1-provisioning.mdx:::--- title: 'Provisioning' --- This document describes how provisioning of pipeline configuration files works in Conduit. To see how a pipeline configuration file is structured check the [specifications](/docs/using/pipelines/configuration-file). ## Conduit provisions pipelines at startup Conduit provisions all pipeline configuration files found in the [pipelines directory](#pipelines-directory) when it starts. After that it does not monitor the configuration files for changes. If you want to apply changes to one of the configurations you need to restart Conduit for the changes to take effect. ## Pipelines Directory When Conduit starts, it will search for all files ending with `.yml` or `.yaml` in folder `./pipelines` and its sub-folders. It will parse the configuration files and provision the pipelines defined in them. If you want Conduit to search a different directory, or want to provision a single file, you can specify the CLI flag `pipelines.path` and point to your file or directory: ```shell ./conduit -pipelines.path /path/to/my-directory ``` If the directory does not exist, Conduit will fail with the error `"pipelines.path" config value is invalid`. ## Errors are logged and ignored Conduit will try to provision all pipelines regardless of the errors it may encounter while going through the configuration files. Invalid configuration files will be ignored (e.g. files that miss a mandatory field or have an invalid configuration value) and won't impact the provisioning of valid configuration files. Conduit will log a warning for every error it encounters during this process. ## Pipelines need unique IDs You need to be careful when choosing IDs for your pipelines, connectors and processors. Each pipeline needs to get a unique pipeline ID, otherwise all pipelines with the same ID will be ignored. If a pipeline contains connectors or processors with the same ID, they will be ignored as well. **_Note_**: Connector IDs and processor IDs will be prefixed with the parent ID. In case of a connector this means the pipeline ID will be attached as a prefix to the connector ID (e.g. `pipelineID:connectorID`). Similarly, a pipeline processor's ID will be prefixed with the pipeline ID (e.g. `pipelineID:processorID`) and a connector processor's ID will be prefixed with the connector ID (e.g. `pipelineID:connectorID:processorID`). This ensures that connector and processor IDs are globally unique as long as the pipeline ID is unique. ## Environment variables Conduit injects environment variables in pipeline configuration files. To specify a value using an environment variable, wrap the name of the variable in `${}` (e.g. `${MY_ENV_VAR}`). Any value in the configuration file can be injected using an environment variable. This can be useful for injecting secrets into a pipeline without specifying it explicitly in the pipeline configuration file. You need to make sure that the environment variable is set when running Conduit. ## Pipelines Immutability Pipelines provisioned by configuration files are **immutable**, they can not be changed using the API. Any updates on a provisioned pipeline have to be done through the configuration file. You can still stop or start the pipeline through the UI or API. ## Updating a provisioned pipeline Pipelines provisioned by configuration files can only be updated through configuration files. Conduit has to be restarted for the changes to take effect. Generally, updates will preserve the state of the pipeline. This means that even after an update Conduit will resume the processing at the last record that was successfully processed. However, updates of some fields will cause the state to be lost, as it can't be guaranteed that the state is still applicable to the new configuration. In that case the pipeline will start processing records from the beginning, as if it's running for the first time. Here is a full list of fields that will cause the connector to start from the beginning if they are updated: - [`pipeline.id`](/docs/using/pipelines/configuration-file#id) - The entire pipeline will be recreated and all source connectors will start from the beginning. - [`connector.id`](/docs/using/pipelines/configuration-file#id-1) - The updated connector will start from the beginning (only source connectors). - [`connector.type`](/docs/using/pipelines/configuration-file#type) - The updated connector will start from the beginning (only source connectors). - [`connector.plugin`](/docs/using/pipelines/configuration-file#plugin-1) - The updated connector will start from the beginning (only source connectors). ## Deleting a provisioned pipeline Pipelines provisioned by configuration files can be deleted by removing them from the configuration file or deleting the file entirely. Next time Conduit starts it will delete the pipeline. ![scarf pixel conduit-site-docs-using-pipelines](https://static.scarf.sh/a.png?x-pxid=4f41012a-be75-4b82-8112-86a7cb40f98a)
./1-using/3-pipelines/2-statuses.mdx:::--- title: 'Statuses' --- Understanding the different statuses of your pipeline is crucial for effective monitoring and management. Below are the various statuses that a pipeline can have in Conduit. ## Statuses Overview | Status | Description | |----------------|---------------------------------------------------------------------------------------------| | **Running** | The pipeline is actively processing data and functioning as expected. | | **Stopped** | The pipeline has been stopped by the system or manually by a user shutting down gracefully. | | **Degraded** | The pipeline has been stopped due to an error or force-stopped by a user. | | **Recovering** | The pipeline is in the process of recovering from a degraded state or error. | ## Status Descriptions ### Running - **Definition**: The pipeline is currently active and processing data without any issues. - **Implication**: All systems are functioning normally, and data is flowing as expected. ### Stopped - **Definition**: The pipeline has been halted by the system, or manually stopped by a user. - **Implication**: The pipeline gracefully stopped, any in-flight records were flushed, acks were delivered back to the source, no error was encountered while stopping the pipeline. The pipeline could be restarted. ### Degraded - **Definition**: The pipeline stopped due to an error that couldn't be recovered, or force-stopped by the user. - **Implication**: Any in-flight records were dropped, not all acks might have been delivered back to the source. Starting a degraded pipeline again could potentially result in duplicated data (depending on how the destination connector handles records), since some data might get re-delivered. ### Recovering - **Definition**: The pipeline is in the process of recovering from a degraded state or error. - **Implication**: The system is attempting to restore normal operations. Monitor the status to ensure recovery is successful. ![scarf pixel conduit-site-docs-using-pipelines](https://static.scarf.sh/a.png?x-pxid=4f41012a-be75-4b82-8112-86a7cb40f98a)
./1-using/3-pipelines/index.mdx:::--- title: "Pipelines" --- import DocCardList from '@theme/DocCardList'; <DocCardList/> ![scarf pixel conduit-site-docs-using-pipelines](https://static.scarf.sh/a.png?x-pxid=4f41012a-be75-4b82-8112-86a7cb40f98a)
./1-using/5-processors/4-processor-concurrency.mdx:::--- title: 'Processor Concurrency' --- Conduit allows you to process records concurrently using the processor's `workers` parameter. This can be particularly beneficial for processors performing time-consuming computations on a fast stream of records. :::tip Ordering guarantee When processors are configured to process records concurrently, they guarantee that the order of records won't change. Although records may be processed out of order, the resulting order of records coming out of the processor is guaranteed to be unchanged. ::: Consider a pipeline that needs to process 100 records per second. One of the processors in the pipeline needs to retrieve data from a server, and this operation typically takes 1 second to complete. Since this operation is executed for every record, sequential processing won't keep up with the stream. However, by configuring the processor to use 100 workers, we can execute the slow operation concurrently on 100 records at a time. This enables us to process 100 records per second. Note that in reality we would want to configure an even higher number of workers to provide some room for handling larger bursts. ## Configuration You can configure a processor to run concurrently by specifying the number of workers. By default, the number of workers is set to 1, which means that records are processed sequentially. Here is an example pipeline configuration that configures the `js` processor to run in parallel using 10 workers: ```yaml version: 2.2 pipelines: - id: example-pipeline connectors: # define source and destination connectors # ... processors: - id: example-js type: js workers: 10 # configure 10 workers that run concurrently settings: script: > function process(record) { record.Metadata["foo-key"] = "foo-value"; return record; } ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/3-referencing-fields.mdx:::--- title: 'Referencing Record Fields' --- Many builtin processors can be configured to work on a specific field in an [OpenCDC record](/docs/using/opencdc-record). That is done through _field references_, strings that describe the path to a field in a record. That can be a field within an OpenCDC record (such as the metadata or the payload), but it can also be a [nested field](#accessing-nested-fields). To reference one of the OpenCDC record fields, you can use a similar notation to accessing fields in a [Go template](https://pkg.go.dev/text/template) executed on an [`opencdc.Record`](https://github.com/ConduitIO/conduit-commons/blob/v0.1.1/opencdc/record.go#L30) value. For example, `.Metadata` will reference the field named `Metadata`. :::tip[Why do record fields start with an uppercase letter?] The main record fields start with an uppercase letter, because they are public fields in the Go type [`opencdc.Record`](https://github.com/ConduitIO/conduit-commons/blob/v0.1.1/opencdc/record.go#L30) which is used to resolve field references (e.g. `Metadata`, `Position`, `Operation`, `Key`, `Payload`). ::: ## Accessing Nested Fields Nested fields can be accessed using two different notations: the **dot notation** and the **bracket notation**: * The **dot notation** is used to access fields containing only alphanumeric characters. For example, the reference `.Metadata.foo` will access the field named `foo` in the `Metadata` field. * The **bracket notation** is used to access fields containing non-alphanumeric characters. For example, the reference `.Metadata["opencdc.readAt"]` will access the field named `opencdc.readAt` in the `Metadata` field. ## Examples Below is an example OpenCDC record (left) and the field references that can be used to access the fields in the record (right): <table style={{backgroundColor: "#282A36"}}> <tr> <td> ```json title="Example OpenCDC record" { "position": "c3RhbmRpbmc=", "operation": "update", "metadata": { "foo": "bar", "opencdc.readAt": "1663858188836816000" }, "key": "cGFkbG9jay1rZXk=", "payload": { "before": "eWVsbG93", "after": { "boolField": true, "nested": { "non-alphanumeric key!": "baz" } } } } ``` </td> <td> ```json title="Field references per line" . .Position .Operation .Metadata .Metadata.foo / .Metadata["foo"] .Metadata["opencdc.readAt"] .Key .Payload .Payload.Before .Payload.After .Payload.After.boolField / .Payload.After["boolField"] .Payload.After.nested / .Payload.After["nested"] .Payload.After.nested["non-alphanumeric key!"] / .Payload.After["nested"]["non-alphanumeric key!"] ``` </td> </tr> </table> A few things to note: * `.` references the entire record (i.e. the JSON object above). * Fields `.Key` and `.Payload.Before` contain raw data (i.e. byte arrays), which is represented as a base64-encoded string. We can not reference nested fields in raw data. However, if the raw data is first parsed into structured data (e.g. if it's a JSON string we can use the [`json.decode`](/docs/using/processors/builtin/json.decode) processor), then we can reference the fields in the structured data. * `.Metadata["opencdc.readAt"]` references the metadata field `opencdc.readAt` using the bracket notation. The dot notation (i.e. `.Metadata.opencdc.readAt`) cannot be used here because the referenced key `opencdc.readAt` contains a non-alphanumeric character (the dot). * Note that references to fields nested inside `.Key`, `.Payload.Before` and `.Payload.After` **do not** start with an uppercase letter, because these fields are not part of the [`opencdc.Record`](https://github.com/ConduitIO/conduit-commons/blob/v0.1.1/opencdc/record.go#L30) type. They are referenced by their actual names, as they appear in JSON. ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/0-getting-started.mdx:::--- title: 'Getting Started with Processors' sidebar_label: 'Getting Started' --- A processor is a component that operates on a single record that flows through a pipeline. It can either **transform** the record, or **filter** it out based on some criteria. Since they are part of pipelines, making yourself familiar with [pipeline semantics](/docs/core-concepts/pipeline-semantics) is highly recommended. ![Pipeline](/img/pipeline_example.svg) Processors are **optional** components in a pipeline (i.e. a pipeline can be started without them), and they are always attached to a single parent, which can be either a connector or a pipeline: - **Connector processors**: - **Source processors** only receive messages originating at a specific source connector. Source processors are created by specifying the corresponding source connector as the parent entity. - **Destination processors** only receive messages that are meant to be sent to a specific destination connector. Destination processors are created by specifying the corresponding destination connector as the parent entity. - **Pipeline processors** receive all messages that flow through the pipeline, regardless of the source or destination. Pipeline processors are created by specifying the pipeline as the parent entity. ## Processor types When it comes to using a processor, Conduit supports different types: - [Built-in processors](/docs/using/processors/builtin) will perform the most common operations you could expect such as filtering fields, replacing fields, posting payloads to a HTTP endpoint, etc. These are already coming as part of Conduit, and you can simply start using them with a bit of configuration. [Check out this document to see everything that's available](/docs/using/processors/builtin). - [Standalone processors](/docs/developing/processors) are the ones you could write yourself to do anything that's not already covered by the [Built-in](/docs/using/processors/builtin) ones. [Here's](/docs/developing/processors) more information about them. ## How to use a processor In these following examples, we're using the [`json.decode`](/docs/using/processors/builtin/json.decode), but you could use any other you'd like from our [Built-in](/docs/using/processors/builtin/) ones, or even [reference](/docs/using/processors/referencing) your own [Standalone processor](/docs/developing/processors). :::info When referencing the name of a processor plugin there are different ways you can make sure you're using the one you'd like. Please, check out the [Referencing Processors](/docs/using/processors/referencing) documentation for more information. ::: ### Using a [pipeline configuration file](/docs/using/pipelines/configuration-file) #### Using a pipeline processor Creating a pipeline processor through a pipeline configuration file can be done as below: ```yaml version: 2.2 pipelines: - id: example-pipeline connectors: # define source and destination connectors # ... processors: - id: extract-name plugin: json.decode settings: field: name ``` #### Using a connector processor Similarly, we can configure a connector processor, i.e. a processor attached to a connector: ```yaml version: 2.2 pipelines: - id: example-pipeline connectors: - id: conn1 # other connector configuration processors: - id: extract-name plugin: json.decode settings: field: name # other connectors ``` The documentation about how to configure processors in pipeline configuration files can be found [here](/docs/using/pipelines/configuration-file#processor). ### Using the [HTTP API](/api#get-/v1/processors) The processor endpoints live under the `/v1/processors` namespace, and to attach a processor to either connector or a pipeline, you could do a `POST` request to `/v1/processors` specifying `parent.type` as `TYPE_PIPELINE` or `TYPE_CONNECTOR`. Default value is `TYPE_UNSPECIFIED`. [Here's](/api#post-/v1/processors) how the entire request could look like. :::tip To list all the different API HTTP requests you could perform check out our [HTTP API](/api). These are also described in our [api.swagger.json](https://github.com/ConduitIO/conduit/blob/main/pkg/web/openapi/swagger-ui/api/v1/api.swagger.json). ::: ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/5-referencing.mdx:::--- title: 'Referencing Processors' --- The name, used to reference a processor plugin in a pipeline configuration file or API requests, is using the following format: `[PLUGIN-TYPE:]PLUGIN-NAME[@VERSION]` - `PLUGIN-TYPE` (`builtin`, `standalone` or `any`) - Defines if the specified plugin should be builtin or standalone. - If `any`, Conduit will use a standalone plugin if it exists and fall back to a builtin plugin. - Default is `any`. - `PLUGIN-NAME` - Defines the name of the plugin as specified in the plugin specifications, it has to be an exact match. - `VERSION` - Defines the plugin version as specified in the plugin specifications, it has to be an exact match. - If `latest`, Conduit will use the latest semantic version. - Default is `latest`. ## Examples - `base64.decode` - will use the **latest** **standalone** **base64.decode** plugin - will fall back to the **latest** **builtin** **base64.decode** plugin if the standalone plugin wasn't found - `base64.decode@v0.2.0` - will use the **standalone** **base64.decode** plugin with version **v0.2.0** - will fall back to a **builtin** **base64.decode** plugin with version **v0.2.0** if the standalone plugin wasn't found - `builtin:base64.decode` - will use the **latest** **builtin** **base64.decode** plugin - `standalone:base64.decode@v0.3.0` - will use the **standalone** **base64.decode** plugin with version **v0.3.0** (no fallback to builtin) :::info Read more about how to register [standalone processors](/docs/developing/processors) in Conduit. ::: ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/index.mdx:::--- title: "Processors" --- import DocCardList from '@theme/DocCardList'; <DocCardList/> ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/2-conditions.mdx:::--- title: 'Conditional Execution' --- When a [processor](/docs/using/connectors/getting-started) is attached to a connector or to a pipeline, we may still want to specify conditions for its execution. To do this, we can add a `condition` key to the processor definition in the [Pipeline Configuration File](/docs/using/processors/getting-started#using-a-pipeline-configuration-file), or using the [HTTP API](/docs/using/processors/getting-started#using-the-http-api) including this parameter in the [POST request](/api#post-/v1/processors). <div style={{textAlign: 'center'}}> ![Processor condition](/img/processor-condition.svg) </div> The `condition` key follows the [Go templates](https://pkg.go.dev/text/template) format, allowing the use of any function provided by [sprig](https://masterminds.github.io/sprig/). If the expression evaluates to **true**, the processor will be executed; otherwise, the record will continue to flow in the pipeline without being processed by this processor. :::note Conduit will parse the output of the go template using [`strconv.ParseBool`](https://pkg.go.dev/strconv#ParseBool) which means that `1`, `t`, `T`, `TRUE`, `true` and `True` will all evaluate to `true`, while `0`, `f`, `F`, `FALSE`, `false`, `False` will be evaluated to `false`. If an expression can't be evaluated as a boolean, the processor will return an error. ::: ## Example of a condition Here's an example of a simple condition. In this case, records **will** be processed by the [`json.decode` builtin processsor](/docs/using/processors/builtin/json.decode) when he [OpenCDC Metadata](/docs/using/opencdc-record#fields) contains a key named `key` which value is equal to `expected-value`. ```yaml version: 2.2 pipelines: - id: example-pipeline connectors: # define source and destination connectors # ... processors: - id: extract-name plugin: json.decode settings: field: .Payload.After.name condition: "{{ eq .Metadata.key \"expected-value\" }}" ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/error.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'error' sidebar_position: 5 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `error` Returns an error for all records that get passed to the processor. ## Description Any time a record is passed to this processor it returns an error, which results in the record being sent to the DLQ if it's configured, or the pipeline stopping. **Important:** Make sure to add a [condition](https://conduit.io/docs/using/processors/conditions) to this processor, otherwise all records will trigger an error. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "error" settings: # Error message to be returned. This can be a Go # [template](https://pkg.go.dev/text/template) executed on each # [`Record`](https://pkg.go.dev/github.com/conduitio/conduit-commons/opencdc#Record) # being processed. # Type: string message: "error processor triggered" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "false" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "false" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "false" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "false" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`message`</td> <td>string</td> <td>`error processor triggered`</td> <td> Error message to be returned. This can be a Go [template](https://pkg.go.dev/text/template) executed on each [`Record`](https://pkg.go.dev/github.com/conduitio/conduit-commons/opencdc#Record) being processed. </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`false`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`false`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`false`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`false`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Error record with custom error message This example shows how to configure the error processor to return a custom error message for a record using a Go template. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "error" settings: message: "custom error message with data from record: {{.Metadata.foo}}" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`message`</td> <td>`custom error message with data from record: {{.Metadata.foo}}`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": {\n \"foo\": \"bar\"\n },\n \"key\": null,\n \"payload\": {\n \"before\": {\n \"bar\": \"baz\"\n },\n \"after\": {\n \"foo\": \"bar\"\n }\n }\n}"} newValue={"{\n \"error\": \"custom error message with data from record: bar\"\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/base64.decode.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'base64.decode' sidebar_position: 2 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `base64.decode` Decode a field to base64. ## Description The processor will decode the value of the target field from base64 and store the result in the target field. It is not allowed to decode the `.Position` field. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "base64.decode" settings: # Field is the reference to the target field. Note that it is not # allowed to base64 decode the `.Position` field. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string field: "" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`field`</td> <td>string</td> <td><Chip label="null" /></td> <td> Field is the reference to the target field. Note that it is not allowed to base64 decode the `.Position` field. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Decode a base64 encoded string This example decodes the base64 encoded string stored in `.Payload.After`. Note that the result is a string, so if you want to further process the result (e.g. parse the string as JSON), you need to chain other processors (e.g. [`json.decode`](/docs/using/processors/builtin/json.decode)). #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "base64.decode" settings: field: ".Payload.After.foo" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Payload.After.foo`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": \"dGVzdC1wb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": \"test-key\",\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"foo\": \"YmFy\"\n }\n }\n}"} newValue={"{\n \"position\": \"dGVzdC1wb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": \"test-key\",\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"foo\": \"bar\"\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/avro.encode.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'avro.encode' sidebar_position: 1 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `avro.encode` Encodes a record's field into the Avro format. ## Description The processor takes a record's field and encodes it using a schema into the [Avro format](https://avro.apache.org/). It provides two strategies for determining the schema: * **preRegistered** (recommended) This strategy downloads an existing schema from the schema registry and uses it to encode the record. This requires the schema to already be registered in the schema registry. The schema is downloaded only once and cached locally. * **autoRegister** (for development purposes) This strategy infers the schema by inspecting the structured data and registers it in the schema registry. If the record schema is known in advance it's recommended to use the preRegistered strategy and manually register the schema, as this strategy comes with limitations. The strategy uses reflection to traverse the structured data of each record and determine the type of each field. If a specific field is set to nil the processor won't have enough information to determine the type and will default to a nullable string. Because of this it is not guaranteed that two records with the same structure produce the same schema or even a backwards compatible schema. The processor registers each inferred schema in the schema registry with the same subject, therefore the schema compatibility checks need to be disabled for this schema to prevent failures. If the schema subject does not exist before running this processor, it will automatically set the correct compatibility settings in the schema registry. This processor is the counterpart to [`avro.decode`](/docs/using/processors/builtin/avro.decode). ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "avro.encode" settings: # The field that will be encoded. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string field: ".Payload.After" # The subject name under which the inferred schema will be registered # in the schema registry. # Type: string schema.autoRegister.subject: "" # The subject of the schema in the schema registry used to encode the # record. # Type: string schema.preRegistered.subject: "" # The version of the schema in the schema registry used to encode the # record. # Type: int schema.preRegistered.version: "" # Strategy to use to determine the schema for the record. Available # strategies are: * `preRegistered` (recommended) - Download an # existing schema from the schema registry. This strategy is further # configured with options starting with `schema.preRegistered.*`. * # `autoRegister` (for development purposes) - Infer the schema from # the record and register it in the schema registry. This strategy is # further configured with options starting with # `schema.autoRegister.*`. # For more information about the behavior of each strategy read the # main processor description. # Type: string schema.strategy: "" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`field`</td> <td>string</td> <td>`.Payload.After`</td> <td> The field that will be encoded. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`schema.autoRegister.subject`</td> <td>string</td> <td><Chip label="null" /></td> <td> The subject name under which the inferred schema will be registered in the schema registry. </td> </tr> <tr> <td>`schema.preRegistered.subject`</td> <td>string</td> <td><Chip label="null" /></td> <td> The subject of the schema in the schema registry used to encode the record. </td> </tr> <tr> <td>`schema.preRegistered.version`</td> <td>int</td> <td><Chip label="null" /></td> <td> The version of the schema in the schema registry used to encode the record. </td> </tr> <tr> <td>`schema.strategy`</td> <td>string</td> <td><Chip label="null" /></td> <td> Strategy to use to determine the schema for the record. Available strategies are: * `preRegistered` (recommended) - Download an existing schema from the schema registry. This strategy is further configured with options starting with `schema.preRegistered.*`. * `autoRegister` (for development purposes) - Infer the schema from the record and register it in the schema registry. This strategy is further configured with options starting with `schema.autoRegister.*`. For more information about the behavior of each strategy read the main processor description. </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Auto-register schema This example shows the usage of the `avro.encode` processor with the `autoRegister` schema strategy. The processor encodes the record's `.Payload.After` field using the schema that is extracted from the data and registered on the fly under the subject `example-autoRegister`. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "avro.encode" settings: field: ".Payload.After" schema.autoRegister.subject: "example-autoRegister" schema.strategy: "autoRegister" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Payload.After`</td> </tr> <tr> <td>`schema.autoRegister.subject`</td> <td>`example-autoRegister`</td> </tr> <tr> <td>`schema.strategy`</td> <td>`autoRegister`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": \"dGVzdC1wb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": null,\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"myFloat\": 2.3,\n \"myInt\": 1,\n \"myMap\": {\n \"bar\": 2.2,\n \"foo\": true\n },\n \"myString\": \"bar\",\n \"myStruct\": {\n \"bar\": false,\n \"foo\": 1\n }\n }\n }\n}"} newValue={"{\n \"position\": \"dGVzdC1wb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": null,\n \"payload\": {\n \"before\": null,\n \"after\": \"\\u0000\\u0000\\u0000\\u0000\\u0001ffffff\\u0002@\\u0002������\\u0001@\\u0001\\u0006bar\\u0000\\u0002\"\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` --- ### Pre-register schema This example shows the usage of the `avro.encode` processor with the `preRegistered` schema strategy. When using this strategy, the schema has to be manually pre-registered. In this example we use the following schema: ```json { "type":"record", "name":"record", "fields":[ {"name":"myString","type":"string"}, {"name":"myInt","type":"int"} ] } ``` The processor encodes the record's`.Key` field using the above schema. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "avro.encode" settings: field: ".Key" schema.preRegistered.subject: "example-preRegistered" schema.preRegistered.version: "1" schema.strategy: "preRegistered" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Key`</td> </tr> <tr> <td>`schema.preRegistered.subject`</td> <td>`example-preRegistered`</td> </tr> <tr> <td>`schema.preRegistered.version`</td> <td>`1`</td> </tr> <tr> <td>`schema.strategy`</td> <td>`preRegistered`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": \"dGVzdC1wb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": {\n \"myInt\": 1,\n \"myString\": \"bar\"\n },\n \"payload\": {\n \"before\": null,\n \"after\": null\n }\n}"} newValue={"{\n \"position\": \"dGVzdC1wb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": \"\\u0000\\u0000\\u0000\\u0000\\u0001\\u0006bar\\u0002\",\n \"payload\": {\n \"before\": null,\n \"after\": null\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/json.decode.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'json.decode' sidebar_position: 11 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `json.decode` Decodes a specific field from JSON raw data (string) to structured data. ## Description The processor takes JSON raw data (`string` or `[]byte`) from the target field, parses it as JSON structured data and stores the decoded structured data in the target field. This processor is only applicable to fields under `.Key`, `.Payload`.Before and `.Payload.After`, as they can contain structured data. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "json.decode" settings: # Field is a reference to the target field. Only fields that are under # `.Key` and `.Payload` can be decoded. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string field: "" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`field`</td> <td>string</td> <td><Chip label="null" /></td> <td> Field is a reference to the target field. Only fields that are under `.Key` and `.Payload` can be decoded. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Decode record key as JSON This example takes a record containing a raw JSON string in `.Key` and converts it into structured data. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "json.decode" settings: field: ".Key" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Key`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": null,\n \"key\": \"{\\\"after\\\":{\\\"data\\\":4,\\\"id\\\":3}}\",\n \"payload\": {\n \"before\": null,\n \"after\": null\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": null,\n \"key\": {\n \"after\": {\n \"data\": 4,\n \"id\": 3\n }\n },\n \"payload\": {\n \"before\": null,\n \"after\": null\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` --- ### Decode nested field as JSON This example takes a record containing a raw JSON string in `.Payload.Before.foo` and converts it into a map. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "json.decode" settings: field: ".Payload.Before.foo" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Payload.Before.foo`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"snapshot\",\n \"metadata\": null,\n \"key\": null,\n \"payload\": {\n \"before\": {\n \"foo\": \"{\\\"before\\\":{\\\"data\\\":4,\\\"id\\\":3},\\\"baz\\\":\\\"bar\\\"}\"\n },\n \"after\": null\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"snapshot\",\n \"metadata\": null,\n \"key\": null,\n \"payload\": {\n \"before\": {\n \"foo\": {\n \"baz\": \"bar\",\n \"before\": {\n \"data\": 4,\n \"id\": 3\n }\n }\n },\n \"after\": null\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/unwrap.kafkaconnect.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'unwrap.kafkaconnect' sidebar_position: 14 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `unwrap.kafkaconnect` Unwraps a Kafka Connect record from an [OpenCDC record](https://conduit.io/docs/using/opencdc-record). ## Description This processor unwraps a Kafka Connect record from the input [OpenCDC record](https://conduit.io/docs/using/opencdc-record). The input record's payload is replaced with the Kafka Connect record. This is useful in cases where Conduit acts as an intermediary between a Debezium source and a Debezium destination. In such cases, the Debezium record is set as the [OpenCDC record](https://conduit.io/docs/using/opencdc-record)'s payload, and needs to be unwrapped for further usage. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "unwrap.kafkaconnect" settings: # Field is a reference to the field that contains the Kafka Connect # record. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string field: ".Payload.After" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`field`</td> <td>string</td> <td>`.Payload.After`</td> <td> Field is a reference to the field that contains the Kafka Connect record. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Unwrap a Kafka Connect record This example shows how to unwrap a Kafka Connect record. The Kafka Connect record is serialized as a JSON string in the `.Payload.After` field (raw data). The Kafka Connect record's payload will replace the [OpenCDC record](https://conduit.io/docs/using/opencdc-record)'s payload. We also see how the key is unwrapped too. In this case, the key comes in as structured data. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "unwrap.kafkaconnect" settings: field: ".Payload.After" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Payload.After`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": \"dGVzdCBwb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"metadata-key\": \"metadata-value\"\n },\n \"key\": {\n \"payload\": {\n \"id\": 27\n },\n \"schema\": {}\n },\n \"payload\": {\n \"before\": null,\n \"after\": \"{\\n\\\"payload\\\": {\\n \\\"description\\\": \\\"test2\\\"\\n},\\n\\\"schema\\\": {}\\n}\"\n }\n}"} newValue={"{\n \"position\": \"dGVzdCBwb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"metadata-key\": \"metadata-value\"\n },\n \"key\": {\n \"id\": 27\n },\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"description\": \"test2\"\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/base64.encode.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'base64.encode' sidebar_position: 3 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `base64.encode` Encode a field to base64. ## Description The processor will encode the value of the target field to base64 and store the result in the target field. It is not allowed to encode the `.Position` field. If the provided field doesn't exist, the processor will create that field and assign its value. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "base64.encode" settings: # Field is a reference to the target field. Note that it is not # allowed to base64 encode the `.Position` field. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string field: "" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`field`</td> <td>string</td> <td><Chip label="null" /></td> <td> Field is a reference to the target field. Note that it is not allowed to base64 encode the `.Position` field. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Encode record key to base64 TThis example takes a record containing raw data in `.Key` and converts it into a base64 encoded string. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "base64.encode" settings: field: ".Key" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Key`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": \"dGVzdC1wb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": \"test-key\",\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"foo\": \"bar\"\n }\n }\n}"} newValue={"{\n \"position\": \"dGVzdC1wb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": \"dGVzdC1rZXk=\",\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"foo\": \"bar\"\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` --- ### Encode nested value to base64 This example takes a record containing a string in `.Payload.Before.foo` and converts it into a base64 encoded string. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "base64.encode" settings: field: ".Payload.After.foo" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Payload.After.foo`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": \"dGVzdC1wb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": \"test-key\",\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"foo\": \"bar\"\n }\n }\n}"} newValue={"{\n \"position\": \"dGVzdC1wb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": \"test-key\",\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"foo\": \"YmFy\"\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/unwrap.debezium.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'unwrap.debezium' sidebar_position: 13 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `unwrap.debezium` Unwraps a Debezium record from the input [OpenCDC record](https://conduit.io/docs/using/opencdc-record). ## Description In this processor, the wrapped (Debezium) record replaces the wrapping record (being processed) completely, except for the position. The Debezium record's metadata and the wrapping record's metadata is merged, with the Debezium metadata having precedence. This is useful in cases where Conduit acts as an intermediary between a Debezium source and a Debezium destination. In such cases, the Debezium record is set as the [OpenCDC record](https://conduit.io/docs/using/opencdc-record)'s payload,and needs to be unwrapped for further usage. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "unwrap.debezium" settings: # Field is a reference to the field that contains the Debezium record. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string field: ".Payload.After" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`field`</td> <td>string</td> <td>`.Payload.After`</td> <td> Field is a reference to the field that contains the Debezium record. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Unwrap a Debezium record This example how to unwrap a Debezium record from a field nested in a record's `.Payload.After` field. It additionally shows how the key is unwrapped, and the metadata merged. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "unwrap.debezium" settings: field: ".Payload.After.nested" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Payload.After.nested`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": \"dGVzdC1wb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"metadata-key\": \"metadata-value\"\n },\n \"key\": \"{\\\"payload\\\":\\\"27\\\"}\",\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"nested\": \"{\\n \\\"payload\\\": {\\n \\\"after\\\": {\\n \\\"description\\\": \\\"test1\\\",\\n \\\"id\\\": 27\\n },\\n \\\"before\\\": null,\\n \\\"op\\\": \\\"c\\\",\\n \\\"source\\\": {\\n \\\"opencdc.readAt\\\": \\\"1674061777225877000\\\",\\n \\\"opencdc.version\\\": \\\"v1\\\"\\n },\\n \\\"transaction\\\": null,\\n \\\"ts_ms\\\": 1674061777225\\n },\\n \\\"schema\\\": {}\\n}\"\n }\n }\n}"} newValue={"{\n \"position\": \"dGVzdC1wb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"metadata-key\": \"metadata-value\",\n \"opencdc.readAt\": \"1674061777225877000\",\n \"opencdc.version\": \"v1\"\n },\n \"key\": \"27\",\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"description\": \"test1\",\n \"id\": 27\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/json.encode.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'json.encode' sidebar_position: 12 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `json.encode` Encodes a specific field from structured data to JSON raw data (string). ## Description The processor takes data from the target field, encodes it into a JSON value and stores the encoded value in the target field. This processor is only applicable to fields under `.Key`, `.Payload`.Before and `.Payload.After`, as they can contain structured data. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "json.encode" settings: # Field is a reference to the target field. Only fields that are under # `.Key` and `.Payload` can be encoded. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string field: "" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`field`</td> <td>string</td> <td><Chip label="null" /></td> <td> Field is a reference to the target field. Only fields that are under `.Key` and `.Payload` can be encoded. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Encode nested field to JSON This example takes a record containing a map in `.Payload.Before.foo` and converts it into a raw JSON string. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "json.encode" settings: field: ".Payload.Before.foo" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Payload.Before.foo`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"snapshot\",\n \"metadata\": null,\n \"key\": null,\n \"payload\": {\n \"before\": {\n \"foo\": {\n \"baz\": \"bar\",\n \"before\": {\n \"data\": 4,\n \"id\": 3\n }\n }\n },\n \"after\": null\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"snapshot\",\n \"metadata\": null,\n \"key\": null,\n \"payload\": {\n \"before\": {\n \"foo\": \"{\\\"baz\\\":\\\"bar\\\",\\\"before\\\":{\\\"data\\\":4,\\\"id\\\":3}}\"\n },\n \"after\": null\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` --- ### Encode record key to JSON This example takes a record containing structured data in `.Key` and converts it into a raw JSON string. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "json.encode" settings: field: ".Key" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Key`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": null,\n \"key\": {\n \"tables\": [\n \"table1,table2\"\n ]\n },\n \"payload\": {\n \"before\": null,\n \"after\": null\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": null,\n \"key\": \"{\\\"tables\\\":[\\\"table1,table2\\\"]}\",\n \"payload\": {\n \"before\": null,\n \"after\": null\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/field.rename.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'field.rename' sidebar_position: 8 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `field.rename` Rename a group of fields. ## Description Rename a group of field names to new names. It is not allowed to rename top-level fields (`.Operation`, `.Position`, `.Key`, `.Metadata`, `.Payload.Before`, `.Payload.After`). Note that this processor only runs on structured data, if the record contains raw JSON data, then use the processor [`json.decode`](/docs/using/processors/builtin/json.decode) to parse it into structured data first. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "field.rename" settings: # Mapping is a comma separated list of keys and values for fields and # their new names (keys and values are separated by colons ":"). # For example: `.Metadata.key:id,.Payload.After.foo:bar`. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string mapping: "" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`mapping`</td> <td>string</td> <td><Chip label="null" /></td> <td> Mapping is a comma separated list of keys and values for fields and their new names (keys and values are separated by colons ":"). For example: `.Metadata.key:id,.Payload.After.foo:bar`. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Rename multiple fields This example renames the fields in `.Metadata` and `.Payload.After` as specified in the `mapping` configuration parameter. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "field.rename" settings: mapping: ".Metadata.key1:newKey,.Payload.After.foo:newFoo" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`mapping`</td> <td>`.Metadata.key1:newKey,.Payload.After.foo:newFoo`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": null,\n \"payload\": {\n \"before\": {\n \"bar\": \"baz\"\n },\n \"after\": {\n \"foo\": \"bar\"\n }\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": {\n \"newKey\": \"val1\"\n },\n \"key\": null,\n \"payload\": {\n \"before\": {\n \"bar\": \"baz\"\n },\n \"after\": {\n \"newFoo\": \"bar\"\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/webhook.http.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'webhook.http' sidebar_position: 16 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `webhook.http` Trigger an HTTP request for every record. ## Description A processor that sends an HTTP request to the specified URL, retries on error and saves the response body and, optionally, the response status. A status code over 500 is regarded as an error and will cause the processor to retry the request. The processor will retry the request according to the backoff configuration. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "webhook.http" settings: # Maximum number of retries for an individual record when backing off # following an error. # Type: float backoffRetry.count: "0" # The multiplying factor for each increment step. # Type: float backoffRetry.factor: "2" # The maximum waiting time before retrying. # Type: duration backoffRetry.max: "5s" # The minimum waiting time before retrying. # Type: duration backoffRetry.min: "100ms" # Headers to add to the request, use `headers.*` to specify the header # and its value (e.g. `headers.Authorization: "Bearer key"`). # Type: string headers.*: "" # Specifies the body that will be sent in the HTTP request. The field # accepts a Go [templates](https://pkg.go.dev/text/template) that's # evaluated using the # [opencdc.Record](https://pkg.go.dev/github.com/conduitio/conduit-commons/opencdc#Record) # as input. By default, the body is empty. # To send the whole record as JSON you can use `{{ toJson . }}`. # Type: string request.body: "" # Deprecated: use `headers.Content-Type` instead. # Type: string request.contentType: "" # Method is the HTTP request method to be used. # Type: string request.method: "GET" # URL is a Go template expression for the URL used in the HTTP # request, using Go [templates](https://pkg.go.dev/text/template). The # value provided to the template is # [opencdc.Record](https://pkg.go.dev/github.com/conduitio/conduit-commons/opencdc#Record), # so the template has access to all its fields (e.g. `.Position`, # `.Key`, `.Metadata`, and so on). We also inject all template # functions provided by [sprig](https://masterminds.github.io/sprig/) # to make it easier to write templates. # Type: string request.url: "" # Specifies in which field should the response body be saved. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string response.body: ".Payload.After" # Specifies in which field should the response status be saved. If no # value is set, then the response status will NOT be saved. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string response.status: "" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`backoffRetry.count`</td> <td>float</td> <td>`0`</td> <td> Maximum number of retries for an individual record when backing off following an error. </td> </tr> <tr> <td>`backoffRetry.factor`</td> <td>float</td> <td>`2`</td> <td> The multiplying factor for each increment step. </td> </tr> <tr> <td>`backoffRetry.max`</td> <td>duration</td> <td>`5s`</td> <td> The maximum waiting time before retrying. </td> </tr> <tr> <td>`backoffRetry.min`</td> <td>duration</td> <td>`100ms`</td> <td> The minimum waiting time before retrying. </td> </tr> <tr> <td>`headers.*`</td> <td>string</td> <td><Chip label="null" /></td> <td> Headers to add to the request, use `headers.*` to specify the header and its value (e.g. `headers.Authorization: "Bearer key"`). </td> </tr> <tr> <td>`request.body`</td> <td>string</td> <td><Chip label="null" /></td> <td> Specifies the body that will be sent in the HTTP request. The field accepts a Go [templates](https://pkg.go.dev/text/template) that's evaluated using the [opencdc.Record](https://pkg.go.dev/github.com/conduitio/conduit-commons/opencdc#Record) as input. By default, the body is empty. To send the whole record as JSON you can use `{{ toJson . }}`. </td> </tr> <tr> <td>`request.contentType`</td> <td>string</td> <td><Chip label="null" /></td> <td> Deprecated: use `headers.Content-Type` instead. </td> </tr> <tr> <td>`request.method`</td> <td>string</td> <td>`GET`</td> <td> Method is the HTTP request method to be used. </td> </tr> <tr> <td>`request.url`</td> <td>string</td> <td><Chip label="null" /></td> <td> URL is a Go template expression for the URL used in the HTTP request, using Go [templates](https://pkg.go.dev/text/template). The value provided to the template is [opencdc.Record](https://pkg.go.dev/github.com/conduitio/conduit-commons/opencdc#Record), so the template has access to all its fields (e.g. `.Position`, `.Key`, `.Metadata`, and so on). We also inject all template functions provided by [sprig](https://masterminds.github.io/sprig/) to make it easier to write templates. </td> </tr> <tr> <td>`response.body`</td> <td>string</td> <td>`.Payload.After`</td> <td> Specifies in which field should the response body be saved. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`response.status`</td> <td>string</td> <td><Chip label="null" /></td> <td> Specifies in which field should the response status be saved. If no value is set, then the response status will NOT be saved. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Send a request to an HTTP server This example shows how to use the HTTP processor to send a record's `.Payload.After` field as a string to a dummy HTTP server that replies back with a greeting. The record's `.Payload.After` is overwritten with the response. Additionally, the example shows how to set a request header and how to store the value of the HTTP response's code in the metadata field `http_status`. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "webhook.http" settings: backoffRetry.count: "0" backoffRetry.factor: "2" backoffRetry.max: "5s" backoffRetry.min: "100ms" headers.content-type: "application/json" request.body: "{{ printf "%s" .Payload.After }}" request.method: "GET" request.url: "http://127.0.0.1:54321" response.body: ".Payload.After" response.status: ".Metadata["http_status"]" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`backoffRetry.count`</td> <td>`0`</td> </tr> <tr> <td>`backoffRetry.factor`</td> <td>`2`</td> </tr> <tr> <td>`backoffRetry.max`</td> <td>`5s`</td> </tr> <tr> <td>`backoffRetry.min`</td> <td>`100ms`</td> </tr> <tr> <td>`headers.content-type`</td> <td>`application/json`</td> </tr> <tr> <td>`request.body`</td> <td>`{{ printf "%s" .Payload.After }}`</td> </tr> <tr> <td>`request.method`</td> <td>`GET`</td> </tr> <tr> <td>`request.url`</td> <td>`http://127.0.0.1:54321`</td> </tr> <tr> <td>`response.body`</td> <td>`.Payload.After`</td> </tr> <tr> <td>`response.status`</td> <td>`.Metadata["http_status"]`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": \"cG9zLTE=\",\n \"operation\": \"update\",\n \"metadata\": null,\n \"key\": null,\n \"payload\": {\n \"before\": null,\n \"after\": \"world\"\n }\n}"} newValue={"{\n \"position\": \"cG9zLTE=\",\n \"operation\": \"update\",\n \"metadata\": {\n \"http_status\": \"200\"\n },\n \"key\": null,\n \"payload\": {\n \"before\": null,\n \"after\": \"hello, world\"\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` --- ### Send a request to an HTTP server with a dynamic URL This example shows how to use the HTTP processor to use a record's `.Payload.After.name` field in the URL path, send it to a dummy HTTP server, and get a greeting with the name back. The response will be written under the record's `.Payload.After.response`. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "webhook.http" settings: backoffRetry.count: "0" backoffRetry.factor: "2" backoffRetry.max: "5s" backoffRetry.min: "100ms" request.method: "GET" request.url: "http://127.0.0.1:54321/{{.Payload.After.name}}" response.body: ".Payload.After.response" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`backoffRetry.count`</td> <td>`0`</td> </tr> <tr> <td>`backoffRetry.factor`</td> <td>`2`</td> </tr> <tr> <td>`backoffRetry.max`</td> <td>`5s`</td> </tr> <tr> <td>`backoffRetry.min`</td> <td>`100ms`</td> </tr> <tr> <td>`request.method`</td> <td>`GET`</td> </tr> <tr> <td>`request.url`</td> <td>`http://127.0.0.1:54321/{{.Payload.After.name}}`</td> </tr> <tr> <td>`response.body`</td> <td>`.Payload.After.response`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": \"cG9zLTE=\",\n \"operation\": \"create\",\n \"metadata\": null,\n \"key\": null,\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"name\": \"foo\"\n }\n }\n}"} newValue={"{\n \"position\": \"cG9zLTE=\",\n \"operation\": \"create\",\n \"metadata\": null,\n \"key\": null,\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"name\": \"foo\",\n \"response\": \"aGVsbG8sIGZvbyE=\"\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/field.exclude.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'field.exclude' sidebar_position: 7 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `field.exclude` Remove a subset of fields from the record. ## Description Remove a subset of fields from the record, all the other fields are left untouched. If a field is excluded that contains nested data, the whole tree will be removed. It is not allowed to exclude `.Position` or `.Operation` fields. Note that this processor only runs on structured data, if the record contains raw JSON data, then use the processor [`json.decode`](/docs/using/processors/builtin/json.decode) to parse it into structured data first. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "field.exclude" settings: # Fields is a comma separated list of target fields which should be # excluded. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string fields: "" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`fields`</td> <td>string</td> <td><Chip label="null" /></td> <td> Fields is a comma separated list of target fields which should be excluded. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Exclude all fields in payload Excluding all fields in `.Payload` results in an empty payload. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "field.exclude" settings: fields: ".Payload" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`fields`</td> <td>`.Payload`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": null,\n \"payload\": {\n \"before\": {\n \"bar\": \"baz\"\n },\n \"after\": {\n \"foo\": \"bar\"\n }\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": null,\n \"payload\": {\n \"before\": null,\n \"after\": null\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` --- ### Exclude multiple fields It's possible to exclude multiple fields by providing a comma-separated list of fields. In this example, we exclude `.Metadata`, `.Payload.After.foo` and `.Key.key1`. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "field.exclude" settings: fields: ".Metadata,.Payload.After.foo,.Key.key1" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`fields`</td> <td>`.Metadata,.Payload.After.foo,.Key.key1`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": {\n \"source\": \"s3\"\n },\n \"key\": {\n \"key1\": \"val1\",\n \"key2\": \"val2\"\n },\n \"payload\": {\n \"before\": {\n \"bar\": \"baz\"\n },\n \"after\": {\n \"foo\": \"bar\",\n \"foobar\": \"baz\"\n }\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": {},\n \"key\": {\n \"key2\": \"val2\"\n },\n \"payload\": {\n \"before\": {\n \"bar\": \"baz\"\n },\n \"after\": {\n \"foobar\": \"baz\"\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/avro.decode.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'avro.decode' sidebar_position: 0 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `avro.decode` Decodes a field's raw data in the Avro format. ## Description The processor takes raw data (bytes or a string) in the specified field and decodes it from the [Avro format](https://avro.apache.org/) into structured data. It extracts the schema ID from the data, downloads the associated schema from the [schema registry](https://docs.confluent.io/platform/current/schema-registry/index.html) and decodes the payload. The schema is cached locally after it's first downloaded. If the processor encounters structured data or the data can't be decoded it returns an error. This processor is the counterpart to [`avro.encode`](/docs/using/processors/builtin/avro.encode). ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "avro.decode" settings: # The field that will be decoded. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string field: ".Payload.After" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`field`</td> <td>string</td> <td>`.Payload.After`</td> <td> The field that will be decoded. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Decode a record field in Avro format This example shows the usage of the `avro.decode` processor. The processor decodes the record's`.Key` field using the schema that is downloaded from the schema registry and needs to exist under the subject`example-decode`. In this example we use the following schema: ```json { "type":"record", "name":"record", "fields":[ {"name":"myString","type":"string"}, {"name":"myInt","type":"int"} ] } ``` #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "avro.decode" settings: field: ".Key" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Key`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": \"dGVzdC1wb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": \"\\u0000\\u0000\\u0000\\u0000\\u0001\\u0006bar\\u0002\",\n \"payload\": {\n \"before\": null,\n \"after\": null\n }\n}"} newValue={"{\n \"position\": \"dGVzdC1wb3NpdGlvbg==\",\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": {\n \"myInt\": 1,\n \"myString\": \"bar\"\n },\n \"payload\": {\n \"before\": null,\n \"after\": null\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/custom.javascript.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'custom.javascript' sidebar_position: 4 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `custom.javascript` Run custom JavaScript code. ## Description A processor that makes it possible to process Conduit records using JavaScript. The following helper functions and variables are available: * `logger`: a logger that outputs to Conduit's logs. Check out [Zerolog's API](https://github.com/rs/zerolog) on how to use it. * `Record()`: constructs a new record which represents a successful processing result. It's analogous to `sdk.SingleRecord` from Conduit's Go processor SDK. * `RawData()`: creates a raw data object. It's analogous to `opencdc.RawData`. Optionally, it accepts a string argument, which will be cast into a byte array, for example: `record.Key = RawData("new key")`. * `StructuredData()`: creates a structured data (map-like) object. To find out what's possible with the JS processor, also refer to the documentation for [goja](https://github.com/dop251/goja), which is the JavaScript engine we use. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "custom.javascript" settings: # JavaScript code for this processor. It needs to have a function # `process()` that accept a record and returns a record. The # `process()` function can either modify the input record and return # it, or create a new record. If a record needs to be filtered # (dropped from the pipeline), then the `process()` function should # return `null`. # Type: string script: "" # The path to a .js file containing the processor code. # Type: string script.path: "" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`script`</td> <td>string</td> <td><Chip label="null" /></td> <td> JavaScript code for this processor. It needs to have a function `process()` that accept a record and returns a record. The `process()` function can either modify the input record and return it, or create a new record. If a record needs to be filtered (dropped from the pipeline), then the `process()` function should return `null`. </td> </tr> <tr> <td>`script.path`</td> <td>string</td> <td><Chip label="null" /></td> <td> The path to a .js file containing the processor code. </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Modify a record's metadata and payload using JavaScript In this example we use the `custom.javascript` processor to add a metadata key to the input record. It also prepends "hello, " to `.Payload.After`. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "custom.javascript" settings: script: | function process(rec) { rec.Metadata["processed"] = "true"; let existing = String.fromCharCode.apply(String, rec.Payload.After); rec.Payload.After = RawData("hello, " + existing); return rec; } ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`script`</td> <td> ```js function process(rec) { rec.Metadata["processed"] = "true"; let existing = String.fromCharCode.apply(String, rec.Payload.After); rec.Payload.After = RawData("hello, " + existing); return rec; } ``` </td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"Operation(0)\",\n \"metadata\": {\n \"existing-key\": \"existing-value\"\n },\n \"key\": null,\n \"payload\": {\n \"before\": null,\n \"after\": \"world\"\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"Operation(0)\",\n \"metadata\": {\n \"existing-key\": \"existing-value\",\n \"processed\": \"true\"\n },\n \"key\": null,\n \"payload\": {\n \"before\": null,\n \"after\": \"hello, world\"\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/filter.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'filter' sidebar_position: 10 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `filter` Acknowledges all records that get passed to the filter. ## Description Acknowledges all records that get passed to the filter, so the records will be filtered out if the condition provided to the processor is evaluated to `true`. **Important:** Make sure to add a [condition](https://conduit.io/docs/using/processors/conditions) to this processor, otherwise all records will be filtered out. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "filter" settings: # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "false" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "false" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "false" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "false" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`false`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`false`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`false`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`false`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Filter out the record #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "filter" ``` </TabItem> <TabItem value="table" label="Table"> No configuration parameters. </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": {\n \"key1\": \"val1\"\n },\n \"key\": null,\n \"payload\": {\n \"before\": {\n \"bar\": \"baz\"\n },\n \"after\": {\n \"foo\": \"bar\"\n }\n }\n}"} newValue={""} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/field.convert.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'field.convert' sidebar_position: 6 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `field.convert` Convert the type of a field. ## Description Convert takes the field of one type and converts it into another type (e.g. string to integer). The applicable types are string, int, float and bool. Converting can be done between any combination of types. Note that booleans will be converted to numeric values 1 (true) and 0 (false). Processor is only applicable to `.Key`, `.Payload.Before` and `.Payload.After` prefixes, and only applicable if said fields contain structured data. If the record contains raw JSON data, then use the processor [`json.decode`](/docs/using/processors/builtin/json.decode) to parse it into structured data first. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "field.convert" settings: # Field is the target field that should be converted. Note that you # can only convert fields in structured data under `.Key` and # `.Payload`. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string field: "" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" # Type is the target field type after conversion, available options # are: `string`, `int`, `float`, `bool`, `time`. # Type: string type: "" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`field`</td> <td>string</td> <td><Chip label="null" /></td> <td> Field is the target field that should be converted. Note that you can only convert fields in structured data under `.Key` and `.Payload`. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`type`</td> <td>string</td> <td><Chip label="null" /></td> <td> Type is the target field type after conversion, available options are: `string`, `int`, `float`, `bool`, `time`. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Convert `int` to `time` This example takes an `int` in field `.Payload.After.createdAt` and parses it as a unix timestamp into a `time.Time` value. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "field.convert" settings: field: ".Payload.After.createdAt" type: "time" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Payload.After.createdAt`</td> </tr> <tr> <td>`type`</td> <td>`time`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": null,\n \"key\": {\n \"id\": 123.345\n },\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"createdAt\": 1704198896123456800\n }\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": null,\n \"key\": {\n \"id\": 123.345\n },\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"createdAt\": \"2024-01-02T12:34:56.123456789Z\"\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` --- ### Convert `float` to `string` This example takes the `float` in field `.Key.id` and changes its data type to `string`. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "field.convert" settings: field: ".Key.id" type: "string" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Key.id`</td> </tr> <tr> <td>`type`</td> <td>`string`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"update\",\n \"metadata\": null,\n \"key\": {\n \"id\": 123.345\n },\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"foo\": \"bar\"\n }\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"update\",\n \"metadata\": null,\n \"key\": {\n \"id\": \"123.345\"\n },\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"foo\": \"bar\"\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` --- ### Convert `int` to `bool` This example takes the `int` in field `.Payload.After.done` and changes its data type to `bool`. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "field.convert" settings: field: ".Payload.After.done" type: "bool" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Payload.After.done`</td> </tr> <tr> <td>`type`</td> <td>`bool`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"update\",\n \"metadata\": null,\n \"key\": {\n \"id\": \"123\"\n },\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"done\": 1\n }\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"update\",\n \"metadata\": null,\n \"key\": {\n \"id\": \"123\"\n },\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"done\": true\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` --- ### Convert `string` to `int` This example takes the string in field `.Key.id` and changes its data type to `int`. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "field.convert" settings: field: ".Key.id" type: "int" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Key.id`</td> </tr> <tr> <td>`type`</td> <td>`int`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"update\",\n \"metadata\": null,\n \"key\": {\n \"id\": \"123\"\n },\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"foo\": \"bar\"\n }\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"update\",\n \"metadata\": null,\n \"key\": {\n \"id\": 123\n },\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"foo\": \"bar\"\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/field.set.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'field.set' sidebar_position: 9 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `field.set` Set the value of a certain field. ## Description Set the value of a certain field to any value. It is not allowed to set the `.Position` field. The new value can be a Go template expression, the processor will evaluate the output and assign the value to the target field. If the provided `field` doesn't exist, the processor will create that field and assign its value. This processor can be used for multiple purposes, like extracting fields, hoisting data, inserting fields, copying fields, masking fields, etc. Note that this processor only runs on structured data, if the record contains raw JSON data, then use the processor [`json.decode`](/docs/using/processors/builtin/json.decode) to parse it into structured data first. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "field.set" settings: # Field is the target field that will be set. Note that it is not # allowed to set the `.Position` field. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string field: "" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" # Value is a Go template expression which will be evaluated and stored # in `field` (e.g. `{{ .Payload.After }}`). # Type: string value: "" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`field`</td> <td>string</td> <td><Chip label="null" /></td> <td> Field is the target field that will be set. Note that it is not allowed to set the `.Position` field. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`value`</td> <td>string</td> <td><Chip label="null" /></td> <td> Value is a Go template expression which will be evaluated and stored in `field` (e.g. `{{ .Payload.After }}`). </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Add field This example adds a new field to the record. The field is added to `.Payload.After` and is set to `bar`. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "field.set" settings: field: ".Payload.After.foo" value: "bar" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Payload.After.foo`</td> </tr> <tr> <td>`value`</td> <td>`bar`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"snapshot\",\n \"metadata\": null,\n \"key\": {\n \"my-key\": \"id\"\n },\n \"payload\": {\n \"before\": null,\n \"after\": null\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"snapshot\",\n \"metadata\": null,\n \"key\": {\n \"my-key\": \"id\"\n },\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"foo\": \"bar\"\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` --- ### Sets the record operation to `update` This example sets the `.Operation` field to `update` for all records. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "field.set" settings: field: ".Operation" value: "update" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Operation`</td> </tr> <tr> <td>`value`</td> <td>`update`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"create\",\n \"metadata\": null,\n \"key\": null,\n \"payload\": {\n \"before\": null,\n \"after\": null\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"update\",\n \"metadata\": null,\n \"key\": null,\n \"payload\": {\n \"before\": null,\n \"after\": null\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` --- ### Set field using Go template This example sets the `.Payload.After.postgres` field to `true` if the `.Metadata.table` field contains `postgres`. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "field.set" settings: field: ".Payload.After.postgres" value: "{{ eq .Metadata.table "postgres" }}" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Payload.After.postgres`</td> </tr> <tr> <td>`value`</td> <td>`{{ eq .Metadata.table "postgres" }}`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": null,\n \"operation\": \"snapshot\",\n \"metadata\": {\n \"table\": \"postgres\"\n },\n \"key\": null,\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"postgres\": \"false\"\n }\n }\n}"} newValue={"{\n \"position\": null,\n \"operation\": \"snapshot\",\n \"metadata\": {\n \"table\": \"postgres\"\n },\n \"key\": null,\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"postgres\": \"true\"\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/index.mdx:::--- title: 'Builtin Processors' --- ```mdx-code-block import DocCardList from '@theme/DocCardList'; <DocCardList /> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./1-using/5-processors/1-builtin/unwrap.opencdc.mdx:::--- IMPORTANT: This file was generated using src/processorgen/main.go. DO NOT EDIT. title: 'unwrap.opencdc' sidebar_position: 15 --- import ReactDiffViewer from 'react-diff-viewer'; import Chip from '@mui/material/Chip'; import Box from "@mui/system/Box"; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; # `unwrap.opencdc` Unwraps an [OpenCDC record](https://conduit.io/docs/using/opencdc-record) saved in one of the record's fields. ## Description The `unwrap.opencdc` processor is useful in situations where a record goes through intermediate systems before being written to a final destination. In these cases, the original [OpenCDC record](https://conduit.io/docs/using/opencdc-record) is part of the payload read from the intermediate system and needs to be unwrapped before being written. Note: if the wrapped [OpenCDC record](https://conduit.io/docs/using/opencdc-record) is not in a structured data field, then it's assumed that it's stored in JSON format. ## Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "unwrap.opencdc" settings: # Field is a reference to the field that contains the OpenCDC record. # For more information about the format, see [Referencing # fields](https://conduit.io/docs/using/processors/referencing-fields). # Type: string field: ".Payload.After" # Whether to decode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.decode.key.enabled: "true" # Whether to decode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.decode.payload.enabled: "true" # Whether to encode the record key using its corresponding schema from # the schema registry. # Type: bool sdk.schema.encode.key.enabled: "true" # Whether to encode the record payload using its corresponding schema # from the schema registry. # Type: bool sdk.schema.encode.payload.enabled: "true" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Type</th> <th>Default</th> <th>Description</th> </tr> <tr> <td>`field`</td> <td>string</td> <td>`.Payload.After`</td> <td> Field is a reference to the field that contains the OpenCDC record. For more information about the format, see [Referencing fields](https://conduit.io/docs/using/processors/referencing-fields). </td> </tr> <tr> <td>`sdk.schema.decode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.decode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to decode the record payload using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.key.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record key using its corresponding schema from the schema registry. </td> </tr> <tr> <td>`sdk.schema.encode.payload.enabled`</td> <td>bool</td> <td>`true`</td> <td> Whether to encode the record payload using its corresponding schema from the schema registry. </td> </tr> </table> </TabItem> </Tabs> ## Examples ### Unwrap an [OpenCDC record](https://conduit.io/docs/using/opencdc-record) In this example we use the `unwrap.opencdc` processor to unwrap the [OpenCDC record](https://conduit.io/docs/using/opencdc-record) found in the record's `.Payload.After` field. #### Configuration parameters <Tabs groupId="config-params"> <TabItem value="yaml" label="YAML"> ```yaml version: 2.2 pipelines: - id: example status: running connectors: # define source and destination ... processors: - id: example plugin: "unwrap.opencdc" settings: field: ".Payload.After" ``` </TabItem> <TabItem value="table" label="Table"> <table class="no-margin-table"> <tr> <th>Name</th> <th>Value</th> </tr> <tr> <td>`field`</td> <td>`.Payload.After`</td> </tr> </table> </TabItem> </Tabs> #### Record difference ```mdx-code-block <Box className='diff-viewer'> <ReactDiffViewer styles={{ diffContainer: { overflowX: 'auto', overflowY: 'hidden', }, }} leftTitle={'Before'} rightTitle={'After'} oldValue={"{\n \"position\": \"d3JhcHBpbmcgcG9zaXRpb24=\",\n \"operation\": \"create\",\n \"metadata\": {},\n \"key\": \"wrapping key\",\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"key\": {\n \"id\": \"test-key\"\n },\n \"metadata\": {},\n \"operation\": \"update\",\n \"payload\": {\n \"after\": {\n \"msg\": \"string 0e8955b3-7fb5-4dda-8064-e10dc007f00d\",\n \"sensor_id\": 1250383582,\n \"triggered\": false\n },\n \"before\": null\n },\n \"position\": \"dGVzdC1wb3NpdGlvbg==\"\n }\n }\n}"} newValue={"{\n \"position\": \"d3JhcHBpbmcgcG9zaXRpb24=\",\n \"operation\": \"update\",\n \"metadata\": {},\n \"key\": {\n \"id\": \"test-key\"\n },\n \"payload\": {\n \"before\": null,\n \"after\": {\n \"msg\": \"string 0e8955b3-7fb5-4dda-8064-e10dc007f00d\",\n \"sensor_id\": 1250383582,\n \"triggered\": false\n }\n }\n}"} hideLineNumbers={false} showDiffOnly={false} splitView={true} /> </Box> ``` ![scarf pixel conduit-site-docs-using-processors](https://static.scarf.sh/a.png?x-pxid=02ff4382-6501-4410-b523-fa8e7f879b00)
./0-what-is/0-introduction.mdx:::--- sidebar_position: 0 hide_title: true title: 'Introduction' sidebar_label: "Introduction" slug: '/' --- <img alt="Conduit Logo" style={{ maxWidth: "400px", marginTop: 0 }} src="/img/conduit/on-white-conduit-logo.png" /> Conduit is a data streaming tool for software and data engineers, powered by [Meroxa](https://meroxa.io). Its purpose is to help you move data from A to B. You can use Conduit to send data from Kafka to Postgres, between files and APIs, between [supported connectors](/docs/using/connectors/list), and [any datastore you can build a plugin for](/docs/developing/connectors/). It's written in [Go](https://go.dev/), compiles to a single binary, and is designed to be easy to use and [install](/docs/installing-and-running). Out of the box, Conduit comes with: - Built-in connectors - Processors - Observability - Schema Support ## Features Conduit comes with many powerful features such as: * **Simple deployments**: Conduit is distributed as a single binary and doesn't require any external dependencies to run. We also provide a [Kubernetes operator](/docs/scaling/conduit-operator). * **Real time by default**: Conduit pipelines continuously listen for changes to a database, data warehouse, etc., and allows your data applications to act upon those changes in real time. * **Easily extensible**: Conduit connectors are plugins that communicate with Conduit via a gRPC interface. This means that plugins can be written in any language as long as they conform to the required interface. Check out our [connector docs](/docs/using/connectors/getting-started)! For a detailed look at additional features, visit our [Features](/docs/using/other-features) page. Here, you'll find a selection of tools and functionalities that complement our main features. Now let's [get started](/docs/getting-started) with our first pipeline! ![scarf pixel conduit-site-docs-what-is](https://static.scarf.sh/a.png?x-pxid=01346572-0d57-4df3-8399-1425db913a0a)
./0-what-is/1-getting-started.mdx:::--- title: 'Getting Started' sidebar_label: "Getting Started" sidebar_position: 0 slug: '/getting-started' --- ## Our goal In this guide, our goal will be to stream information about flights from an imaginary airport to a file. The flight records contain just the airline name and the scheduled departure time. ## Install Conduit If you're using a macOS or Linux system, you can install Conduit with the following command: ```shell $ curl https://conduit.io/install.sh | bash ``` If you're not using macOS or Linux system, you can still install Conduit following one of the different options provided in [our installation page](/docs/installing-and-running). :::note The Conduit binary contains both, the Conduit service and the Conduit CLI, with which you can interact with Conduit. ::: ## Initialize Conduit Firs, let's initialize the working environment: ```shell $ conduit init Created directory: processors Created directory: connectors Created directory: pipelines Configuration file written to conduit.yaml Conduit has been initialized! To quickly create an example pipeline, run 'conduit pipelines init'. To see how you can customize your first pipeline, run 'conduit pipelines init --help'. ``` `conduit init` creates the directories where you can put your pipeline configuration files, connector binaries, and processor binaries. There's also a `conduit.yaml` that contains all the configuration parameters that Conduit supports. In this guide, we'll only use the `pipelines` directory, since we won't need to install any additional connector nor to change Conduit's default configuration. ## Build a pipeline Next, we can use the Conduit CLI to build the example pipeline: ```shell $ conduit pipelines init ``` `conduit pipelines init` builds an example that generates flight information from an imaginary airport every second. Use `conduit pipelines init --help` to learn how to customize the pipeline. If the `pipelines` directory, you'll notice a new file, `pipeline-generator-to-file.yaml` that contains our pipeline's configuration: ```yaml version: "2.2" pipelines: - id: example-pipeline status: running name: "generator-to-file" connectors: - id: example-source type: source plugin: "generator" settings: # Generate field 'airline' of type string # Type: string # Optional format.options.airline: 'string' # Generate field 'scheduledDeparture' of type 'time' # Type: string # Optional format.options.scheduledDeparture: 'time' # The format of the generated payload data (raw, structured, file). # Type: string # Optional format.type: 'structured' # The maximum rate in records per second, at which records are # generated (0 means no rate limit). # Type: float # Optional rate: '1' - id: example-destination type: destination plugin: "file" settings: # Path is the file path used by the connector to read/write records. # Type: string # Optional path: './destination.txt' ``` The configuration above tells us some basic information about the pipeline (ID and name) and that we want Conduit to start the pipeline automatically ( `status: running`). Then we see a source connector, that uses the [ `generator` plugin](https://github.com/ConduitIO/conduit-connector-generator), which is a built-in plugin that can generate random data. The source connector's settings translate into: generate structured data, 1 record per second. Each generated record should contain an `airline` field (type: string) and a `scheduledDeparture` field (type: duration). What follows is a destination connector where the data will be written to. It uses the `file` plugin, which is a built-in plugin that writes all the incoming data to a file. It has only one configuration parameter, which is the path to the file where the records will be written. ## Run Conduit With the pipeline configuration being ready, we can run Conduit: ```shell $ conduit --pipelines.path pipelines/ ``` Conduit is now running the pipeline. Let's check the contents of the `destination.txt` using: ```shell tail -f destination.txt | jq ``` Every second, you should a JSON object like this: ```shell { "position": "MjU=", "operation": "create", "metadata": { "conduit.source.connector.id": "example-pipeline:example-source", "opencdc.createdAt": "1730801194148460912", "opencdc.payload.schema.subject": "example-pipeline:example-source:payload", "opencdc.payload.schema.version": "1" }, "key": "cHJlY2VwdG9yYWw=", "payload": { "before": null, "after": { "airline": "wheelmaker", "scheduledDeparture": "2024-11-05T10:06:34.148469Z" } } } ``` The JSON object you see is the [OpenCDC record](/docs/using/opencdc-record) that holds the data being streamed as well as other data and metadata. In the `.payload.after` field you will see the user data that was generated by the `generator` connector: ```json { "airline": "wheelmaker", "scheduledDeparture": "2024-11-05T10:06:34.148469Z" } ``` The pipeline will keep streaming the data from the generator source connector to the file destination connector as long as Conduit is running. To stop Conduit, press `Ctrl + C` (on a Linux OS, or the equivalent on other operating systems). This will trigger a graceful shutdown that stops reads from source connectors and waits for records that are still in the pipeline to be acknowledged. The next time Conduit starts, it will start reading data from where it stopped. ## What's next? Now that you've got the basics of running Conduit and creating a pipeline covered, here are a few places to dive in deeper: - [Connectors](/docs/using/connectors/getting-started) - [Pipelines](/docs/using/pipelines/configuration-file) - [Processors](/docs/using/processors/getting-started) Or, if you want to experiment a bit more, check out the examples in our [GitHub repository](https://github.com/ConduitIO/conduit/tree/main/examples). ![scarf pixel conduit-site-docs-getting-started](https://static.scarf.sh/a.png?x-pxid=adb4c80b-9e87-4103-98dc-8d9801ccc865)
./0-what-is/1-core-concepts/1-pipeline-semantics.mdx:::--- title: "Pipeline Semantics" slug: '/core-concepts/pipeline-semantics' --- This document describes the inner workings of a Conduit pipeline, its structure, and behavior. It also describes a Conduit message and its lifecycle as it flows through the pipeline. ## Pipeline structure A Conduit pipeline is a directed acyclic graph of nodes. Each node runs in its own goroutine and is connected to other nodes via unbuffered Go channels that can transmit messages. In theory, we could create arbitrarily complex graphs of nodes, but for the sake of a simpler API we expose the ability to create graphs with the following structure: ![Pipeline](/img/pipeline_example.svg) In the diagram above we see 7 sections: - **Source connectors** - represents the code that communicates with a 3rd party system and continuously fetches records and sends them to Conduit (e.g. [kafka connector](https://github.com/conduitio/conduit-connector-kafka)). Every source connector is managed by a source node that receives records, wraps them in a message, and sends them downstream to the next node. A pipeline requires at least one source connector to be runnable. - **Source processors** - these processors only receive messages originating at a specific source connector. Source processors are created by specifying the corresponding source connector as the parent entity. Source processors are not required for starting a pipeline. - **Fan-in node** - this node is essentially a multiplexer that receives messages produced by all source connectors and sends them into one output channel. The order of messages coming from different connectors is nondeterministic. A fan-in node is automatically created for all pipelines. - **Pipeline processors** - these processors receive all messages that flow through the pipeline, regardless of the source or destination. Pipeline processors are created by specifying the pipeline as the parent entity. Pipeline processors are not required for starting a pipeline. - **Fan-out node** - this node is the counterpart to the fan-in node and acts as a demultiplexer that sends messages coming from a single input to multiple downstream nodes (one for each destination). The fan-out node does not buffer messages, instead, it waits for a message to be sent to all downstream nodes before processing the next message (see [backpressure](#backpressure)). A fan-out node is automatically created for all pipelines. - **Destination processors** - these processors receive only messages that are meant to be sent to a specific destination connector. Destination processors are created by specifying the corresponding destination connector as the parent entity. Destination processors are not required for starting a pipeline. - **Destination connectors** - represents the code that communicates with a 3rd party system and continuously receives records from Conduit and writes them to the destination (e.g. [kafka connector](https://github.com/conduitio/conduit-connector-kafka)). Every destination connector is managed by a destination node that receives records and sends them to the connector. A pipeline requires at least one destination connector to be runnable. There are additional internal nodes that Conduit adds to each pipeline not shown in the diagram, as they are inconsequential for the purpose of this document (e.g. nodes for gathering metrics, nodes for managing acknowledgments, etc.). ## Message A message is a wrapper around a record that manages the record's lifecycle as it flows through the pipeline. Messages are created in source nodes when they receive records from the source connector, and they are passed down the pipeline between nodes until they are acked or nacked. Nodes are only allowed to hold a reference to a single message at a time, meaning that they need to pass the message to the next node before taking another message¹. This also means there is no explicit buffer in Conduit, a pipeline can only hold only as many messages as there are nodes in the pipeline (see [backpressure](#backpressure) for more information). ¹This might change in the future if we decide to add support for multi-message transforms. ### Message states A message can be in one of 3 states: - **Open** - all messages start in the open state. This means the message is currently in processing, either by a node or a destination connector. A pipeline won't stop until all messages transition from the open state into one of the other two states. - **Acked** - a message was successfully processed and acknowledged. This can be done either by a processor (e.g. it filtered the message out) or by a destination. If a pipeline contains multiple destinations, the message needs to be acknowledged by all destinations before it is marked as acked. Acks are propagated back to the source connector and can be used to advance the position in the source system if applicable. - **Nacked** - the processing of the message failed and resulted in an error, so the message was negatively acknowledged. This can be done either by a processor (e.g. a transform failed) or by a destination. If a pipeline contains multiple destinations, the message needs to be negatively acknowledged by at least one destination before it is marked as nacked. When a message is nacked, the message is passed to the [DLQ](#dead-letter-queue) handler, which essentially controls what happens after a message is nacked (stop pipeline, drop message and continue running or store message in DLQ and continue running). **Important**: if a message gets nacked and the DLQ handler successfully processes the nack (e.g. stores the message in the dead-letter queue), the source connector will receive an ack as if the message was successfully processed, even though Conduit marks it internally as nacked. In other words, the source connector will receive an ack every time Conduit handled a message end-to-end and it can be safely discarded from the source. Pipeline nodes will either leave the message open and send it to the next node for processing or ack/nack it and not send it further down the pipeline. If the ack/nack fails, the node will stop running and return an error that will consequently stop the whole pipeline. The returned error is stored in the pipeline for further inspection by the user. ### Message state change handlers A pipeline node can register state change handlers on a message that will be called when the message state changes. This is used for example to register handlers that reroute nacked messages to a dead-letter queue or to update metrics when a message reaches the end of the pipeline. If a message state change handler returns an error, the node that triggered the ack/nack will stop running, essentially causing the whole pipeline to stop. ## Semantics ### Messages are delivered in order Since messages are passed between nodes in channels and a node only processes one message at a time, it is guaranteed that messages from a single source connector will flow through the Conduit pipeline in the same order that was produced by that source. There are two caveats: - If a pipeline contains multiple source connectors, the order of two messages coming from different connectors is nondeterministic. Messages coming from the same source connector are still guaranteed to retain their order. - If a dead-letter queue is configured, negatively acknowledged messages will be removed from the stream while the pipeline will keep running, thus impacting the order of messages. The order guarantee only holds inside of Conduit. Once a message reaches a destination connector, it is allowed to buffer messages and batch write them to 3rd party systems. Normally the connector would retain the order, although we can't vouch for badly written connectors that don't follow this behavior. ### Messages are delivered at least once Between pipeline restarts, it is guaranteed that any message that is processed successfully by all nodes and not filtered out will be delivered to a destination connector at least once. Multiple deliveries can occur in pipelines with multiple destinations that stopped because of a negatively acknowledged record, or pipelines where a destination negatively acknowledged a record and processed more messages after that. For this reason, we strongly recommend implementing the write operation of a destination connector in an idempotent way (if possible). The delivery guarantee can be changed to "at most once" by adding a [dead-letter queue](#dead-letter-queue) handler that drops unsuccessfully processed messages. ### Acks are delivered in order Conduit ensures that acknowledgments are sent to the source connector in the exact same order as records produced by the connector. This guarantee still holds, even if a badly implemented destination connector acknowledges records in a different order, or if a processor filters out a record (i.e. acks the message) while a message that came before it is still being processed. ### Acks are delivered at most once Acknowledgments are sent back to the source connector at most once. This means that if a message gets negatively acknowledged and is not successfully processed by a DLQ handler, the acknowledgment for that message won't be delivered to the source connector. Acknowledgments of all messages produced after this message also won't be delivered to the source connector, otherwise the order delivery guarantee would be violated. The absence of an acknowledgment after the source connector teardown is initiated can be interpreted as a negative acknowledgment. ### Backpressure The usage of unbuffered channels between nodes and the absence of explicit buffers results in backpressure. This means that the speed of the destination connector dictates the speed of the whole pipeline. There is an implicit buffer that needs to be filled up before backpressure takes effect. The buffer is equal to the number of nodes in a pipeline - similar to how a longer garden hose holds more water, a longer Conduit pipeline can hold more messages. There are two exceptions to this rule: - Conduit is using gRPC streams to communicate with standalone connectors, which internally buffers requests before sending them over the wire, thus creating another implicit buffer (we are aware of this issue: [#211](https://github.com/ConduitIO/conduit/issues/211)). - Destination connectors are allowed to collect multiple records and write them in batches to the destination, which creates a buffer that depends on the connector implementation. If there are multiple destinations the fanout node won't fetch a new message from the upstream node until the current message was successfully sent to all downstream nodes (this doesn't mean the message was necessarily processed, just that all downstream nodes received the message). As a consequence, the speed of the pipeline will be throttled to accommodate the abilities of the slowest destination connector. ### Dead-letter queue Messages that get negatively acknowledged can be rerouted to another destination called a dead-letter queue (DLQ) where they are stored and can be reprocessed at a later point in time after manual intervention. If rerouting is set up and the message successfully reaches the DLQ the message will be internally nacked, but an acknowledgment will be sent to the source connector since Conduit handled the message and it can be discarded from the source. The user has the option to configure a DLQ that simply logs a warning and drops messages to achieve "at most once" delivery guarantees. ### Pipeline stop A pipeline can be stopped in two ways - either it's stopped gracefully or forcefully. - A graceful stop is initiated either by Conduit shutting down or by the user requesting the pipeline to stop. Only the source connector nodes will receive the signal to stop running. The source nodes will stop running and close their outgoing channels, notifying the downstream nodes that there will be no more messages. This behavior propagates down the pipeline until the last node stops running. Any messages that were being processed while the pipeline received a stop signal will be processed normally and written to all destinations. - A forceful stop is initiated when a node stops running because it experienced an unrecoverable error (e.g. it nacked a message and received an error because no DLQ is configured, or the connector plugin returned an unexpected error). In that case, the context that is shared by all nodes will get canceled, signaling to all nodes simultaneously that they should stop running as soon as possible. Messages that are in the pipeline won't be drained, instead, they are dropped and will be requested from the source again once the pipeline is restarted. The error returned from the first node that failed will be stored in the pipeline and can be retrieved through the API. ![scarf pixel conduit-site-docs-what-is-core-concepts](https://static.scarf.sh/a.png?x-pxid=8d2e502f-e2dd-43ab-a1c1-7a00ac3b0d68)
./0-what-is/1-core-concepts/index.mdx:::--- title: "Core concepts" slug: '/core-concepts' --- ## Pipeline A pipeline receives records from one or multiple source connectors, pushes them through zero or multiple processors until they reach one or multiple destination connectors. ## Connector A connector is the internal entity that communicates with a connector plugin and either pushes records from the plugin into the pipeline (source connector) or the other way around (destination connector). ## Connector plugin Sometimes also referred to as "plugin", is an external process which communicates with Conduit and knows how to read/write records from/to a data source/destination (e.g. a database). ## Processor A component that executes an operation on a single record that flows through the pipeline. It can either change the record or filter it out based on some criteria. ## OpenCDC Record A record represents a single piece of data that flows through a pipeline (e.g. one database row). [More info here](/docs/using/opencdc-record). ## Collection A generic term used in Conduit to describe an entity in a 3rd party system from which records are read from or to which records they are written to. Examples are: topics (in Kafka), tables (in a database), indexes (in a search engine), collections (in NoSQL databases), etc. ![scarf pixel conduit-site-docs-what-is-core-concepts](https://static.scarf.sh/a.png?x-pxid=8d2e502f-e2dd-43ab-a1c1-7a00ac3b0d68)
./0-what-is/1-core-concepts/0-architecture.mdx:::--- title: "Conduit Architecture" slug: '/core-concepts/architecture' --- Here is an overview of the internal Conduit Architecture. {/* Diagram source can be found at: https://lucid.app/lucidspark/414c75e1-bd3d-46f4-8baf-1b51f264ccea/edit?viewport_loc=-4169%2C-1359%2C6150%2C4089%2C0_0&invitationId=inv_9bb11c46-a33a-4119-ade6-a006a77a5f3b */} ![Conduit Architecture](/img/conduit/conduit-architecture.svg) Conduit is split into the following layers: ## API layer Exposes the public APIs used to communicate with Conduit. It exposes 2 types of APIs: ### gRPC This is the main API provided by Conduit. The gRPC API definition can be found in [api.proto](https://github.com/ConduitIO/conduit/blob/main/proto/api/v1/api.proto), it can be used to generate code for the client. ### HTTP The HTTP API is generated using [grpc-gateway](https://github.com/grpc-ecosystem/grpc-gateway) and forwards the requests to the gRPC API. Conduit exposes an openapi definition that describes the HTTP API, which is also exposed through Swagger UI on `http://localhost:8080/openapi/`. ## Orchestration layer The orchestration layer is responsible for coordinating the flow of operations between the core services. It also takes care of transactions, making sure that changes made to specific entities are not visible to the outside until the whole operation succeeded. There are five orchestrators, each responsible for actions related to one of the main entities: - Pipeline Orchestrator. - Connector Orchestrator. - Processor Orchestrator. - Connector Plugin Orchestrator. - Processor Plugin Orchestrator. ## Core layer We regard the core to be the combination of the entity management layer and the pipeline engine. It provides functionality to the orchestrator layer and does not concern itself with where requests come from and how single operations are combined into more complex flows. ### Entity management This layer is concerned with the creation, editing, deletion and storage of the main entities. You can think about this as a simple CRUD layer. It can be split up further using the main entities: #### Pipeline This is the central entity managed by the Pipeline Service that ties together all other components. A pipeline contains the configuration that defines how pipeline nodes should be connected together in a running pipeline. It has references to at least one source and one destination connector and zero or multiple processors, a pipeline that does not meet the criteria is regarded as incomplete and can't be started. A pipeline can be either running, stopped or degraded (stopped because of an error). The pipeline can only be edited if it's not in a running state. #### Connector A connector takes care of receiving or forwarding records to connector plugins, depending on its type (source or destination). It is also responsible for tracking the connector state as records flow through it. The Connector Service manages the creation of connectors and permanently stores them in the Connector Store. A connector can be configured to reference a number of processors, which will be executed only on records that are received from or forwarded to that specific connector. * **Connector Plugin** - interfaces with Conduit on one side, and with the standalone connector plugin on the other and facilitates the communication between them. A standalone connector plugin is a separate process that implements the interface defined in [conduit-connector-protocol](https://github.com/ConduitIO/conduit-connector-protocol) and provides the read/write functionality for a specific resource (e.g. a database). #### Processor Processors are stateless components that operate on a single record and can execute arbitrary actions before forwarding the record to the next node in the pipeline. A processor can also choose to drop a record without forwarding it. They can be attached either to a connector or to a pipeline, based on that they are either processing only records that flow from/to a connector or all records that flow through a pipeline. * **Processor Plugin** - interfaces with Conduit on one side, and with the standalone processor plugin on the other and facilitates the communication between them. A standalone processor plugin is a WASM binary that implements the interface defined in [conduit-processor-sdk](https://github.com/ConduitIO/conduit-processor-sdk) and provides the logic for processing a record (e.g. transforming its content). ### Pipeline Engine The pipeline engine consists of nodes that can be connected together with Go channels to form a data pipeline. #### Node A node is a lightweight component that runs in its own goroutine and runs as long as the incoming channel is open. As soon as the previous node stops forwarding records and closes its out channel, the current node also stops running and closes its out channel. This continues down the pipeline until all records are drained and the pipeline gracefully stops. In case a node experiences an error all other nodes will be notified and stop running as soon as possible without draining the pipeline. ## Persistence layer This layer is used directly by the [Orchestration layer](#orchestration-layer) and indirectly by the [Core layer](#core-layer), and [Schema registry service](#schema-registry-service) (through stores) to persist data. It provides the functionality of creating transactions and storing, retrieving and deleting arbitrary data like configurations or state. More information on [storage](/docs/using/other-features/storage). ## Connector utility services ### Schema registry service The schema service is responsible for managing the schema of the records that flow through the pipeline. It provides functionality to infer a schema from a record. The schema is stored in the schema store and can be referenced by connectors and processors. By default, Conduit provides a built-in schema registry, but this service can be run separately from Conduit. More information on [Schema Registry](/docs/using/other-features/schema-support#schema-registry). ![scarf pixel conduit-site-docs-what-is-core-concepts](https://static.scarf.sh/a.png?x-pxid=8d2e502f-e2dd-43ab-a1c1-7a00ac3b0d68)
./2-developing/0-connectors/6-behavior.mdx:::--- title: "Connector Behavior" --- This document provides insights on how Conduit communicates with a connector. ## Conduit Connector Protocol Conduit expects all connectors to follow the [Conduit Connector Protocol](https://github.com/ConduitIO/conduit-connector-protocol). The connector protocol is a set of protobuf files describing the [interface](#protocol-grpc-interface) between Conduit and the connector in the form of gRPC services. This approach allows connectors to be written in any language with support for gRPC. The connector protocol splits the connector interface in 3 gRPC services - one for the source, another for the destination, and a third one for the connector specifications. A connector needs to implement the specifications and at least the source or destination. Note that you don't need to use the connector protocol directly - we provide a [Go connector SDK](https://github.com/ConduitIO/conduit-connector-sdk) that hides the complexity of the protocol and simplifies the implementation of a connector. ### Standalone vs built-in connectors While the Conduit Connector Protocol decouples Conduit from its connectors by using gRPC, it also provides a thin Go layer that allows any Go connector to be compiled into the Conduit binary as a built-in connector. The following diagram shows how Conduit communicates with a standalone connector and a built-in connector. ![Standalone vs built-in connectors](/img/standalone-vs-builtin.svg) **Standalone connectors** are run as separate processes, separate from the Conduit process. They need to have an entrypoint (binary or script) which runs the connector and starts the gRPC server responsible for communicating with Conduit. A standalone connector process is started and stopped by Conduit on demand. One connector process will be started for every pipeline connector in Conduit. **Built-in connectors** on the other hand are executed in the same process as Conduit and communicate with Conduit through Go channels instead of gRPC. Any connector written in Go can be compiled into the Conduit binary and used as a built-in connector. Find out more about the [Conduit connector plugin architecture](https://github.com/ConduitIO/conduit/blob/main/docs/architecture-decision-records/20220121-conduit-plugin-architecture.md). ## Protocol gRPC Interface The protocol interface is hosted on the [Buf schema registry](https://buf.build/conduitio/conduit-connector-protocol/docs/main:connector.v1). Use it as a starting point when implementing a connector in a language other than Go. ![scarf pixel conduit-site-docs-developing-connectors](https://static.scarf.sh/a.png?x-pxid=3ada0949-fa61-40d6-a44a-76447ea4e39f)
./2-developing/0-connectors/3-developing-source-connectors.mdx:::--- title: "Developing a Source Connector" --- A Source is responsible for continuously reading data from a third party system and returning it in the form of an [OpenCDC Record](https://pkg.go.dev/github.com/conduitio/conduit-commons/opencdc#Record). You need to implement the functions required by the [Source](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Source) interface and provide your own implementations. Information about individual functions are listed below. The **`source.go`** file is the main file where the functionality of your source connector is implemented. ## `Source struct` Every Source implementation needs to include an [UnimplementedSource](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#UnimplementedSource) to satisfy the interface. This allows us to potentially change the interface in the future while remaining backward compatible with existing Source implementations. This struct can be modified to add additional fields that can be accessed throughout the lifecycle of the Connector. ```go type Source struct { sdk.UnimplementedSource config SourceConfig tail *tail.Tail } ``` ## Source Connector Lifecycle Functions ### `NewSource()` A constructor function for your Source struct. Note that this is the same function that should be set as the value of `Connector.NewSource`. The constructor should be used to wrap your Source in the default `DefaultSourceMiddleware`. ```go func NewSource() sdk.Source { // Create Source and wrap it in the default middleware. return sdk.SourceWithMiddleware( &Source{}, sdk.DefaultSourceMiddleware()..., ) } ``` **Additional options via `SourceMiddlewareOption`**: In case you need to add additional middleware options, you can do so by passing it to the `sdk.SourceWithMiddleware` function via `sdk.DefaultSourceMiddleware(opts ...SourceMiddlewareOption)`. Currently, the available source middleware options can be found [here](https://github.com/ConduitIO/conduit-connector-sdk/blob/1cbe778fabc8f903e075872560e6a91049d2e978/source_middleware.go#L42-L46). :::note If you're using a source connector that's not generating structured data (i.e. produces raw data), you might want to disable schema extraction by default by overwriting the `sdk.SourceWithSchemaExtractionConfig` options: ```go sdk.SourceWithMiddleware( &Source{}, sdk.DefaultSourceMiddleware( // disable schema extraction by default, because the source produces raw data sdk.SourceWithSchemaExtractionConfig{ PayloadEnabled: lang.Ptr(false), KeyEnabled: lang.Ptr(false), }, )..., ) ``` ::: ### `Parameters()` A map of named Parameters that describe how to configure the connector. This map is typically generates using [ `paramgen`](https://github.com/ConduitIO/conduit-commons/tree/main/paramgen). ```go func (s *Source) Parameters() config.Parameters { return s.config.Parameters() } ``` ### `Configure()` Validates and stores configuration data for the connector. Any complex validation logic should be implemented here. ```go func (s *Source) Configure(ctx context.Context, cfg config.Config) error { err := sdk.Util.ParseConfig(ctx, cfg, &s.config, NewSource().Parameters()) if err != nil { return err } // add custom validations here return nil } ``` ### `Open()` Prepares the connector to start producing records based on the last known successful position. If needed, the connector should open connections in this function. Every record read by a source connector has a [position](https://conduit.io/docs/using/opencdc-record#fields) attached. The position given to `Open()` is the position of the record that was the last to be successfully processed end-to-end, before the connector stopped, or `nil` if no records were read. Hence, a position needs to contain enough information for a source connector to resume reading records from where it exactly stopped. A position is a slice of bytes that can represent any data structure. In Conduit connectors, it's common to see that a position is actually a `struct`, that's marshalled into a JSON string. In the example below, the position is an offset within the file being read. ```go func (s *Source) Open(ctx context.Context, p opencdc.Position) error { // parse the position var offset int64 if p != nil { var err error offset, err = strconv.ParseInt(string(p), 10, 64) if err != nil { return fmt.Errorf("invalid position %v, expected a number", p) } } // seek to the position, i.e. the offset sdk.Logger(ctx).Info(). Int64("position", offset). Msgf("seeking...") t, err := tail.TailFile( s.config.Path, tail.Config{ Follow: true, Location: &tail.SeekInfo{ Offset: offset, Whence: io.SeekStart, }, Logger: tail.DiscardingLogger, }, ) if err != nil { return fmt.Errorf("could not tail file: %w", err) } s.tail = t return nil } ``` ### `Read()` Gathers data from the configured data source and formats it into a `opencdc.Record` that is returned from the function. The returned `opencdc.Record` is queued into the pipeline to be consumed by a Destination connector. ```go func (s *Source) Read(ctx context.Context) (opencdc.Record, error) { select { case line, ok := <-s.tail.Lines: if !ok { return opencdc.Record{}, s.tail.Err() } return sdk.Util.Source.NewRecordCreate( opencdc.Position(strconv.FormatInt(line.SeekInfo.Offset, 10)), map[string]string{ MetadataFilePath: s.config.Path, }, opencdc.RawData(strconv.Itoa(line.Num)), // use line number as key opencdc.RawData(line.Text), // use line content as payload ), nil case <-ctx.Done(): return opencdc.Record{}, ctx.Err() } } ``` ### `Ack()` `Ack` signals to the third party system that the record with the supplied position was successfully processed. It's worth noting that while some source connectors need to implement this functionality (e.g. in the case of messaging brokers), others don't have to (e.g. a file source). ```go func (s *Source) Ack(ctx context.Context, position opencdc.Position) error { sdk.Logger(ctx).Trace().Msg("record successfully processed") return nil // no ack needed } ``` ### `Teardown()` Teardown signals to the connector that there will be no more calls to any other function. Any connections that were created in the `Open()` function should be closed here. ```go func (s *Source) Teardown(context.Context) error { if s.tail != nil { return s.tail.Stop() } return nil } ``` ![scarf pixel conduit-site-docs-developing-connectors](https://static.scarf.sh/a.png?x-pxid=3ada0949-fa61-40d6-a44a-76447ea4e39f)
./2-developing/0-connectors/1-conduit-connector-template.mdx:::--- title: "Conduit Connector Template" --- ## Initializing a Conduit Connector Project To begin the development of a custom Conduit connector, it is recommended developers should initialize their project using the [Conduit connector template](https://github.com/ConduitIO/conduit-connector-template). This template streamlines the setup process by providing a fundamental project structure, along with utility configurations for GitHub actions and a Makefile. Included in the Conduit Connector Template are: - Base code for configuring the connector's source and destination. - Sample unit tests to validate connector functionality. - A preconfigured Makefile to assist with common build tasks. - GitHub Actions Workflows for continuous integration including building, testing, linting, and automated release creation upon tag push. - Dependabot configurations to automate dependency updates and minor version auto-merging. - Issue and Pull Request templates for consistent contribution practices. - A README template to guide project documentation. ### Steps to Use the Connector Template 1. On the repository's main page, select "Use this template". 2. Provide the new repository details as prompted. 3. Upon repository creation, clone it to your local development environment. 4. Execute the `./setup.sh` script with the desired module name, e.g., `./setup.sh github.com/awesome-org/conduit-connector-file`. 5. (Optional) Define code owners in the `CODEOWNERS` file. :::note By convention the name of the repository should be conduit-connector-[connector name]. So if you would like to reference the connector using foo, the repository should be named conduit-connector-foo. ::: ## Developing Connectors A Conduit Connector can serve as a source, destination, or both. Implement a source connector by defining a struct that satisfies the [sdk.Source](<https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Source>) interface, or create a destination connector by creating a struct that satisfies [sdk.Destination](<https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Destination>). Both types of connectors follow a similar lifecycle which is: configure, open connection, read/write records, and teardown. There are other auxiliary functions that help with configuration that are autogenerated when using the Conduit Connector template. Further information on developing a Source or Destination connector can be found at the following links: - [Developing a Source Connector](/docs/developing/connectors/developing-source-connectors) - [Developing a Destination Connector](/docs/developing/connectors/developing-destination-connectors) ![scarf pixel conduit-site-docs-developing-connectors](https://static.scarf.sh/a.png?x-pxid=3ada0949-fa61-40d6-a44a-76447ea4e39f)
./2-developing/0-connectors/7-connector-lifecycle.mdx:::--- title: "Connector Lifecycle" --- :::info This document describes the lifecycle of a connector implemented in Go using the [Connector SDK](https://github.com/ConduitIO/conduit-connector-sdk), which abstracts away some parts of the Connector Protocol. If you are implementing a connector in a different language you should refer to the [Connector Protocol](https://github.com/ConduitIO/conduit-connector-protocol). ::: Connector plugins that are implemented using the [Connector SDK](https://github.com/ConduitIO/conduit-connector-sdk) can implement 2 interfaces: [`Source`](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Source) for source connectors and [`Destination`](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Destination) for destination connectors. Below we will describe the order of the method calls in sources and destinations, as well as when lifecycle events are triggered. For more information on how to implement a connector see the [Connector SDK godocs](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk), the [Connector Template](https://github.com/ConduitIO/conduit-connector-template) or the [guide for building a connector](/docs/developing/connectors/). ## Lifecycle The lifecycle of the connector generally looks like this: 1. The [connector plugin is loaded](#connector-plugin-is-loaded) 2. The user [creates the connector](#connector-is-created) 3. The user [starts and stops the pipeline](#pipeline-is-started-and-stopped) 4. The user [deletes the connector](#connector-is-deleted) ### Connector plugin is loaded Conduit loads all connector plugins it discovers in the [connector path](/docs/using/connectors/installing#installing-a-connector-in-conduit) while starting up. Each plugin is started in a separate process to ensure it is a valid binary and to obtain the connector specifications. These specifications are then stored in Conduit's memory and exposed through the API. This helps the UI display the available connector plugins and their corresponding parameters that can be configured. During the loading of connector specifications, the [`Source.Parameters`](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Source) and [`Destination.Parameters`](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Destination) methods are invoked to retrieve the parameters. These methods are called only once, and no other methods are triggered before the connector process is stopped. ### Connector is created Upon creating a connector ([`POST /v1/connectors`](/api#post-/v1/connectors)), Conduit validates the connector configuration. It starts an instance of the connector plugin and invokes the `Configure` method to validate the configuration. If the method returns no errors, the configuration is deemed valid and the connector creation process continues, otherwise, the creation of the connector is aborted. After that Conduit signals the plugin to stop by invoking the `Teardown` method. It's worth noting that the methods mentioned above are called either on [`Source`](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Source) or [`Destination`](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Destination), depending on the type of the connector. ### Pipeline is started and stopped Starting ([`POST /v1/pipelines/{id}/start`](/api#post-/v1/pipelines/-id-/start)) and stopping ([`POST /v1/pipelines/{id}/stop`](/api#post-/v1/pipelines/-id-/stop)) a pipeline triggers the start and stop of all connectors on that pipeline. This means that a connector plugin process is started for each connector, and it is kept running until the pipeline is stopped. When the connector plugin starts, Conduit calls its methods in a specific order. Note that the methods mentioned here are called either on [`Source`](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Source) or [`Destination`](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Destination), depending on the type of the connector. - `Configure` is first called to validate and store the connector configuration. - Then Conduit triggers any applicable lifecycle events, specifically [`LifecycleOnCreated`](#on-created) or [`LifecycleOnUpdated`](#on-updated). See [lifecycle events](#lifecycle-events) for more detailed information on when each event is triggered. - `Open` is called to allow the connector to prepare for reading or writing records (e.g. open connections). - At this point the called methods depend on the type of the connector: - For a [`Source`](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Source), Conduit will start continuously calling `Read` to fetch new records and `Ack` to signal acknowledgments for successfully processed records. - For a [`Destination`](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Destination), Conduit will continuously call `Write` with records that should be written to the destination resource. When the pipeline is stopped and Conduit stops reading/writing records, the last method called is `Teardown`. This signals that the connector plugin process will be terminated and provides a chance to clean up any open resources (e.g. close connections). ### Connector is deleted When a user deletes a connector ([`DELETE /v1/connectors/{id}`](/api#delete-/v1/connectors/-id-)), Conduit starts the connector plugin and triggers the [On Deleted](#on-deleted) event. After that it calls `Teardown` and stops the connector plugin process. The specific methods called depend on whether the connector is a [`Source`](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Source) or [`Destination`](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Destination). ## Lifecycle Events Connector Lifecycle Events are additional integration points that let connector plugins execute certain operations when the connector is created, updated, or deleted. These events are useful for connectors that need to initialize resources only once in their lifecycle and clean them up when the connector is deleted. For example, the Postgres connector needs to create a logical replication slot when the pipeline is started for the first time. This slot should persist between pipeline restarts but should be deleted when the pipeline is deleted. Lifecycle events can be used to implement this behavior. The methods mentioned below are called either on [`Source`](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Source) or [`Destination`](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Destination), depending on the type of the connector. ### On Created When this event is triggered, Conduit invokes the method `LifecycleOnCreated`. This method can be used to perform initialization tasks that only need to happen once in the lifetime of a connector, such as creating a bucket or preparing a replication slot. Any resources created during this method are owned by the connector and should be cleaned up during the [On Deleted](#on-deleted) lifecycle event. The On Created event is triggered the first time the pipeline is started after the connector is created. The connector is considered "successfully started" only after this event is successfully triggered (i.e. `LifecycleOnCreated` returns no error). After that, the On Created event won't be triggered again, and instead, [On Updated](#on-updated) or [On Deleted](#on-deleted) events will be triggered when applicable. Edge cases explained: - If a connector is created and deleted without being started in between, the event won't be triggered. - If a connector is created and updated without being started in between, the On Created event will be triggered at the next start with the latest configuration. ### On Updated When this event is triggered, Conduit calls the method `LifecycleOnUpdated`. This method can be used to update anything that was initialized in [On Created](#on-created), in case the configuration change affects it. This event is triggered the first time the pipeline is started after the connector plugin configuration was updated, provided that the connector was started at least once before and has already received the [On Created](#on-created) event. The event will only be triggered if the connector plugin configuration is different than the last time the connector was started. Edge cases explained: - If a connector is created and updated without being started in between, the On Updated event will _not_ be triggered. Instead it will trigger the [On Created](#on-created) event. - If the connector plugin configuration is updated multiple times, and the connector ends up with the same configuration as the last time it was started, then no lifecycle event will be triggered. - If the connector plugin configuration is updated and changed multiple times, the On Updated event will be triggered only with the newest configuration. ### On Deleted When this event is triggered, Conduit calls the method `LifecycleOnDeleted`. This method can be used to perform cleanup operations for any resources that were initialized during the [On Created](#on-created) event. The On Deleted event is triggered when the connector gets deleted ([`DELETE /v1/connectors/{id}`](/api#delete-/v1/connectors/-id-)), but only if the connector was previously started and received the [On Created](#on-created) event. Edge cases explained: - If a connector is created and deleted without ever being started, the event won't be triggered. - If a connector is updated and deleted without being started in between, the On Deleted event will receive the configuration that was active the last time the connector successfully started. ![scarf pixel conduit-site-docs-developing-connectors](https://static.scarf.sh/a.png?x-pxid=3ada0949-fa61-40d6-a44a-76447ea4e39f)
./2-developing/0-connectors/5-using-a-custom-connector.mdx:::--- title: "Using a Custom Connector in Conduit" --- ## Adding the Connector to Conduit The Conduit Connector template includes a `Makefile` that can be used to build a binary version of your connector. ```bash make build ``` After the command runs you should see a binary file with the name of the connector in the local directory. To run the Connector, you will need to move it to a directory that Conduit can access. By default, this directory is the `connectors` directory located alongside the Conduit binary. Move your newly created Connector binary into the `connectors` directory and run Conduit using the following command: ```bash ./conduit ``` Navigate to the Conduit UI (by default located at [http://localhost:8080/ui](http://localhost:8080/ui) while Conduit is running) to see your Connector in the list of available Connectors ![Custom Connector in Conduit UI](/img/conduit-custom-connector-ui-screenshot.png) ## Using the Connector in a Conduit Pipeline Although you can use the Connector directly within the Conduit UI, the recommended way to use the Connector is define a [pipeline configuration file](/docs/using/pipelines/configuration-file). When using custom Connectors in a pipeline configuration, they are prefixed with phrase 'standalone' unlike built in Connectors which are prefixed with 'builtin'. For example, to use a custom Connector named `file-sync` it would look as follows: `plugin: standalone:file-sync@[replace-with-connector-version]`. More information about [Referencing Connectors](/docs/using/connectors/referencing). Below is an example of using a custom Connector in a pipeline configuration file: ``` yaml title="pipelines/use-custom-connector.yml" version: 2.2 pipelines: - id: use-custom-connector # run pipeline on startup status: running description: > Example pipeline reading to use a custom file sync connector. connectors: - id: source-sync type: source # use the custom file-sync plugin as the source plugin: standalone:file-sync@v0.1.0 settings: # use ./data as the directory input to the file-sync connector; the connnector will watch for new files in this directory directory: ./data - id: destination-sync type: destination # use the custom file-sync plugin as the destination plugin: standalone:file-sync@v0.1.0 settings: # use ./new_destination as the directory input to the file-sync connector; the connnector will write files from the pipeline to this directory directory: ./new_destination ``` ![scarf pixel conduit-site-docs-developing-connectors](https://static.scarf.sh/a.png?x-pxid=3ada0949-fa61-40d6-a44a-76447ea4e39f)
./2-developing/0-connectors/4-developing-destination-connectors.mdx:::--- title: "Developing a Destination Connector" --- A Destination is responsible for writing an [OpenCDC Record](https://pkg.go.dev/github.com/conduitio/conduit-commons/opencdc#Record) to third party systems. You need to implement the functions required by Destination and provide your own implementations. Information about individual functions are listed below. The * *`destination.go`** file is the main file where the functionality of your Destination Connector is implemented. ## `Destination struct` Every Destination implementation needs to include an [UnimplementedDestination](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#UnimplementedDestination) to satisfy the interface. This allows us to potentially change the interface in the future while remaining backward compatible with existing Destination implementations. This struct can be modified to add additional fields that can be accessed throughout the lifecycle of the Connector. ```go type Destination struct { sdk.UnimplementedDestination config DestinationConfig } ``` ## Destination Connector Lifecycle Functions ### `NewDestination()` A constructor function for your Destination struct. Note that this is the same function that should be set as the value of `Connector.NewDestination`. The constructor should be used to wrap your Destination in the default middleware. You can add additional middleware, but unless you have a very good reason, you should always include the default middleware. ```go func NewDestination() sdk.Destination { // Create Destination and wrap it in the default middleware. return sdk.DestinationWithMiddleware( &Destination{}, sdk.DefaultDestinationMiddleware()..., ) } ``` **Additional options via `DestinationMiddlewareOption`**: Currently, the available destination middleware options can be found [here](https://github.com/ConduitIO/conduit-connector-sdk/blob/1cbe778fabc8f903e075872560e6a91049d2e978/destination_middleware.go#L44-L50). ### `Parameters()` A map of named Parameters that describe how to configure the connector. This map is typically generated using [ `paramgen`](https://github.com/ConduitIO/conduit-commons/tree/main/paramgen). ```go func (d *Destination) Parameters() config.Parameters { return d.config.Parameters() } ``` ### `Configure()` Validates and stores configuration data for the connector. Any complex validation logic should be implemented here. ```go func (d *Destination) Configure(ctx context.Context, cfg config.Config) error { err := sdk.Util.ParseConfig(ctx, cfg, &d.config, NewDestination().Parameters()) if err != nil { return err } // add custom validations here return nil } ``` ### `Open()` Prepares the connector to start writing records. If needed, the connector should open connections in this function. ```go func (d *Destination) Open(context.Context) error { // opens or creates a file at the given path file, err := d.openOrCreate(d.config.Path) if err != nil { return err } d.file = file return nil } ``` ### `Write()` Writes `len(records)` from a slice of `opencdc.Record`s received from the Conduit pipeline to the destination right away without caching. It should return the number of records written from the slice and any error encountered that caused the write to stop early. ```go func (d *Destination) Write(_ context.Context, recs []opencdc.Record) (int, error) { for i, r := range recs { _, err := d.file.Write(append(r.Bytes(), '\n')) if err != nil { return i, err } } return len(recs), nil } ``` ### `Teardown()` Teardown signals to the connector that there will be no more calls to any other function. Any connections that were created in the `Open()` function should be closed here. ```go func (d *Destination) Teardown(context.Context) error { if d.file != nil { return d.file.Close() } return nil } ``` ![scarf pixel conduit-site-docs-developing-connectors](https://static.scarf.sh/a.png?x-pxid=3ada0949-fa61-40d6-a44a-76447ea4e39f)
./2-developing/0-connectors/2-connector-specification.mdx:::--- title: "Connector Specification" --- [Specification](https://pkg.go.dev/github.com/conduitio/conduit-connector-sdk#Specification) contains general information regarding the plugin like its name and what it does. ### `spec.go` This file defines the public-facing characteristics of the connector such as its name and capabilities. Begin by updating placeholder values with the specific details of your connector. ```go var version = "v0.1.0" // Specification returns the connector's specification. func Specification() sdk.Specification { return sdk.Specification{ Name: "file-sync", Summary: "<describe your connector>", Description: "<describe your connector in detail>", Version: version, Author: "<your name>", } } ``` ![scarf pixel conduit-site-docs-developing-connectors](https://static.scarf.sh/a.png?x-pxid=3ada0949-fa61-40d6-a44a-76447ea4e39f)
./2-developing/0-connectors/0-connectors.mdx:::--- title: "Guidelines for writing a connector" toc_max_heading_level: 6 --- :::info This page describes patterns and processes that have been successfully used in many Conduit connectors over the past few years. The document assumes basic knowledge about how Conduit works. ::: Conduit connectors can be built in any programming language that supports gRPC. To make it easier to write connectors we provide a [Connector SDK](https://github.com/ConduitIO/conduit-connector-sdk) written in Go. Using the SDK is the recommended way of writing a Conduit connector. If you are implementing a connector in a different language you should refer to the [Connector Protocol](https://github.com/ConduitIO/conduit-connector-protocol). ## Start with the connector template The easiest way to start implementing your own Conduit connector is by using the [Conduit connector template](/docs/developing/connectors/conduit-connector-template). It contains the basic project structure as well as some additional utilities like GitHub actions and a Makefile with commonly used targets and GitHub workflows for linting the code, running unit tests, automatically merging minor dependabot upgrades, etc. ## Research the 3rd party system Researching the 3rd party system for which a Conduit connector is built helps understand the capabilities and limitations of the system, which in turn results in a better connector design and better work organization. Some questions that typically need to be answered: 1. **How is the data organized in the system?** The way data is organized in the system will affect how the connector is configured (e.g. what data is a user able to read or write to) and how the hierarchy is mapped to records and collections in Conduit. 2. **What parameters are needed to connect to the system?** We generally recommend using connection strings/URIs, if available. The reason for this is that modifying a connector's parameters is a matter of changing an existing string, and not separate configuration parameters in a connector. 3. **What APIs or drivers are available?** The choice is influenced by a number of factors, such as: - Is there a driver available? If a public API and a driver are available, we recommend using a driver, since it's a higher level of abstraction. - In which language should the connector be written? - If the language of choice is Go, is a pure Go driver available? A CGo driver may make the usage and deployment of a connector more complex. - Does the driver expose all the functionalities needed? - How mature is the driver? 4. **What authentication methods should be supported?** If multiple authentication methods are available, then a decision needs to be made about the methods that should be implemented first. Additionally, it should be understood how expired credentials should be handled. For example, a connector won't be able to handle an expired password, but a token can sometimes be refreshed. 5. **Can the connector be isolated from other clients of the system?** In some cases a connector, as a client using a system, might affect other clients, for example when getting messages from a message broker's queue, the message will be delivered to the connector, while other clients might expect the message. 6. **Are the source/destination specific features supported?** Source and destination connectors may have specific requirements (some of them are outlined in later sections). When researching, attention should be paid if those requirements can be fulfilled. ## Development ### Familiarize with the Connector SDK/connector protocol Conduit's Connector SDK is the Go software development kit for implementing a connector for Conduit. If you want to implement a connector in another language please check the [connector protocol](https://github.com/conduitio/conduit-connector-protocol). ### Write a test pipeline Having a test pipeline helps see the results of development sooner rather than later. When working on a source connector, a file or log destination can be used to build a test pipeline. When working on a destination, a file or generator source can be used to manually or automatically generate some test data for the destination connector. ### Use built-in connectors as examples Conduit's [built-in connectors](/docs/using/connectors/list) are great references that can be used when writing connectors. They are also the first ones to be updated with latest SDK and protocol changes. ### Configuration A connector's configuration is a map in which keys are parameter names and values are parameter values. Every source and destination needs to return the parameters that it accepts, and, optionally, the validations that need to be run on those. Conduit offers [ParamGen](https://github.com/ConduitIO/conduit-commons/tree/main/paramgen), a tool that generates the parameters map from a configuration struct. The SDK contains a function, `sdk.Util.ParseConfig`, that can parse and validate a user's configuration map into the configuration struct. ### Middleware Conduit's Connector SDK adds default middleware that, by default, enables some functionality. A connector developer should be familiar with the middleware, especially with the [schema related middleware](/docs/using/connectors/configuration-parameters/schema-extraction). ### Source connectors The following section summarizes best practices that are specific to source connectors. #### Deciding how should a record position look like Every record read by a source connector has a [position](/docs/using/opencdc-record#fields) attached. If a connector is stopped after that record is read, its position will be given to the connector's `Open()` method the next time it starts. Hence, a position needs to contain enough information for a source connector to resume reading records from where it exactly stopped. A position is a slice of bytes that can represent any data structure. In Conduit connectors, it's common to see that a position is actually a `struct`, that's marshalled into a JSON string. #### Implementing snapshots Firstly, it should be clarified if supporting snapshots is a requirement or if it's possible to do at all. If a connector is required to support snapshots, then it's recommended to make it possible to turn off snapshots through the connector configuration. The following things need to be taken into account when implementing a snapshot procedure: - The snapshot needs to be consistent. - The set of the existing data can be quite large. - Restarting a connector during a snapshot should **not** require re-reading all the data again. This is because in some destination connectors it may cause data duplication, and it could be a significant performance overhead. #### Implementing change data capture (CDC) Change Data Capture (CDC) should be implemented so that the following criteria is met: 1. If a snapshot is needed, changes that happened while the snapshot was running should be captured too. 2. Different types of changes might be possible (new data inserted, existing data updated or deleted). 3. Changes that happened while a connector was stopped need to be read by the connector when it starts up (assuming that the changes are still present in the source system). Some source systems may provide a change log (e.g. WAL in PostgreSQL, binlog in MySQL, etc.). Others may not and a different way to capture changes is needed. Specifically, for connectors written for SQL databases, two patterns can be used: 1. **Triggers** With triggers, it's possible to capture all types of changes (creates, updates and deletes). A trigger will write the event with necessary metadata (operation performed, timestamp, etc.) into a "trigger table", that will be read by the connector. An advantage of this approach is that all types of operations can be captures. However, it may incur a performance penalty. 2. **A timestamp-based query** If the table a source connector is reading from has a timestamp column that is updated whenever a new row is inserted or an existing row is updated, then a query can be used to fetch changes. A disadvantage of this approach is that delete operations cannot be captured. #### Iterator pattern A pattern that we found to be useful when writing source connectors is the iterator pattern. The basic idea is that a source connector's `Read()` method reads records through an iterator: ```go type Iterator interface { HasNext() bool Next() opencdc.Record } type Source struct {} func (s *Source) Read(ctx context.Context) (opencdc.Record, error) { if s.iterator.HasNext(ctx) { return opencdc.Record{}, sdk.ErrBackoffRetry } return s.iterator.Next(ctx) } ``` There are three implementations of the iterator interface: - `SnapshotIterator`: used when performing a snapshot - `CDCIterator`: used in CDC - `CombinedIterator`: an iterator that combines the above two and is able to perform a snapshot and then switch to CDC. The iterator to be used is determined in the source's `Open()` method based on the connector's configuration: if a snapshot is required, then a `CombinedIterator` will be used, if not, then the `CDCIterator` will be used. There are multiple advantages of this approach. The source connector remains " focused" on the higher level operations, such as configuration, opening the connectors, tearing down, etc. The iterators contain the code that deals with the source system itself and convert the data into `opencdc.Record`s. They also take care of switching from snapshot mode to CDC mode. ### Destination connectors #### Batching Batching can considerably improve the write performance and should be considered when developing a destination connector. The Connector SDK provides [batching middleware](/docs/using/connectors/configuration-parameters/batching) that automatically builds batches. When writing batches into a destination system, the order of records should **not** be changed, even if grouping records would be useful in a way. For example, in some connectors, it's useful to group records by operation, because the destination system has different procedures for those operations. If a destination connector receives the following batch: ```text [create_1, update_1, update_2, create_2] ``` then this batch can be split into following batches: ```text Batch 1: [create_1] Batch 2: [update_1, update_2] Batch 3: [create_2] ``` This way, ordering is preserved and the connector can still take advantage of batching. ### Acceptance tests Conduit's Connector SDK contains a set of acceptance tests that verify many of a connector's methods, such as: - Are missing configuration parameters detected? - Can a source that is stopped during CDC be successfully restarted? - Can a destination write a batch of records? An example of implemented acceptance tests can be found in the [Kafka connector](https://github.com/ConduitIO/conduit-connector-kafka/blob/3233c37392e7249e9d64c9cae0374a0b47ca840f/acceptance_test.go). ### Debugging the connector The steps for debugging a standalone connector have been described [here](https://github.com/ConduitIO/conduit/discussions/1724#discussioncomment-10178484). ![scarf pixel conduit-site-docs-developing-connectors](https://static.scarf.sh/a.png?x-pxid=3ada0949-fa61-40d6-a44a-76447ea4e39f)
./2-developing/1-processors/1-building.mdx:::--- title: 'Build your own' --- You can build your own Conduit standalone processors in Go using the [Processor SDK](https://github.com/ConduitIO/conduit-processor-sdk). We currently only provide a Go SDK for processors. However, if you'd like to use another language for writing a processor, feel free to [open an issue](https://github.com/ConduitIO/conduit/issues/new?assignees=&labels=feature%2Ctriage&projects=&template=1-feature-request.yml&title=Processor+SDK+%3A+%3Clanguage%3E) and request a specific language SDK. You can also read [how standalone processors work](/docs/developing/processors/how-it-works) under the hood and build an SDK yourself. The [Processor SDK](https://github.com/ConduitIO/conduit-processor-sdk) exposes two ways of building processors, one for simple processors without configuration parameters, and another that gives you full control over the processor. ## Using `sdk.NewProcesorFunc` If the processor is very simple and can be reduced to a single function (e.g. no configuration needed), then we can use `sdk.NewProcessorFunc()` to create a processor as below: ```go //go:build wasm package main import ( sdk "github.com/conduitio/conduit-processor-sdk" ) func main() { sdk.Run(&sdk.NewProcessorFunc( sdk.Specification{Name: "simple-processor"}), func(ctx context.Context, rec opencdc.Record) (opencdc.Record, error) { // do something with the record return rec }, ) } ``` However, if the processor needs configuration, or is more complicated than only one function, then we should use the full processor approach. ## Using `sdk.Processor` To build the full-blown processor, the SDK contains an interface called [sdk.Processor](https://pkg.go.dev/github.com/conduitio/conduit-processor-sdk#Processor) that contains some methods to be implemented. These methods are: ### Specification Specification contains the metadata for the processor, which can be used to define how to reference the processor, describe what the processor does and the configuration parameters it expects. Here's a list of the fields in the `sdk.Specification` struct and their descriptions: * `Name`: the name of the processor. Note that the name should be unique across all processors, as it's used to reference the processor in a pipeline (see [referencing processors](/docs/using/processors/referencing)). * `Version`: the version of the processor. This should be a valid [semver](https://semver.org) version and needs to be updated whenever the processor's behavior changes. * `Summary`: a short description of what the processor does (ideally a one-liner). * `Description`: a more detailed description of what the processor does. This field can contain markdown. * `Author`: the author of the processor. * `Parameters`: a map of the processor's configuration parameters. Each parameter should have a name, a type, a description, and a list of validations. Note that the validations defined on a parameter are automatically executed in `sdk.ParseConfig` (see [Configure](#configure)). Conduit also provides [`paramgen`](https://github.com/ConduitIO/conduit-commons/tree/main/paramgen), a helpful tool that generates the `Parameters` map from a Go struct. This allows us to create a configuration struct that contains the processor's parameters, define default values and validations using struct tags, and generate the `Parameters` map. Check out the [ParamGen readme](https://github.com/ConduitIO/conduit-commons/tree/main/paramgen) for more details. :::info Note that a processor's name and version need to be unique across all processors, as they are used to [reference](/docs/using/processors/referencing) the processor in a pipeline. If two processors have the same name and version, Conduit will refuse to load them. ::: #### Example without ParamGen You can define the `Specification` method as below and manually define the parameters map: ```go package example import ( "context" "github.com/conduitio/conduit-commons/config" sdk "github.com/conduitio/conduit-processor-sdk" ) func (p *AddFieldProcessor) Specification(context.Context) (sdk.Specification, error) { return sdk.Specification{ Name: "myAddFieldProcessor", Summary: "Add a field to the record.", Description: `This processor lets you configure a field that will be added to the record into field. If the payload is not structured data, this processor will panic.`, Version: "v1.0.0", Author: "John Doe", Parameters: map[string]config.Parameter{ "field": { Type: config.ParameterTypeString, Description: "Field is the target field that will be set.", Validations: []config.Validation{ config.ValidationRequired{}, }, }, "name": { Type: config.ParameterTypeString, Description: "Name is the value of the field to add.", Validations: []config.Validation{ config.ValidationRequired{}, }, }, }, }, nil } ``` #### Example with ParamGen 1. Add a struct that contains the needed parameters: ```go //go:generate paramgen -output=addField_paramgen.go addFieldConfig type addFieldConfig struct { // Field is the target field that will be set. Field string `json:"field" validate:"required"` // Name is the value of the field to add. Name string `json:"value" validate:"required"` } ``` 2. Generate the parameters by running: ```` paramgen -output=addField_paramgen.go addFieldConfig ```` This will generate a file called `addField_paramgen.go` that contains the generated parameters map, which in turn can be used under `specification` to make it simpler and shorter, example: ```go //go:generate paramgen -output=addField_paramgen.go addFieldConfig type addFieldConfig struct { // Field is the target field that will be set. Field string `json:"field" validate:"required"` // Name is the value of the field to add. Name string `json:"value" validate:"required"` } func (p *AddFieldProcessor) Specification(context.Context) (sdk.Specification, error) { return sdk.Specification{ Name: "myAddFieldProcessor", Summary: "Add a field to the record.", Description: `This processor lets you configure a field that will be added to the record into field. If the payload is not structured data, this processor will panic.`, Version: "v1.0.0", Author: "John Doe", Parameters: addFieldConfig{}.Parameters(), // generated by paramgen }, nil } ``` ### Configure Configure is the first function to be called in a processor. It provides the processor with the configuration that needs to be validated and stored to be used in other methods. This method should not open connections or any other resources. It should solely focus on parsing and validating the configuration itself. To add custom validations, simply validate the parameters manually under this method, and return an error if the `config` map is not valid. On the other hand, using the utility function below would apply the builtin validations to the configuration. The [Processor SDK](https://github.com/ConduitIO/conduit-processor-sdk) provides some useful utility functions to help implementing this method: * `sdk.ParseConfig`: used to sanitize the configuration, apply defaults, validate it using builtin validations, and copy the values into the target object. * `sdk.NewReferenceResolver`: creates a new reference resolver from the input string. The input string is a reference to a field in a record, check [Referencing record fields](/docs/using/processors/referencing-fields for more details. The method will return a `resolver` that can be used to resolve a reference to the specified field in a record and manipulate that field (`get`, `set` and `delete` the value, or `rename` the referenced field). Using these utility functions, most of the `Configure` method implementations would look something like: ````go func (p *AddFieldProcessor) Configure(ctx context.Context, m map[string]string) error { err := sdk.ParseConfig(ctx, m, &p.config, addFieldConfig{}.Parameters()) if err != nil { return fmt.Errorf("failed to parse configuration: %w", err) } resolver, err := sdk.NewReferenceResolver(p.config.Field) if err != nil { return fmt.Errorf("failed to parse the %q param: %w", "field", err) } p.referenceResolver = resolver return nil } ```` ### Open This function is used to open connections, start background jobs, or initialize resources that are needed for the processor. Note that implementing this function is **_optional_**. ### Process Process is the main show of the processor, here we would manipulate the records received and return the processed ones. After processing the slice of records that the function got, and if no errors occurred, it should return a slice of `sdk.ProcessedRecord` that matches the length of the input slice. However, if an error occurred while processing a specific record, then it should be reflected in the `ProcessedRecord` with the same index as the input record, and should return the slice at that index length. For the interface `sdk.ProcessedRecord`, there are three main processed records types: 1. `sdk.SingleRecord`: is a single processed record that will continue down the pipeline. 2. `sdk.FilterRecord`: is a record that will be acked and filtered out of the pipeline. 3. `sdk.ErrorRecord`: is a record that failed to be processed and will be nacked. Example: ````go func (p *AddFieldProcessor) Process(ctx context.Context, records []opencdc.Record) []sdk.ProcessedRecord { out := make([]sdk.ProcessedRecord, 0, len(records)) for _, record := range records { resolver, err := p.referenceResolver.Resolve(&record) if err != nil { return append(out, sdk.ErrorRecord{Error: err}) } err = resolver.Set(p.config.Name) if err != nil { return append(out, sdk.ErrorRecord{Error: err}) } out = append(out, sdk.SingleRecord(record)) } return out } ```` Note that `Process` should be idempotent, as it may be called multiple times with the same records (e.g. after a restart when records were not flushed). ### Teardown This function acts like a counterpart to [`Open`](#open), use this function to close any open connections or resources that were initialized under `Open`. Note that implementing this function is also **_optional_**. ### Entrypoint Since the processor will be run as a standalone _WASM_ plugin, we need to add an entrypoint to it. Also, we should add a `go:build` tag to ensure that this file is only included in the build when targeting WebAssembly. the entrypoint will have to be in a separate package (i.e. folder), by Go convention it's normally under `cmd/my-binary-name`, so it would look something like: ``` . ├── my-processor.go # actual processor implementation └── cmd └── processor └── main.go # entrypoint ``` Entry point example: ```go //go:build wasm package main import ( sdk "github.com/conduitio/conduit-processor-sdk" "github.com/conduitio/my-processor/example" ) func main() { sdk.Run(example.NewProcessor()) } ``` Check [Compiling the processor](#compiling-the-processor) for what to do next, and how to compile the processor. ## Schemas Processors have access to the schemas in the used Schema Registry. By default, if the pipeline uses a schema registry and the processor gets a record with the schema info in the `Metadata`, then the processor will have a middleware enabled. The middleware will decode the records before they are passed to the processor using their corresponding schema from the schema registry, and encode them again after the processing is done. To change this default behaviour, you can change these processor's configurations accordingly: - `sdk.schema.decode.key.enabled`: Whether to decode the record key using its corresponding schema from the schema registry. - `sdk.schema.decode.payload.enabled`: Whether to decode the record payload using its corresponding schema from the schema registry. - `sdk.schema.encode.key.enabled`: Whether to encode the record key using its corresponding schema from the schema registry. - `sdk.schema.encode.payload.enabled`: Whether to encode the record payload using its corresponding schema from the schema registry. **Example** of a pipeline configuration file with these parameters: ```yaml version: 2.2 pipelines: - id: test status: running connectors: - id: employees-source type: source plugin: standalone:generator settings: rate: 1 collections.str.format.type: structured collections.str.format.options.id: int collections.str.format.options.name: string collections.str.format.options.admin: bool collections.str.operations: create - id: logger-dest type: destination plugin: standalone:log processors: - id: access-schema plugin: standalone:processor-simple sdk.schema.decode.key.enabled: false # disabling the default behaviour sdk.schema.encode.key.enabled: false # disabling the default behaviour ``` Processors can access the Schema Registry and the schemas using two utility functions: 1. `schema.Create` : You can use this utility function to create a new schema and add it to the Schema Registry. This function can be called in any of the main processor methods. **Example**: ```go func (p *exampleProcessor) Open(ctx context.Context) error { // Add a new schema to the schema registry before starting to process the records. schemaBytes := []byte(`{ "name": "record", "type": "record", "fields": [ { "name": "admin", "type": "boolean" }, { "name": "id", "type": "int" }, { "name": "name", "type": "string" } ] }`) _, err := schema.Create(ctx, schema.TypeAvro, "subject1", schemaBytes) return err } ``` 2. `schema.Get`: You can use this utility function to get a schema from the Schema Registry using its `version` and `subject`. This function can be called in any of the main processor methods. **Example**: ```go func (p *exampleProcessor) Process(ctx context.Context, records []opencdc.Record) []sdk.ProcessedRecord { out := make([]sdk.ProcessedRecord, 0, len(records)) for _, record := range records { // get the schema subject name from the metadata subject, err := rec.Metadata.GetPayloadSchemaSubject() if err != nil { return append(out, sdk.ErrorRecord{Error: err}) } // get the schema version from the metadata version, err := rec.Metadata.GetPayloadSchemaVersion() if err != nil { return append(out, sdk.ErrorRecord{Error: err}) } // get the schema using the subject and the version get, err := schema.Get(ctx, subject, version) // print the schema fmt.Println(string(get.Bytes)) } return out } ``` Example output (the printed schema): ```json { "name": "record", "type": "record", "fields": [ { "name": "admin", "type": "boolean" }, { "name": "id", "type": "int" }, { "name": "name", "type": "string" } ] } ``` ## Logging You can get a `zerolog.logger` instance from the context using the [`sdk.Logger`](https://pkg.go.dev/github.com/conduitio/conduit-processor-sdk#Logger) function. This logger is pre-configured to append logs in the format expected by Conduit. Keep in mind that logging in the hot path (i.e. in the `Process` method) can have a significant impact on the performance of the processor, therefore we recommend using the `Trace` level for logs that are not essential for the operation of the processor. Example: ```go func (p *AddFieldProcessor) Process(ctx context.Context, records []opencdc.Record) []sdk.ProcessedRecord { logger := sdk.Logger(ctx) logger.Trace().Msg("Processing records") // ... } ``` ## Compiling the processor Conduit uses [WebAssembly](https://webassembly.org) to run standalone processors. This means that we need to build the processor as a WebAssembly module. You can do this by setting the environment variables `GOARCH=wasm` and `GOOS=wasip1` when running `go build`. This will produce a WebAssembly module that can be used as a processor in Conduit. So, to compile the processor, run: ```sh GOARCH=wasm GOOS=wasip1 go build -o processor.wasm cmd/processor/main.go ``` **_Congratulations!_** Now you have a new standalone processor. Check [Standalone processors](/docs/developing/processors/#where-to-put-them) for details on how to use your standalone processor in a Conduit pipeline. :::note To see more standalone processor examples, check out our [example processor repository](https://github.com/ConduitIO/conduit-processor-example). ::: ![scarf pixel conduit-site-docs-developing-processors](https://static.scarf.sh/a.png?x-pxid=3108abd7-7971-40bd-a92e-87c4f2562fe6)
./2-developing/1-processors/0-conduit-processor-template.mdx:::--- title: "Conduit Processor Template" --- ## Initializing a Conduit Processor Project To begin the development of a custom Conduit processor, it is recommended developers should initialize their project using the [Conduit processor template](https://github.com/ConduitIO/conduit-processor-template). This template streamlines the setup process by providing a fundamental project structure, along with utility configurations for GitHub actions and a Makefile. Included in the Conduit Processor Template are: - Base code for processor's configuration and lifecycle. - Sample unit tests to validate processor functionality. - A preconfigured Makefile to assist with common build tasks. - GitHub Actions Workflows for continuous integration including building, testing, linting, and automated release creation upon tag push. - Dependabot configurations to automate dependency updates and minor version auto-merging. - Issue and Pull Request templates for consistent contribution practices. - A README template to guide project documentation. ### Steps to Use the Processor Template 1. On the repository's main page, select "Use this template". 2. Provide the new repository details as prompted. 3. Upon repository creation, clone it to your local development environment. 4. Execute the `./setup.sh` script with the desired module name, e.g., `./setup.sh github.com/awesome-org/conduit-processor-file`. 5. (Optional) Define code owners in the `CODEOWNERS` file. :::note By convention the name of the repository should be conduit-processor-`processor name`. So if you would like to reference the processor using foo, the repository should be named conduit-processor-foo. ::: ## Developing Processors Implement a processor by defining a struct that satisfies the [sdk.Processor](https://pkg.go.dev/github.com/conduitio/conduit-processor-sdk#Processor) interface. Processors follow a lifecycle of: Configure, Open, Process, and Teardown, However a simple processor can be created with only Specification and Process being overridden. There are other optional functions that help with the configuration of more complex processors such as: Configure, Open, Teardown and MiddlewareOptions. Further information on developing a processor can be found at the following link(s): - [Build your own](/docs/developing/processors/building) - [How it works](/docs/developing/processors/how-it-works)
./2-developing/1-processors/index.mdx:::--- title: 'Guidelines for writing a processor' --- These are processors you'd write yourself in cases the [Built-in](/docs/using/processors/builtin/) ones don't meet your needs. ## How to write one Thanks to our [Web Assembly (Wasm) processor](/docs/developing/processors/how-it-works) you can start writing processors in any language that can be compiled to Web Assembly. As a start, Conduit already provides a [conduit-processor-sdk](https://github.com/ConduitIO/conduit-processor-sdk) that will let you [write a processor](/docs/developing/processors/building) in Go. ### Where to put them By default, standalone processors are expected to be found in a folder named `processors` alongside of your pipelines or [standalone connectors](/docs/using/connectors/getting-started): ```shell │ # Conduit binary ├── conduit │ # Folder with pipeline configurations (yaml files) ├── pipelines │ # Folder with standalone connectors (binary files) ├── connectors │ # Folder with standalone processors (wasm files) └── processors ``` However, in case you need to reference processors in a different location, you could use the `-processors.path` flag when running Conduit: ``` ./conduit -processors.path /my-custom-processors-path ``` ### Using the [conduit-processor-sdk](https://github.com/ConduitIO/conduit-processor-sdk) Assuming you use our [conduit-processor-sdk](https://github.com/ConduitIO/conduit-processor-sdk), this is how a processor **plugin** written in Go could look like. In the following example, we're going to be adding a `processed` field to each record processed by our pipeline: ```go //go:build wasm package main import ( "context" "github.com/conduitio/conduit-commons/opencdc" sdk "github.com/conduitio/conduit-processor-sdk" ) func main() { sdk.Run(sdk.NewProcessorFunc( sdk.Specification{Name: "simple-processor", Version: "v1.0.0"}, func(ctx context.Context, record opencdc.Record) (opencdc.Record, error) { record.Payload.After.(opencdc.StructuredData)["processed"] = true return record, nil }, )) ``` After that, you'd need to compile it, and locate its `.wasm` file into the desired `processors` directory as previously mentioned: ```shell GOARCH=wasm GOOS=wasip1 go build -o simple-processor.wasm main.go ``` ## Using it in your pipeline As mentioned in our [Getting Started page](/docs/using/processors/getting-started), in order to use a processor in your pipeline, you need to update its [configuration file](/docs/using/processors/getting-started#how-to-use-a-processor) and [reference it](/docs/using/processors/referencing) accordingly: ```yaml version: 2.2 pipelines: - id: example-pipeline connectors: # define source and destination connectors # ... processors: - id: add-processed-field plugin: standalone:simple-processor ``` When running your pipeline again, you should expect seeing a new `processed` field on every record processed. :::info If you end up writing a standalone processor you'd like to share with the community, please let us know! We'd love to hear from you by: - [Joining our Discord](https://discord.meroxa.com/) - [Posting a comment on GitHub Discussions](https://github.com/ConduitIO/conduit/discussions) ::: ![scarf pixel conduit-site-docs-developing-processors](https://static.scarf.sh/a.png?x-pxid=3108abd7-7971-40bd-a92e-87c4f2562fe6)
./2-developing/1-processors/2-how-it-works.mdx:::--- title: 'How it works' --- Here we will describe how Conduit runs standalone processors under the hood. This is useful to understand when writing a processor SDK. ## WebAssembly Conduit uses [WebAssembly](https://webassembly.org) (Wasm) to run standalone processors. Wasm is a binary instruction format, designed as a portable target for compilation of high-level languages like C/C++, Rust, Java and [more](https://webassembly.org/getting-started/developers-guide/). Wasm binaries are run using a Wasm runtime, in our case the [wazero](https://wazero.io) runtime. The main reason for using Wasm is to provide a secure and portable execution environment for the processor. Wasm is designed to be run in a sandboxed environment, which means that the processor cannot access the host system directly. This is important to ensure that the processor cannot access or modify the host system, and that it cannot access or modify other processors running in the same environment. ### WASI support and limitations Conduit uses [WASI](https://wasi.dev) (WebAssembly System Interface) to provide a set of standard APIs that the processor can use to interact with the host system. Since WASI is still a work in progress, we are currently using the interface defined in [WASI preview 1](https://github.com/WebAssembly/WASI/blob/main/legacy/preview1/docs.md). :::info Because the processor has access to a limited set of system calls, it is not possible to use all the features of the host system. For example, the processor cannot access the file system, network, or any other system resource directly. ::: ## Processor lifecycle A standalone processor is a plugin, compiled into a Wasm binary which interacts with Conduit using two host functions: `command_request` and `command_response`. The plugin is expected to continuously call these functions, allocate memory as needed, parse commands, execute them, and respond back to the host. Below is a sequence diagram that shows the high level interaction between Conduit and a standalone processor. ```mermaid sequenceDiagram autonumber participant Conduit create participant Processor Conduit->>Processor: Run Processor->>Processor: Allocate initial memory for command retrieval loop Continuous interaction Processor->>Conduit: Call `command_request` Conduit-->>Processor: Populate command and return size of command break When size is an error code Processor-->>Processor: Exit with error code end Processor->>Processor: Parse and execute command Processor->>Conduit: Send result using `command_response` Conduit-->>Processor: Ok end ``` 1. The processor starts to run only when Conduit starts a pipeline containing a standalone processor. The lifetime of the processor is tied to the lifetime of the pipeline. 2. The processor allocates initial memory for command retrieval. This buffer should be reused for every command request. In case the buffer is too small, the processor should reallocate a larger buffer (see point 4). 3. The processor calls `command_request` to retrieve a command from Conduit. This call is blocking and will return when a command is available. 4. Conduit populates the allocated memory with a command and returns the size of the command or an error code. See [error codes](#error-codes) for more a list of possible error codes. See [`command_request`](#command_request) for more information about the command format. 5. In case Conduit returns an error code, the processor should exit with the error code. 6. The processor parses and executes the command. Note that everything except the actual command business logic can and should be implemented by a processor SDK. 7. The processor calls `command_response` to send the result back to Conduit. This call should happen even if the command execution failed (the result should include the error). See [`command_response`](#command_response) for more information about the response format. 8. Conduit acknowledges the result and the processor can continue retrieving the next command. ## Host functions Conduit exposes a module named `conduit` with host functions, that allow the processor to interact with Conduit. Example host function import in Go: ```go //go:wasmimport conduit command_request func _commandRequest(ptr unsafe.Pointer, size uint32) uint32 ``` All host functions accept two arguments: 1. A pointer to a memory address where Conduit can retrieve the request and/or write the result. 2. The size of the memory buffer. All host functions return a single integer which indicates the size of the response or an error code. See [error codes](#error-codes) for a list of possible error codes. If the host function returns 0 it indicates success. If the returned integer is an error code, the processor should exit immediately. The data in the memory buffer is always serialized using [Protocol Buffers](https://protobuf.dev). The schema for the data can be found in the [Buf schema registry](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1). ### `command_request` The processor should call this function to retrieve a command request from Conduit. The function is blocking and will return only when a command is available. The returned integer represents the size of the command, or an [error code](#error-codes). If the allocated memory buffer is smaller than the returned command size, the processor should reallocate a larger buffer and call `command_request` again. Conduit will populate the memory buffer with a [`CommandRequest`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.CommandRequest), which can contain one of the following commands: - [`Specify.Request`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Specify.Request) - The processor should return its specifications by calling [`command_response`](#command_response) with a [`Specify.Response`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Specify.Response). - [`Configure.Request`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Configure.Request) - Conduit passes the configuration to the processor which should be parsed and stored. The processor should respond with a [`Configure.Response`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Configure.Response). - [`Open.Request`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Open.Request) - The processor should initialize its state and respond with an [`Open.Response`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Open.Response). - [`Process.Request`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Process.Request) - The processor should process the records in the request and return the result using a [`Process.Response`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Process.Response). - [`Teardown.Request`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Teardown.Request) - The processor should clean up its state, close any open resources and respond with a [`Teardown.Response`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Teardown.Response). The plugin can expect that the next command after a `Teardown.Request` will be an error code indicating no more commands. ### `command_response` The processor should call this function to send a [`CommandResponse`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.CommandResponse) back to Conduit. The function returns 0 if the response was successfully received, or an [error code](#error-codes). The memory address sent to `command_response` should contain a [`CommandResponse`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.CommandResponse): - [`Specify.Response`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Specify.Response) should be sent in response to a [`Specify.Request`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Specify.Request). - [`Configure.Response`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Configure.Response) should be sent in response to a [`Configure.Request`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Configure.Request). - [`Open.Response`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Open.Response) should be sent in response to a [`Open.Request`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Open.Request). - [`Process.Response`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Process.Response) should be sent in response to a [`Process.Request`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Process.Request). - [`Teardown.Response`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Teardown.Response) should be sent in response to a [`Teardown.Request`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Teardown.Request). - [`Error`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:processor.v1#processor.v1.Error) should be sent in response to any command request that failed to be processed. ### `create_schema` The processor should call this function to create a new Schema in the schema registry. The memory address sent to `create_schema` should contain a [`CreateSchemaRequest`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:procutils.v1#procutils.v1.CreateSchemaRequest), then Conduit will populate the memory buffer with a [`CreateSchemaResponse`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:procutils.v1#procutils.v1.CreateSchemaResponse). The returned integer represents the size of the response if the request was successful. If the allocated memory buffer is smaller than the returned response size, the processor should reallocate a larger buffer and call `create_schema` again. If the schema creation failed, an [error code](#error-codes) will be returned instead. ### `get_schema` The processor should call this function to get a Schema from the schema registry. The memory address sent to `get_schema` should contain a [`GetSchemaRequest`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:procutils.v1#procutils.v1.GetSchemaRequest), then Conduit will populate the memory buffer with a [`GetSchemaResponse`](https://buf.build/conduitio/conduit-processor-sdk/docs/main:procutils.v1#procutils.v1.GetSchemaResponse). The returned integer represents the size of the response if the request was successful. If the allocated memory buffer is smaller than the returned response size, the processor should reallocate a larger buffer and call `get_schema` again. If getting the schema fails, an [error code](#error-codes) will be returned instead. ## Error codes The last 100 numbers at the end of an `uin32` (between 4,294,967,195 and 4,294,967,294) are reserved for error codes. If one of the host functions returns a number in this range it represents an error code and the plugin should stop running. **Current error codes:** - `4,294,967,294` - no more commands. Returned by `command_request` when Conduit has no more commands to return and the plugin should gracefully stop running (exit code 0). - `4,294,967,293` - unknown command request. Internal error in Conduit, where the command request failed to be marshaled into a proto message. - `4,294,967,292` - unknown command response. Internal error in Conduit, where the command response failed to be unmarshalled from the proto message into a struct. - `4,294,967,291` - memory out of range. Internal error in Conduit, where it tried to write into the allocated memory, but failed to do so, as the size was insufficient. - `4,294,967,290` - Internal error in Conduit, could be an error while marshalling/unmarshalling the buffer, or other reasons, the specific error should be logged by Conduit. - `4,294,967,289` - Subject not found, an internal Conduit error where the Schema with this subject does not exist. - `4,294,967,288` - Version not found, an internal Conduit error where the Schema with this Version does not exist. - `4,294,967,287` - Invalid schema, an internal Conduit error where the Schema provided is invalid. ## Logging The processor can log messages to `stderr` and `stdout` and they will show up in the Conduit logs. Even though no specific format is enforced, it is recommended to emit logs in a JSON format, as Conduit will be able to parse and display them in a more readable way. There are a few standard fields that should be included in every log message: - `level` - The log level, one of `trace`, `debug`, `info`, `warn`, `error`. If the log level is not specified, the message will be logged without a level. - `message` - The log message. Conduit will automatically add the following fields to all messages: - `time` - Will contain the time when the log was emitted. - `processor_id` - The unique identifier of the processor entity in Conduit associated with this plugin instance. Example log message emitted by the plugin: ```json { "level": "info", "contextual-info": "some contextual info", "message": "Processor is running" } ``` :::tip Please be mindful of the log level you use. Processors are executed in the hot-path of the data processing pipeline and excessive logging can have a negative impact on the performance of the pipeline. Also make sure you respect the log level set by Conduit in the environment variable `CONDUIT_LOG_LEVEL` to reduce unnecessary verbosity. ::: ## Environment variables When Conduit starts the processor, it will set the following environment variables: - `CONDUIT_LOG_LEVEL` indicates the log level that the processor should use. This is useful to control the verbosity of the logs emitted by the processor. The log level can be one of `trace`, `debug`, `info`, `warn`, `error`. - `CONDUIT_PROCESSOR_ID` is the unique identifier of the processor entity in Conduit associated with this plugin instance. Note that Conduit will automatically add this ID to log messages emmitted by the plugin, so you don't have to attach it manually. ![scarf pixel conduit-site-docs-developing-processors](https://static.scarf.sh/a.png?x-pxid=3108abd7-7971-40bd-a92e-87c4f2562fe6)
./3-scaling/0-conduit-operator.mdx:::--- title: "Conduit Operator" toc_max_heading_level: 6 --- The Conduit Operator is a [Kubernetes operator](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) designed to simplify the management and orchestration of Conduit instances. The operator extends the [Kubernetes API](https://kubernetes.io/docs/concepts/overview/kubernetes-api/) with a [custom resource](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) for Conduit. This custom resource allows users to define and manage Conduit instances declaratively, enabling seamless integration with Kubernetes' native features such as scaling, monitoring, and logging. ### Overview Conduit pipelines are represented as Conduit custom resources, where each pipeline will be provisioned as a distinct Conduit instance with its own lifecycle. The Conduit custom resource definition format is very similar to that of [pipeline configurations](/docs/using/pipelines/configuration-file): ```yaml apiVersion: operator.conduit.io/v1alpha kind: Conduit metadata: name: conduit-generator spec: running: true name: generator.log description: generator pipeline connectors: - name: source-connector type: source plugin: builtin:generator settings: - name: format.type value: structured - name: format.options.id value: "int" - name: format.options.name value: "string" - name: format.options.company value: "string" - name: format.options.trial value: "bool" - name: recordCount value: "3" - name: destination-connector type: destination plugin: builtin:log ``` ### Quickstart :::info To get started with the Conduit Operator, follow the steps described on the [Conduit Operator GitHub repository](https://github.com/ConduitIO/conduit-operator?tab=readme-ov-file#quickstart). ::: ### Installing standalone connectors #### `plugin` and `pluginVersion` A standalone connector can be installed by providing the connector name via `plugin`, and version via `pluginVersion`. The operator will then install the connector and create the necessary resources to run it. These connectors need to be referred to by their GitHub `organization/repository` names and are currently limited to the [following organizations](https://github.com/ConduitIO/conduit-operator/blob/7321042d1fa1205f3c1e5faf70775184ebafedba/pkg/conduit/plugin.go#L9-L11): - [conduitio](https://github.com/conduitio) - [conduitio-labs](https://github.com/conduitio-labs) - [meroxa](https://github.com/meroxa) :::tip Plugin version can optionally be specified or the latest will be used. ::: #### Example Here's a full example using [`conduitio/conduit-connector-generator`](https://github.com/conduitio/conduit-connector-generator) as `plugin` and `v0.8.0` as `pluginVersion`: ```yaml apiVersion: operator.conduit.io/v1alpha kind: Conduit metadata: name: conduit-generator spec: running: true name: generator.log description: generator pipeline connectors: - name: source-connector type: source plugin: conduitio/conduit-connector-generator pluginVersion: v0.8.0 settings: - name: format.type value: structured - name: format.options.id value: "int" - name: format.options.name value: "string" - name: format.options.company value: "string" - name: format.options.trial value: "bool" - name: recordCount value: "3" - name: destination-connector type: destination plugin: conduitio/conduit-connector-log ``` ### Schema Support As of [v0.11.0](/changelog/2024-08-19-conduit-0-11-0-release) Conduit supports the use of a schema registry. This allows connectors to automatically extract or use the schema referred to by the [OpenCDC record](/docs/using/opencdc-record) to encode/decode data. By default Conduit uses a builtin schema registry, however in certain use cases a schema registry needs to be shared between multiple instances. The Conduit resource allows for schema registry to be defined as of [v0.2.0](https://github.com/ConduitIO/conduit-operator/releases/tag/v0.0.2). Details are provided directly via the Conduit resource. `basicAuthUser` and `basicAuthPassword` can be provided as secrets but will be copied over into controller-owned secrets for management and ease of use. #### Example ```yaml apiVersion: operator.conduit.io/v1alpha kind: Conduit metadata: name: conduit-generator-schema-registry spec: running: true name: generator.standalone.log description: generator pipeline schemaRegistry: url: http://apicurio:8080/apis/ccompat/v7 # basicAuthUser: # - value: <schemaUser> # basicAuthPassword: # - secretRef: # key: schema-registry-password # name: schema-registry-secret connectors: - name: source-connector type: source plugin: conduitio/conduit-connector-generator settings: - name: format.type value: structured - name: format.options.id value: "int" - name: format.options.name value: "string" - name: format.options.company value: "string" - name: format.options.trial value: "bool" - name: recordCount value: "3" - name: destination-connector type: destination plugin: conduitio/conduit-connector-log pluginVersion: v0.4.0 ``` ### Deploying The operator provides a [helm chart](https://github.com/ConduitIO/conduit-operator/blob/main/charts/conduit-operator) that can be used for deployment on any cluster. Additional metadata can be injected into each provisioned conduit instance via the `controller.conduitMetadata` configuration. For example, to instruct a prometheus instance to scrape each Conduit instance metrics: ```yaml controller: conduitMetadata: podAnnotations: prometheus.io/scrape: true prometheus.io/path: /metrics prometheus.io/port: 8080 ``` For more configuration options see [charts/conduit-operator/values.yaml](https://github.com/ConduitIO/conduit-operator/blob/main/charts/conduit-operator/values.yaml). Additional metadata can be injected in each provisioned conduit instance via `controller.conduitMetadata` configuration. For example, to instruct a prometheus instance to scrape each Conduit instance metrics: ```yaml controller: conduitMetadata: podAnnotations: prometheus.io/scrape: true prometheus.io/path: /metrics prometheus.io/port: 8080 ``` For more configuration options see [charts/conduit-operator/values.yaml](https://github.com/ConduitIO/conduit-operator/blob/main/charts/conduit-operator/values.yaml). ### Using the helm chart repository Alternatively the operator can be deployed via the [Helm repository](https://helm.sh/docs/topics/chart_repository/). To add the repository to your helm repos: ```shell helm repo add conduit https://conduitio.github.io/conduit-operator ``` Install the operator in the `conduit-operator` namespace of your cluster: ```shell helm install conduit-operator \ conduit/conduit-operator --create-namespace -n conduit-operator ``` :::tip For a low-code experience, and many other Enterprise features, we recommend you use the [Conduit Platform](https://meroxa.io). :::
